{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression in depth\n",
    "============================\n",
    "\n",
    "*Fraida Fund*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# for 3d plots\n",
    "from ipywidgets import interact, fixed\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic logistic regression\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "w_true = [-1, 1.2, 1]\n",
    "sigma = 0.1\n",
    "plot_colors = np.array(sns.color_palette().as_hex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "X = np.random.uniform(0, 1, size=(n_samples,2))\n",
    "y = np.array(w_true[0]+w_true[1]*X[:,0]+w_true[2]*X[:,1] >= 0).astype(int)\n",
    "\n",
    "# Add some noise?\n",
    "X = X + sigma*np.random.randn(X.shape[0]*X.shape[1]).reshape(X.shape[0], X.shape[1])\n",
    "\n",
    "# Figure formatting stuff\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.xlim(0,1);\n",
    "plt.ylim(0,1);\n",
    "plt.xlabel('x1');\n",
    "plt.ylabel('x2')\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, \n",
    "                s=50, edgecolor='white');\n",
    "x_true = np.linspace(0, 1)\n",
    "y_true =  -(x_true * w_true[1] + w_true[0])/w_true[2]\n",
    "sns.lineplot(x=x_true, y=y_true, color='black', label='True separating hyperplane');\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning weights using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression learns the coefficient vector $w$ to minimize the loss function\n",
    "\n",
    "$$L(w) = \\sum_{i=1}^n - \\left( y_i \\log \\frac{1}{1 + e^{- \\langle w, x_i \\rangle}} + (1 - y_i) \\log \\frac{e^{- \\langle w, x_i \\rangle}}{1 + e^{- \\langle w, x_i \\rangle}} \\right) $$\n",
    "\n",
    "(Assume that the data matrix has a column of 1s at the beginning, so that the intercept can be learned the same way as the other coefficients)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use gradient descent to learn the intercept and coefficient vector, using the gradient update rule\n",
    "\n",
    "$$w_{k+1} = w_k + \\alpha \\sum_{i=1}^n (y_i - \\frac{1}{1 + e^{-\\langle w,x_i \\rangle}}) x_i ,$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug = np.hstack((np.ones((n_samples, 1)), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(w, X, y):\n",
    "  p_pred = sigmoid(np.dot(X,w))\n",
    "  loss_pos = y*np.log(p_pred)\n",
    "  loss_neg = (1-y)*np.log(1-p_pred)\n",
    "  return np.sum(-1*(loss_pos + loss_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_step(w, X, y, lr):\n",
    "  p_pred = sigmoid(np.dot(X,w)) \n",
    "  gradient = np.dot(X.T, y - p_pred)\n",
    "  w = w + lr * gradient\n",
    "  l = cross_entropy_loss(w,X,y)\n",
    "  return w, l, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = 20000\n",
    "lr = 0.005\n",
    "tol = 0.01\n",
    "w_init = np.random.randn(3)\n",
    "print(w_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_steps = np.zeros((itr, len(w_init)))\n",
    "l_steps = np.zeros(itr)\n",
    "grad_steps =  np.zeros((itr, len(w_init)))\n",
    "stop = 0\n",
    "\n",
    "w_star = w_init\n",
    "for i in range(itr):\n",
    "  w_star, loss, grad = gd_step(w_star, X_aug, y, lr)\n",
    "  w_steps[i] = w_star\n",
    "  l_steps[i] = loss\n",
    "  grad_steps[i] = grad\n",
    "  if np.linalg.norm(grad, ord=1) <= tol:\n",
    "    print(\"Stopping gradient descent at iteration %d\" % i)\n",
    "    stop = i\n",
    "    break\n",
    "  stop = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure formatting stuff\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.xlim(0,1);\n",
    "plt.ylim(0,1);\n",
    "plt.xlabel('x1');\n",
    "plt.ylabel('x2')\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, \n",
    "                s=50, edgecolor='white');\n",
    "x_true = np.linspace(0, 1)\n",
    "y_true =  -(x_true * w_true[1] + w_true[0])/w_true[2]\n",
    "sns.lineplot(x=x_true, y=y_true, color='black', label='True separating hyperplane');\n",
    "y_fit =  -(x_true * w_star[1] + w_star[0])/w_star[2]\n",
    "sns.lineplot(x=x_true, y=y_true, color='red', label='Fitted separating hyperplane');\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the logistic regression choose *this* version of the separating hyperplane?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette(\"hls\", len(w_star))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "plt.subplot(1,3,1);\n",
    "for n in range(len(w_star)):\n",
    "  sns.lineplot(x=np.arange(stop), y=w_steps[:stop,n], color=colors[n]);\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Coefficient Value\");\n",
    "\n",
    "plt.subplot(1,3, 2);\n",
    "sns.lineplot(x=np.arange(stop), y=l_steps[0:stop]);\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Cross Entropy Loss\");\n",
    "\n",
    "plt.subplot(1,3,3);\n",
    "for n in range(len(w_star)):\n",
    "  sns.lineplot(x=np.arange(stop), y=grad_steps[:stop,n], color=colors[n]);\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Gradient\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing conditional probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression learns weights, then for each point $x_{\\text{test}}$, it computes a conditional probability\n",
    "\n",
    "$$P(y_{\\text{test}} =1 | x = x_{\\text{test}}) =  \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "where $z = w_0 + w_1 x_1 + w_2 x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the probability contours\n",
    "xx, yy = np.mgrid[0:1:.01, 0:1:.01]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "#z = clf.intercept_ + np.dot(grid, clf.coef_.T)\n",
    "z = w_star[0] + np.dot(grid, w_star[1:])\n",
    "probs = 1/(1+np.exp(-z)).reshape(xx.shape)\n",
    "\n",
    "# Figure formatting stuff\n",
    "fig = plt.figure()\n",
    "plt.xlim(0,1);\n",
    "plt.ylim(0,1);\n",
    "plt.xlabel('x1');\n",
    "plt.ylabel('x2')\n",
    "\n",
    "\n",
    "# Plot conditional probabilities\n",
    "contours = plt.contourf(xx, yy, probs, 25, cmap=\"RdBu_r\",\n",
    "                      vmin=0, vmax=1);\n",
    "fig.colorbar(contours)\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, \n",
    "                s=50, edgecolor='white');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can define a threshold. A common choice is 0.5, but we can choose any threshold we like, depending on the *cost* of different types of mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure formatting stuff\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.xlim(0,1);\n",
    "plt.ylim(0,1);\n",
    "plt.xlabel('x1');\n",
    "plt.ylabel('x2')\n",
    "\n",
    "# Plot conditional probabilities\n",
    "contours = plt.contour(xx, yy, probs, levels=[0.25, 0.5, 0.75], cmap=\"RdBu_r\",\n",
    "                      vmin=0, vmax=1);\n",
    "plt.clabel(contours, colors='black', inline=True, fontsize=10)\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, \n",
    "                s=50, edgecolor='black', alpha=0.2);\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which threshold would you choose:\n",
    "\n",
    "-   If you care most about overall accuracy (ratio of correct predictions to total predictions)?\n",
    "-   If you care most about avoiding false positives (labeling a point as positive, when it belongs to the negative class)?\n",
    "-   If you care most about avoiding false negatives (labeling a point as negative, when it belongs to the positive class)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try:\n",
    "\n",
    "-   What happens as we increase $\\sigma$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tradeoff is often visualized using the *receiver operating characteristic* and the overall performance of the classifier is evaluated using the *area under the \\[ROC\\] curve*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = sigmoid(np.dot(X_aug,w_star)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y, y_hat)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure();\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc);\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--');\n",
    "plt.xlim([0.0, 1.0]);\n",
    "plt.ylim([0.0, 1.05]);\n",
    "plt.xlabel('False Positive Rate');\n",
    "plt.ylabel('True Positive Rate');\n",
    "plt.title('ROC curve');\n",
    "plt.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a Logistic Regression classifier using `sklearn`, and see if it learns the same intercept and coefficient.\n",
    "\n",
    "Like other `sklearn` models, we train the Logistic Regression using its `fit()` method and then we can predict values using `predict()`. We can also get the conditional probabilities using `predict_proba()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='none', \n",
    "                         tol=0.01, solver='saga')\n",
    "clf.fit(X, y)\n",
    "print(clf.intercept_, clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the probability contours\n",
    "xx, yy = np.mgrid[0:1:.01, 0:1:.01]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "# Figure formatting stuff\n",
    "fig = plt.figure()\n",
    "plt.xlim(0,1);\n",
    "plt.ylim(0,1);\n",
    "plt.xlabel('x1');\n",
    "plt.ylabel('x2')\n",
    "\n",
    "# Plot conditional probabilities\n",
    "contours = plt.contourf(xx, yy, probs, 25, cmap=\"RdBu_r\",\n",
    "                      vmin=0, vmax=1);\n",
    "fig.colorbar(contours)\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, \n",
    "                s=50, edgecolor='white');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure formatting stuff\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.xlim(0,1);\n",
    "plt.ylim(0,1);\n",
    "plt.xlabel('x1');\n",
    "plt.ylabel('x2')\n",
    "\n",
    "# Plot conditional probabilities\n",
    "contours = plt.contour(xx, yy, probs, levels=3, cmap=\"RdBu_r\",\n",
    "                      vmin=0, vmax=1);\n",
    "plt.clabel(contours, colors='black', inline=True, fontsize=10)\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, \n",
    "                s=50, edgecolor='black',  alpha=0.5);\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data with non-linear decision boundary\n",
    "--------------------------------------\n",
    "\n",
    "As with linear regression, we can use a logistic regression to classify data with a boundary that is linear, if we can find a transformed feature space with a linear boundary between classes.\n",
    "\n",
    "Consider the following data with a polynomial boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "sigma = 0\n",
    "coefs=np.array([0.3, 1, -1.5, -2])\n",
    "xrange=[-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_polynomial_classifier_data(n=100, xrange=[-1,1], coefs=[1,0.5,0,2], sigma=0.5):\n",
    "  x = np.random.uniform(xrange[0], xrange[1], size=(n, 2))\n",
    "  ysep = np.polynomial.polynomial.polyval(x[:,0],coefs)\n",
    "  y = (x[:,1]>ysep).astype(int)\n",
    "  x[:,0] = x[:,0] + sigma * np.random.randn(n)\n",
    "  x[:,1] = x[:,1] + sigma * np.random.randn(n)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_polynomial_classifier_data(n=n_samples, xrange=xrange, coefs=coefs, sigma=sigma)\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y);\n",
    "\n",
    "plt.xlabel('x1');\n",
    "plt.ylabel('x2')\n",
    "\n",
    "\n",
    "# Plot true function\n",
    "xtrue = np.linspace(-1, 2)\n",
    "ytrue = np.polynomial.polynomial.polyval(xtrue,coefs) \n",
    "sns.lineplot(x=xtrue, y=ytrue, color='black')\n",
    "plt.xlim((xrange[0], xrange[1]));\n",
    "plt.ylim((xrange[0], xrange[1]));\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logistic regression can only learn linear boundaries, so it does not do very well on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='none', \n",
    "                         tol=0.01, solver='saga')\n",
    "clf.fit(X, y)\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the probability contours\n",
    "xx, yy = np.mgrid[-1:1:.01, -1:1:.01]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "# Plot conditional probabilities\n",
    "contours = plt.contourf(xx, yy, probs, 25, cmap=\"RdBu_r\",\n",
    "                      vmin=0, vmax=1, alpha=0.5);\n",
    "fig.colorbar(contours)\n",
    "plt.contour(xx, yy, probs, levels=[0.5], colors='black', linestyles='dashed',\n",
    "                      vmin=0, vmax=1);\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, \n",
    "                s=50, edgecolor='white');\n",
    "\n",
    "\n",
    "sns.lineplot(x=xtrue, y=ytrue, color='black')\n",
    "\n",
    "plt.xlim((xrange[0], xrange[1]));\n",
    "plt.ylim((xrange[0], xrange[1]));\n",
    "plt.xlabel('x1');\n",
    "plt.ylabel('x2');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Here, the dashed black line shows the decision boundary, and the solid black line shows the true boundary.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we add $x_1^2$, $x_1^3$ to our model, then we can create a linear boundary between classes using a linear combination of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmax = 3\n",
    "\n",
    "X_trans = np.hstack( [X[:,0].reshape(-1,1)**d for d in np.arange(0,dmax+1)] )\n",
    "X_trans = np.hstack((X_trans, X[:,1].reshape(-1,1)))\n",
    "X_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=np.dot(X_trans[:,0:dmax+1], coefs.reshape(-1,1)).squeeze(), y=X_trans[:,-1], hue=y)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('y')\n",
    "plt.xlim((xrange[0], xrange[1]));\n",
    "plt.ylim((xrange[0], xrange[1]));\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can classify this data very well with a logistic regression on the transformed features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_trans = LogisticRegression(penalty='none', \n",
    "                         tol=0.01, solver='saga')\n",
    "clf_trans.fit(X_trans, y)\n",
    "clf_trans.score(X_trans, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the probability contours\n",
    "xx, yy = np.mgrid[-1:1:.01, -1:1:.01]\n",
    "grid = np.hstack([xx.reshape(-1,1)**d for d in np.arange(0, dmax+1)])\n",
    "grid = np.hstack([grid, yy.reshape(-1,1)])\n",
    "probs = clf_trans.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "# Figure formatting stuff\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plot conditional probabilities\n",
    "contours = plt.contourf(xx, yy, probs, 25, cmap=\"RdBu_r\",\n",
    "                      vmin=0, vmax=1, alpha=0.5);\n",
    "fig.colorbar(contours)\n",
    "plt.contour(xx, yy, probs, levels=[0.5], colors='black', linestyles='dashed',\n",
    "                      vmin=0, vmax=1);\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, \n",
    "                s=50, edgecolor='white');\n",
    "\n",
    "sns.lineplot(x=xtrue, y=ytrue, color='black', label='True polynomial separator')\n",
    "\n",
    "plt.xlim((xrange[0], xrange[1]));\n",
    "plt.ylim((xrange[0], xrange[1]));\n",
    "plt.xlabel('x1');\n",
    "plt.ylabel('x2');\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with linear regression:\n",
    "\n",
    "-   When there is under-modeling, adding transformed features can reduce bias.\n",
    "-   However, adding features also increases variance.\n",
    "-   We can use cross validation to choose a model that fits the data well but also generalizes.\n",
    "-   We can add a regularization penalty to our loss function to reduce variance."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "colab": {
   "toc_visible": "true"
  }
 }
}

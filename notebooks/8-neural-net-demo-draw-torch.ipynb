{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Neural network for classification (PyTorch version)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "*Fraida Fund*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<small>Attribution: Some parts of this notebook are written by Sundeep Rangan, from his [IntroML GitHub repo](https://github.com/sdrangan/introml/). This notebook was adapted for PyTorch by Rahul Raj at NYU.</small>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously considered a general approach to learning a non-linear function of the input data: we can supply a non-linear data representation, by applying a non-linear transformation to the input data, or specifying a non-linear kernel. Then, the machine learning model learns from the transformed data.\n",
    "\n",
    "The power of neural networks is their ability to *learn* a “transformation”, rather than having to specify it ourselves! In this demo, we will see how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw a classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, draw on the canvas to fill in the region of the feature space that should be part of the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ### Colab drawing magic\n",
    "# colab drawing magic via \n",
    "# https://gist.github.com/korakot/8409b3feec20f159d8a50b0a811d3bca\n",
    "from IPython.display import HTML, Image\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "\n",
    "canvas_html = \"\"\"\n",
    "<canvas width=%d height=%d style='border:1px solid #000000;'></canvas>\n",
    "<button>Finish</button>\n",
    "<script>\n",
    "var canvas = document.querySelector('canvas')\n",
    "var ctx = canvas.getContext('2d')\n",
    "ctx.lineWidth = %d\n",
    "ctx.lineCap = 'round'\n",
    "ctx.lineJoin = 'round'\n",
    "var button = document.querySelector('button')\n",
    "var mouse = {x: 0, y: 0}\n",
    "\n",
    "canvas.addEventListener('mousemove', function(e) {\n",
    "  mouse.x = e.pageX - this.offsetLeft\n",
    "  mouse.y = e.pageY - this.offsetTop\n",
    "})\n",
    "canvas.onmousedown = ()=>{\n",
    "  ctx.beginPath()\n",
    "  ctx.moveTo(mouse.x, mouse.y)\n",
    "  canvas.addEventListener('mousemove', onPaint)\n",
    "}\n",
    "canvas.onmouseup = ()=>{\n",
    "  canvas.removeEventListener('mousemove', onPaint)\n",
    "}\n",
    "var onPaint = ()=>{\n",
    "  ctx.lineTo(mouse.x, mouse.y)\n",
    "  ctx.stroke()\n",
    "}\n",
    "\n",
    "var data = new Promise(resolve=>{\n",
    "  button.onclick = ()=>{\n",
    "    resolve(canvas.toDataURL('image/png'))\n",
    "  }\n",
    "})\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "def draw(filename='drawing.png', w=256, h=256, line_width=8):\n",
    "  display(HTML(canvas_html % (w, h, line_width)))\n",
    "  data = eval_js(\"data\")\n",
    "  binary = b64decode(data.split(',')[1])\n",
    "  with open(filename, 'wb') as f:\n",
    "    f.write(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "im = Image.open(\"drawing.png\")\n",
    "np_im = np.array(im)\n",
    "print(np_im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_im_bw = np_im[:,:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(np_im_bw, cmap='binary');\n",
    "plt.xlim(0,255);\n",
    "plt.ylim(255,0);\n",
    "plt.xlabel('x1');\n",
    "plt.ylabel('x2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 5000\n",
    "n_val   = 1000\n",
    "n_test  = 10000\n",
    "\n",
    "X_train = np.column_stack((np.random.randint(0, np_im_bw.shape[0], size=n_train), np.random.randint(0, np_im_bw.shape[1], size=n_train)))\n",
    "y_train = np.rot90(np_im_bw, k=3)[X_train[:,0], X_train[:,1]]\n",
    "\n",
    "X_train = X_train/255.0\n",
    "y_train = y_train/255.0\n",
    "\n",
    "\n",
    "X_val = np.column_stack((np.random.randint(0, np_im_bw.shape[0], size=n_val), np.random.randint(0, np_im_bw.shape[1], size=n_val)))\n",
    "y_val = np.rot90(np_im_bw, k=3)[X_val[:,0], X_val[:,1]]\n",
    "\n",
    "X_val = X_val/255.0\n",
    "y_val = y_val/255.0\n",
    "\n",
    "\n",
    "X_test = np.column_stack((np.random.randint(0, np_im_bw.shape[0], size=n_test), np.random.randint(0, np_im_bw.shape[1], size=n_test)))\n",
    "y_test = np.rot90(np_im_bw, k=3)[X_test[:,0], X_test[:,1]]\n",
    "\n",
    "X_test = X_test/255.0\n",
    "y_test = y_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.scatter(x=X_train[:,0], y=X_train[:,1], c=y_train, cmap='binary', edgecolors= \"gray\")\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");\n",
    "plt.title(\"Training data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will train a model using Pytorch. You can refer to the [official documentation](https://pytorch.org/) for extra details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see if we can build a simple neural network classifier to learn the decision region that we drew above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a new class which inherits from a parent class, `nn.Module`: we name this class `SimpleNN`, but we could name it anything we want.\n",
    "\n",
    "Within this class, we need to define:\n",
    "\n",
    "-   a `__init__` function that initializes the neural network parameters. We will have two [`nn.Linear` layers](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) - meaning that the input to the layer is computed as the weighed sum of the outputs of the previous layer.\n",
    "    -   The first layer is a hidden layer with `nh=4` hidden units,\n",
    "    -   and there is an output layer with `nout=1` output units corresponding to the estimated class label.\n",
    "-   a `forward` function. This describes what the network should do in a forward step - in this case, it will apply the sigmoid activation function to its output.\n",
    "\n",
    "All of the layers are dense or fully connected layers, meaning that each node has a link to every node in the adjacent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nin = 2   # dimension of input data\n",
    "nh = 4    # number of hidden units\n",
    "nout = 1  # number of outputs = 1 since this is binary\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "\n",
    "    def __init__(self, nin, nh, nout):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden = nn.Linear(nin, nh)\n",
    "        self.output = nn.Linear(nh, nout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.hidden(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(nin, nh, nout)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our visualization of the network architechture, we will also add a *bias node* at each layer. This simplifies the computation of weights by adding an extra input whose value is always 1. The bias term then comes from the weight applied to that input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Visualize the network architecture\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "inputLayerSize  = nin\n",
    "outputLayerSize = nout\n",
    "hiddenLayerSize = nh\n",
    "\n",
    "\n",
    "nodePos = {}\n",
    "G=nx.Graph()\n",
    "graphHeight = max(inputLayerSize, outputLayerSize, hiddenLayerSize)\n",
    "\n",
    "# create nodes and note their positions\n",
    "for n in range(inputLayerSize):\n",
    "  nodePos['x'+str(n+1)]=(1, n)\n",
    "  G.add_node('x'+str(n+1))\n",
    "for n in range(outputLayerSize):\n",
    "  nodePos['o'+str(n+1)]=(5, n)\n",
    "  G.add_node('o'+str(n+1))\n",
    "for n in range(hiddenLayerSize):\n",
    "  nodePos['h'+str(n+1)]=(3, n)\n",
    "  G.add_node('h'+str(n+1))\n",
    "\n",
    "# add edges\n",
    "for n in range(hiddenLayerSize):\n",
    "  for m in range(inputLayerSize):\n",
    "    G.add_edge('x' + str(m+1), 'h' + str(n+1))\n",
    "  for m in range(outputLayerSize):\n",
    "    G.add_edge('h' + str(n+1), 'o' + str(m+1))\n",
    "\n",
    "# add bias nodes\n",
    "\n",
    "nodePos['xb']=(1, inputLayerSize)\n",
    "G.add_node('xb')\n",
    "for n in range(hiddenLayerSize):\n",
    "  G.add_edge('xb', 'h' + str(n+1))\n",
    "\n",
    "nodePos['hb']=(3, hiddenLayerSize)\n",
    "G.add_node('hb')\n",
    "for n in range(outputLayerSize):\n",
    "  G.add_edge('hb', 'o' + str(n+1))\n",
    "\n",
    "nx.draw_networkx(G, pos=nodePos,\n",
    "              node_size=1000, node_color='pink')\n",
    "plt.axis('off');\n",
    "plt.margins(0.2, 0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network needs to learn a weight for each link in the image above (including the weights for the bias nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we have to select an optimizer and a loss function.\n",
    "\n",
    "Since this is a binary classification problem, we select the [binary cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html).\n",
    "\n",
    "We will also choose an [optimizer (Adam)](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) and a learning rate of learning_rate=0.01.\n",
    "\n",
    "<small>Reference: [pytorch loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions), [optimizers](https://pytorch.org/docs/stable/optim.html)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass data to our model, we will prepare a [`DataLoader`](https://pytorch.org/docs/stable/data.html) - this will iterate over the data and “batch” it for us according to the batch size we specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "batch_size = 24\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train)),\n",
    "    batch_size = batch_size, shuffle=True)\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(torch.Tensor(X_val), torch.Tensor(y_val)),\n",
    "    batch_size = batch_size, shuffle=False)\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test)),\n",
    "    batch_size = batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define our training function! On each iteration of the loop, we:\n",
    "\n",
    "-   Get a batch of training data from the `train_loader`.\n",
    "-   Zero the gradients of the `optimizer`. (This is necessary because by default, they accumulate.)\n",
    "-   Do a forward pass on the batch of training data.\n",
    "-   Use the predictions from this forward pass to compute the loss.\n",
    "-   Then, do a backwards pass where we compute the gradients.\n",
    "-   Update the weights of the optimizer using these gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(data_loader):\n",
    "\n",
    "    # Put model in training mode\n",
    "    model.train(True)\n",
    "\n",
    "    running_loss    = 0\n",
    "    running_correct = 0\n",
    "    running_samples = 0\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "        # Every data instance is an X, y pair\n",
    "        X, y = data\n",
    "        y = y.unsqueeze(1) # make it the same shape as predictions\n",
    "\n",
    "        # Zero gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass makes predictions for this batch\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running loss, accuracy, and number of samples\n",
    "        running_correct += ( (y_pred >= 0.5) == y).sum().item()\n",
    "        running_samples += y.size(0)\n",
    "        running_loss    += loss*y.size(0)\n",
    "\n",
    "    # return average loss, average accuracy\n",
    "    return float(running_loss/running_samples), float(running_correct/running_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define a function for evaluating the model without training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(data_loader):\n",
    "\n",
    "    running_loss    = 0\n",
    "    running_correct = 0\n",
    "    running_samples = 0\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation for faster computation/reduced memory\n",
    "    with torch.no_grad():\n",
    "\n",
    "      for i, data in enumerate(data_loader):\n",
    "          # Every data instance is an X, y pair\n",
    "          X, y = data\n",
    "          y = y.unsqueeze(1) # make it the same shape as predictions\n",
    "\n",
    "          # Forward pass makes predictions for this batch\n",
    "          y_pred = model(X)\n",
    "\n",
    "          # Compute the loss\n",
    "          loss = loss_fn(y_pred, y)\n",
    "\n",
    "          # Update running loss, accuracy, and number of samples\n",
    "          running_correct += ( (y_pred.data >= 0.5) == y).sum().item()\n",
    "          running_samples += y.size(0)\n",
    "          running_loss    += loss*y.size(0)\n",
    "\n",
    "    # return average loss, average accuracy\n",
    "    return float(running_loss/running_samples), float(running_correct/running_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will loop over epochs, train the model for one epoch, and then evaluate its performance on the validation data at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "metrics = {'train_losses': [], 'train_accuracies': [], 'val_losses': [], 'val_accuracies': []}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # Train on training data\n",
    "    train_loss, train_accuracy = train_one_epoch(train_loader)\n",
    "    metrics['train_losses'].append(train_loss)\n",
    "    metrics['train_accuracies'].append(train_accuracy)\n",
    "\n",
    "    # Evaluate on validation data\n",
    "    val_loss, val_accuracy = eval_model(val_loader)\n",
    "    metrics['val_losses'].append(val_loss)\n",
    "    metrics['val_accuracies'].append(val_accuracy)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{n_epochs} - Loss: {train_loss:.4f} - Accuracy: {train_accuracy:.4f} - Val_Loss: {val_loss:.4f} - Val_Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize output of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the classification rule our neural network learned, we can plot the predicted class probability over the input space.\n",
    "\n",
    "We can also plot the response in the each of the hidden units.\n",
    "\n",
    "Each of the hidden units produces one linear decision region. The final nonlinear region is then formed by taking a weighted combination of these regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Visualize output of each unit\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Get output layer weights\n",
    "output_layer = model.output\n",
    "\n",
    "# Generate a grid of points\n",
    "n_plot = 256\n",
    "xx1, xx2 = np.meshgrid(np.linspace(0, 1, n_plot), np.linspace(0, 1, n_plot))\n",
    "X_grid = np.hstack((xx1.reshape(n_plot**2, 1), xx2.reshape(n_plot**2, 1)))\n",
    "X_grid_tensor = torch.tensor(X_grid, dtype=torch.float32)\n",
    "\n",
    "# Function to capture outputs of the desired layer\n",
    "def get_activation(layer, input, output):\n",
    "    global activation\n",
    "    activation = torch.sigmoid(output)\n",
    "\n",
    "# Register hook for hidden layer\n",
    "hook_handle = model.hidden.register_forward_hook(get_activation)\n",
    "\n",
    "# Pass the grid through the model\n",
    "with torch.no_grad():\n",
    "    output = model(X_grid_tensor)\n",
    "\n",
    "# Remove the hook\n",
    "hook_handle.remove()\n",
    "\n",
    "# Reshape the activations and output to plot them\n",
    "activation = activation.numpy().reshape((n_plot, n_plot, -1))\n",
    "output = output.numpy().reshape((n_plot, n_plot))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, nh+1, figsize=(nh*5, 4))\n",
    "\n",
    "# Plot for each hidden unit with grayscale colormap\n",
    "for i in range(activation.shape[2]):\n",
    "    axes[i].imshow(activation[:, :, i], cmap='gray_r', origin='lower', extent=[0,1,0,1])\n",
    "    axes[i].set_title(f'h{i+1}, w_h{i+1},o={output_layer.weight.data.numpy()[0,i]:.2f}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Plot for output layer with grayscale colormap\n",
    "axes[-1].imshow(output, cmap='gray_r', origin='lower', extent=[0,1,0,1])\n",
    "axes[-1].set_title(f\"Output, w_hb,o={output_layer.bias.data.numpy()[0]:.2f}\")\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check all of the parameters learned by the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing weights and biases of the hidden layer\n",
    "hidden_layer = model.hidden\n",
    "Wh = hidden_layer.weight.data.numpy()\n",
    "bh = hidden_layer.bias.data.numpy()\n",
    "\n",
    "print('Hidden Layer Weights (Wh)=\\n', Wh)\n",
    "print('Hidden Layer Biases (bh)=\\n', bh)\n",
    "\n",
    "# Accessing weights and biases of the output layer\n",
    "output_layer = model.output\n",
    "Wo = output_layer.weight.data.numpy()\n",
    "bo = output_layer.bias.data.numpy()\n",
    "\n",
    "print('Output Layer Weights (Wo)=\\n', Wo)\n",
    "print('Output Layer Biases (bo)=\\n', bo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can visualize the training progress vs. epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(7, 3))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "nepochs = len(metrics['train_accuracies'])\n",
    "sns.lineplot(x=np.arange(1, nepochs+1), y=metrics['train_accuracies'], label='Training Accuracy')\n",
    "sns.lineplot(x=np.arange(1, nepochs+1), y=metrics['val_accuracies'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.lineplot(x=np.arange(1, nepochs+1), y=metrics['train_losses'], label='Training Loss')\n",
    "sns.lineplot(x=np.arange(1, nepochs+1), y=metrics['val_losses'], label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = eval_model(test_loader)\n",
    "print(f'Accuracy on the test set: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_all = []\n",
    "y_all      = []\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        y_pred = model(X)\n",
    "        y_pred_label = (y_pred.data > 0.5).float()\n",
    "        y_pred_all.extend(y_pred_label.numpy())\n",
    "        y_all.extend(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=np.array(y_pred_all).flatten(), cmap='binary', edgecolors=\"gray\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Predictions for test data\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=np.array(y_all).flatten(), cmap='binary', edgecolors=\"gray\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Actual test data\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to try\n",
    "\n",
    "-   What happens if we use linear activations, instead of sigmoid?\n",
    "-   When do we need to use a large number of hidden units?\n",
    "-   Try to adjust the learning rate and batch size. What is the effect?\n",
    "-   What should you do if you want to train the model for a few more epochs? What should you do if you want to train the model from scratch?"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 }
}

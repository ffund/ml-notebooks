{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Order Selection for Neural Data\n",
    "====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**: \\[Fill in your name here\\]\n",
    "\n",
    "**Net ID**: \\[Fill in your net ID here\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attribution**: This notebook is a slightly adapted version of the [model order selection lab assignment](https://github.com/sdrangan/introml/blob/master/unit04_model_sel/lab_neural_partial.ipynb) by Prof. Sundeep Rangan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is a key tool for neuroscientists to understand how sensory and motor signals are encoded in the brain. In addition to improving our scientific understanding of neural phenomena, understanding neural encoding is critical for brain machine interfaces. In this notebook, you will use model selection for performing some simple analysis on real neural signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data\n",
    "----------------\n",
    "\n",
    "The data in this lab comes from neural recordings described in:\n",
    "\n",
    "<a href=\"http://jn.physiology.org/content/106/2/764.short\"> Stevenson, Ian H., et al. “Statistical assessment of the stability of neural movement representations.” Journal of neurophysiology 106.2 (2011): 764-774</a>\n",
    "\n",
    "Neurons are the basic information processing units in the brain. Neurons communicate with one another via *spikes* or *action potentials* which are brief events where voltage in the neuron rapidly rises then falls. These spikes trigger the electro-chemical signals between one neuron and another. In this experiment, the spikes were recorded from 196 neurons in the primary motor cortex (M1) of a monkey using an electrode array implanted onto the surface of a monkey's brain. During the recording, the monkey performed several reaching tasks and the position and velocity of the hand was recorded as well.\n",
    "\n",
    "The goal of the experiment is to try to *read the monkey's brain*: That is, predict the hand motion from the neural signals from the motor cortex.\n",
    "\n",
    "We first load the key packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full data is available on the CRCNS website <a href=\"http://crcns.org/data-sets/movements/dream\" class=\"uri\">http://crcns.org/data-sets/movements/dream</a>. However, the raw data files can be quite large. To make the lab easier, the [Kording lab](http://kordinglab.com/) at UPenn has put together an excellent [repository](https://github.com/KordingLab/Neural_Decoding) where they have created simple pre-processed versions of the data. You can download the file `example_data_s1.pickle` from the [Dropbox link](https://www.dropbox.com/sh/n4924ipcfjqc0t6/AADOv9JYMUBK1tlg9P71gSSra/example_data_s1.pickle?dl=0). Alternatively, you can directly run the following command. This may take a little while to download since the file is 26 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://www.dropbox.com/sh/n4924ipcfjqc0t6/AADOv9JYMUBK1tlg9P71gSSra/example_data_s1.pickle?dl=1' -O example_data_s1.pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is a *pickle* data structure, which uses the Python package `pickle` to serialize Python objects into data files. Once you have downloaded the file, you can run the following command to retrieve the data from the pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example_data_s1.pickle', 'rb') as fp:\n",
    "    X,y = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix `X` is matrix of spike counts from different neurons, where `X[i,j]` is the number of spikes from neuron `j` in time bin `i`.\n",
    "\n",
    "The matrix `y` has two columns:\n",
    "\n",
    "-   `y[i,0] =` velocity of the monkey's hand in the x-direction in time bin `i`\n",
    "-   `y[i,1] =` velocity of the monkey's hand in the y-direction in time bin `i`\n",
    "\n",
    "Our goal will be to predict `y` from `X`.\n",
    "\n",
    "Each time bin represent `tsamp=0.05` seconds of time. Using `X.shape` and `y.shape`, we can compute and print:\n",
    "\n",
    "-   `nt =` the total number of time bins\n",
    "-   `nneuron =` the total number of neurons\n",
    "-   `nout =` the total number of output variables to track = number of columns in `y`\n",
    "-   `ttotal =` total time of the experiment is seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsamp = 0.05  # sampling time in seconds\n",
    "\n",
    "nt, nneuron = X.shape\n",
    "nout = y.shape[1]\n",
    "ttotal = nt*tsamp\n",
    "\n",
    "print('Number of neurons = %d' % nneuron)\n",
    "print('Number of time samples = %d' % nt)\n",
    "print('Number of outputs = %d' % nout)\n",
    "print('Total time (secs) = %f' % ttotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can plot the velocity against time, for each direction, for the first 1000 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cutoff = 1000\n",
    "directions = ['x', 'y']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(15,7))\n",
    "for n in range(nout):\n",
    "  sns.lineplot(x=np.arange(0, t_cutoff)*tsamp, y=y[0:t_cutoff, n], label=directions[n], ax=axes[n]);\n",
    "\n",
    "  axes[n].set_ylabel(\"Velocity\")\n",
    "  axes[n].set_xlabel(\"Time (s)\")\n",
    "  axes[n].set_ylim(-50,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also “zoom in” on a small slice of time in which the monkey is moving the hand, and see the neural activity at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = 490\n",
    "t_end = 580\n",
    "\n",
    "fig, axes = plt.subplots(nrows=7, ncols=8, figsize=(15,15))\n",
    "\n",
    "# Setting the range for all axes\n",
    "plt.setp(axes, xlim=(t_start, t_end), ylim=(0,12));\n",
    "\n",
    "for n in range(nout):\n",
    "  sns.lineplot(x=np.arange(t_start, t_end)*tsamp, y=y[t_start:t_end, n], ax=axes[n//2,n%2], color='red', label=directions[n])\n",
    "  plt.setp(axes[n//2,n%2], xlim=(t_start*tsamp, t_end*tsamp), ylim=(-50, +50));\n",
    "\n",
    "for n in range(nneuron):\n",
    "  sns.lineplot(x=np.arange(t_start, t_end)*tsamp, y=X[t_start:t_end, n], ax=axes[(n+2)//8,(n+2)%8], label=\"n%d\" % n, color='grey')\n",
    "  plt.setp(axes[(n+2)//8,(n+2)%8], xlim=(t_start*tsamp, t_end*tsamp), ylim=(0, +15));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a linear model\n",
    "----------------------\n",
    "\n",
    "Let’s first try a linear regression model to fit the data.\n",
    "\n",
    "To start, we will split the data into a training set and a test set. We’l fit the model on the training set and then use the test set to estimate the model performance on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To shuffle or not to shuffle?**\n",
    "\n",
    "The `train_test_split` function has an optional `shuffle` argument.\n",
    "\n",
    "-   If you use `shuffle=False`, then `train_test_split` will take the first part of the data as the training set and the second part of the data as the test set, according to the ratio you specify in `test_size` or `train_size`.\n",
    "-   If you use `shuffle=True`, then `train_test_split` will first randomly shuffle the data. Then, it will take the first part of the *shuffled* data as the training set and the second part of the *shuffled* data as the test set, according to the ratio you specify in `test_size` or `train_size`.\n",
    "\n",
    "According to the function [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), by default, `shuffle` is `True`:\n",
    "\n",
    "> **shuffle: bool, default=True**\n",
    ">\n",
    "> Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.\n",
    "\n",
    "so if you do not specify anything related to `shuffle`, your data will be randomly shuffled before it is split into training and test data.\n",
    "\n",
    "Under what conditions should you shuffle data? Suppose your dataset includes samples of a medical experiment on 1000 subjects, and the first 500 samples in the data are from male subjects while the second 500 samples are from female subjects. If you set `shuffle=False`, then your training set would have a much higher proportion of male subjects than your test set (with the specific numbers depending on the ratio you specify).\n",
    "\n",
    "On the other hand, suppose your dataset includes stock prices at closing time, with each sample representing a different date (in order). If you allow `train_test_split` to shuffle the data, then your model will be allowed to “learn” stock prices using prices from the day *after* the one it is trying to predict! Obviously, your model won’t be able to learn from future dates in production, so it shouldn’t be allowed to in the evaluation stage, either. (Predicting the past using the future is considered a type of data leakage.)\n",
    "\n",
    "With this in mind, it is usually inappropriate to shuffle time series data when splitting it up into smaller sets for training, validation, or testing.\n",
    "\n",
    "(There are more sophisticated ways to handle splitting time series data, but for now, splitting it up the usual way, just without shuffling first, will suffice.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the discussion above, use the `train_test_split` function to split the data into training and test sets, but with no shuffling. Let `Xtr,ytr` be the training data set and `Xts,yts` be the test data set. Use `test_size=0.33` so 1/3 of the data is used for evaluating the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split data intro training and test sets\n",
    "# Xtr ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit a linear regression on the training data `Xtr,ytr`. Make a prediction `yhat` using the test data, `Xts`. Compare `yhat` to `yts` to measure `rsq`, the R2 value. Use the sklearn `r2_score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit a linear model\n",
    "# yhat = ...\n",
    "# rsq = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the `rsq` value. You should get `rsq` of around `0.45`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsq```\n",
    ":::\n",
    "\n",
    "\n",
    "::: {.cell .markdown }\n",
    "It is useful to plot the predicted vs. actual values. \n",
    "Use the test data for this visualization.\n",
    "Create a scatter plot of predicted values ($\\hat{y}$) on the\n",
    "vertical axis, and actual values ($y$) on the horizontal axis. \n",
    "Since we have two predicted values for each sample - \n",
    "the velocity in the X direction and the velocity in the Y direction - \n",
    "you should make two subplots,\n",
    "\n",
    "-   one of predicted X direction vs. actual X direction,\n",
    "-   one of predicted Y direction vs. actual Y direction\n",
    "\n",
    "Make sure both axes use the same scale (the range of the vertical axis should be\n",
    "the same as the range of the vertical axis) *and* that all three\n",
    "subplots use the same scale. Label each axes, and each plot \n",
    "(indicate which plot shows the velocity in the X direction and \n",
    "which shows the velocity in the Y direction!)\n",
    "\n",
    ":::\n",
    "\n",
    "::: {.cell .code }\n",
    "```python\n",
    "# TODO: Predicted values vs true values visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be useful to visualize the actual and predicted values over time, for a slice of time. Use the test data for this visualization. Create two subplots, both with time on the horizontal axis, but only including *the first 1000 rows* (50 seconds) in the data. On the vertical axis,\n",
    "\n",
    "-   for one subplot: show the actual X direction as a line of one color, and the predicted X direction as a line of another color.\n",
    "-   for the second subplot: show the actual Y direction as a line of one color, and the predicted Y direction as a line of another color.\n",
    "\n",
    "Make sure to carefully label each axis (including units on the time axis!), and label the data series (i.e. which color is the actual value and which is the predicted value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predicted and true values over time visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on this plot - does the model predict the hand velocity well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a model with delay\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we can improve the model accuracy is to add features using delayed version of the existing features.\n",
    "\n",
    "Specifically, the model we used above tries to predict velocity in direction $k$ at time $i$ using\n",
    "\n",
    "$$\\hat{y}_{i,k} = w_{k,0} + \\sum_{d=1}^{\\text{nneuron}} w_{k,d} X_{i,d}  $$\n",
    "\n",
    "In this model, $\\hat{y}_{i,k}$ at the $i$th time bin was only dependent on $X_i$, the number of spikes of each neuron in time bin $i$. In signal processing, this is called a *memoryless* model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in many physical systems, such as those that arise in neuroscience, there is a delay between the inputs and outputs. To model this effect, we could add additional features to each row of data, representing the number of spikes of each neuron in the *previous* row. Then, the output at time $i$ would be modeled as the effect of the neurons firing in time $i$ *and* the effect of the neurons firing in time $i-1$.\n",
    "\n",
    "We wouldn’t be able to use data from the past for the first row of data, since we don’t *have* data about neurons firing in the previous time step. But we can drop that row. If our original data matrix had `nt` rows and `nneuron` columns, our data matrix with delayed features would have `nt - 1` rows and `nneuron + 1 x nneuron` columns. (The first `nneuron` columns represent the number of spikes in each neuron for the current time, the next `nneuron` columns represent the number of spikes in each neuron for the previous time.)\n",
    "\n",
    "Furthermore, we can “look back” any number of time steps, so that the output at time $i$ is modeled as the effect of the neurons firing in time $i$, the neurons firing in time $i-1$, ..., all the way up to the effect of the neurons firing in time $i- \\text{dly}$ (where $\\text{dly}$ is the maximum number of time steps we're going to “look back” on). Our data matrix with the additional delayed features would have `nt - dly` rows and `nneuron + dly x nneuron` columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a function that accepts `X` and `y` data and a `dly` argument, and returns `X` and `y` with delayed features up to `dly` time steps backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dly_data(X,y,dly):\n",
    "    \"\"\"\n",
    "    Create delayed data\n",
    "    \"\"\"    \n",
    "    n,p = X.shape\n",
    "    Xdly = np.zeros((n-dly,(dly+1)*p))\n",
    "    for i in range(dly+1):\n",
    "        Xdly[:,i*p:(i+1)*p] = X[dly-i:n-i,:]\n",
    "    ydly = y[dly:]\n",
    "    \n",
    "    return Xdly, ydly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convince yourself that this works, try creating a data matrix that includes delayed features one time step back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dly1, y_dly1 = create_dly_data(X, y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the dimensions have changed, as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of original data matrix\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of data matrix with delayed features 1 time step back\n",
    "X_dly1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check row 0 in the matrix with delayed features, and verify that it is the concatenation of row 1 and row 0 in the original data matrix. (Note that row 0 in the matrix with delayed features corresponds to row 1 in the original data matrix.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dly1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack((X[1], X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dly1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit an linear delayed model with `dly=2` delay lags. That is,\n",
    "\n",
    "-   Create delayed data by calling `create_dly_data` with `dly=2`\n",
    "-   Split the data (with the extra delay features!) into training and test as before (again, do not shuffle the data, and use a test size of 0.33)\n",
    "-   Fit the model on the training data\n",
    "-   Use the model to predict the values for the test data and save the result in `yhat`\n",
    "-   Measure the R2 score on the test data and save the result in `rsq`\n",
    "\n",
    "If you did this correctly, you should get a new R2 score around 0.60. This is significantly better than the memoryless model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit a linear model with dly=2\n",
    "\n",
    "# Create the delayed data\n",
    "\n",
    "# Split into training and test\n",
    "\n",
    "# Create linear regression object\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "# Predict values for test data\n",
    "\n",
    "# Measure the new r2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, plot the predicted vs. true values, but for the model with `dly=2`, with one subplot for X velocity and one subplot for Y velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predicted values vs true values visualization with dly=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also as you did before, plot the actual and predicted valuer over time for the first 1000 samples, for the model with `dly=2`. Does the model predict the hand velocity well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predicted and true values over time visualization with dly=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the optimal delay with K-fold CV\n",
    "------------------------------------------\n",
    "\n",
    "In the previous example, we fixed `dly=2`. We can now select the optimal delay using K-fold cross validation.\n",
    "\n",
    "Since we have a large number of data samples, it turns out that the optimal model order uses a very high delay. Using the above fitting method, the computations take too long. So, to simplify things, we will just pretend that we have a very limited data set.\n",
    "\n",
    "We will compute `Xred` and `yred` by taking the first `nred=6000` samples of the data `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nred = 6000\n",
    "\n",
    "Xred = X[:nred]\n",
    "yred = y[:nred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: since we are only using the first 6000 samples to train the model and select the best `dly`, there are plenty of samples left out as the test set. We don’t need to (and shouldn’t) further divide these 6000 samples into training and test sets - we can use all of it for model training and model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at model orders up to `dmax=15`. We will create a delayed data matrix, `Xdly,ydly`, using `create_dly_data` with the reduced data `Xred,yred` and `dly=dmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmax = 15\n",
    "\n",
    "Xdly, ydly = create_dly_data(Xred,yred,dmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "::: {.cell .markdown }\n",
    "Note that we can use `Xdly, ydly` to get a data matrix for any delay *up\n",
    "to* `dmax`, not only for delay = `dmax`. For example, to get a data\n",
    "matrix with delay = 1:\n",
    ":::\n",
    "\n",
    "::: {.cell .code }\n",
    "```python\n",
    "dtest = 1\n",
    "X_dtest = Xdly[:,:(dtest+1)*nneuron]\n",
    "X_dtest.shape\n",
    "```\n",
    ":::\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use K-fold CV with `nfold=10` to find the optimal delay, for all the values of delay in `dtest_list`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest_list = np.arange(0, dmax+1)\n",
    "nd = len(dtest_list)\n",
    "\n",
    "print(dtest_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can refer to the example in the “Model order selection” section of the demo notebook. But, make sure to use `shuffle=False` in your `KFold` object, since for this example it would be inappropriate to shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use K-fold CV to select dly\n",
    "\n",
    "# Number of folds\n",
    "nfold = 10\n",
    " \n",
    "#  Create a k-fold object\n",
    "# kf = KFold(...)\n",
    " \n",
    "# Initialize a matrix Rsq to hold values of the R^2 across the model orders and folds.\n",
    "Rsq = np.zeros((nd,nfold))\n",
    " \n",
    "# Loop over the folds\n",
    "for i, idx_split in enumerate(kf.split(Xdly)):\n",
    "    \n",
    "    # Get the training and validation data in the split\n",
    "    idx_tr, idx_val = idx_split        \n",
    " \n",
    "    for it, dtest in enumerate(dtest_list):\n",
    "        # DO NOT call create_dly_data again\n",
    "        # just select the appropriate subset of columns of Xdly\n",
    "        # X_dtest = Xdly with the columns corresponding to only the `dtest+1` most recent times.\n",
    " \n",
    "        # Split the data (X_dtest,ydly) into training and validation\n",
    "        # using idx_tr and idx_val\n",
    "        # Xtr = ...\n",
    "        # ytr = ...\n",
    "        # Xval = ...\n",
    "        # yval = ...\n",
    " \n",
    "        # Fit linear regression on training data\n",
    " \n",
    "        #  Measure the R2 on validation data and store in the matrix Rsq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to find the delay that has the best mean validation R2. Get the best delay according to the “best R2” rule, and save it in `rsq_opt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use K-fold CV (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write code to find the best delay using the one SE rule (i.e. find the simplest model whose validation R2 is within one SE of the model with the best R2). Get the best delay according to the “one SE rule”, and save it in `rsq_one_se`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use K-fold CV (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the mean and standard error of the validation R2 values for each model (each delay value) as a function of the delay. Use a `plt.errorbar` plot, as shown in the “Model selection using 1-SE rule” section of the demo notebook. Label each axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize mean and SE of R2 across folds for each dly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "::: {.cell .markdown }\n",
    "## Fitting the selected model\n",
    ":::\n",
    "\n",
    "::: {.cell .markdown }\n",
    "Now that we have selected a model order, we can fit the (reduced) data\n",
    "to that model.\n",
    "\n",
    "Use your `Xdly` and `ydly` to fit a linear regression model using the\n",
    "best delay according to the one SE rule.\n",
    ":::\n",
    "\n",
    "::: {.cell .code }\n",
    "```python\n",
    "# TODO 15\n",
    "```\n",
    ":::\n",
    "\n",
    "::: {.cell .markdown }\n",
    "Then, define a test set using data that was not used to train the model:\n",
    ":::\n",
    "\n",
    "::: {.cell .code }\n",
    "```python\n",
    "# TODO 16\n",
    "# if dopt_one_se is the optimal model order, you can use\n",
    "# Xts = X[nred+1:nred+1001+dopt_one_se]\n",
    "# yts = y[nred+1:nred+1001+dopt_one_se]\n",
    "# and then use \n",
    "# Xts_dly, yts_dly = create_dly_data(Xts,yts,dopt_one_se)\n",
    "```\n",
    ":::\n",
    "\n",
    "::: {.cell .markdown }\n",
    "Use your fitted model to find the R2 score on the test set.\n",
    ":::\n",
    "\n",
    "::: {.cell .code }\n",
    "```python\n",
    "# TODO 17\n",
    "```\n",
    ":::\n",
    "\n",
    "::: {.cell .markdown }\n",
    "Also plot the actual and predicted values over time for the first 1000\n",
    "samples of the *test* data (similar to your plots in the previous\n",
    "sections). Comment on this plot - does the model predict the hand\n",
    "velocity well?\n",
    ":::\n",
    "\n",
    "::: {.cell .code }\n",
    "```python\n",
    "# TODO 18\n",
    "```\n",
    ":::\n",
    "\n",
    "-->"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "file_extension": ".py",
   "pygments_lexer": "ipython3",
   "mimetype": "text/x-python",
   "version": "3.6.7",
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   }
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 }
}

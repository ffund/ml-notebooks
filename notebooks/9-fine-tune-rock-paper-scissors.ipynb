{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Transfer learning\n",
    "\n",
    "*Fraida Fund*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, for most machine learning problems, you wouldn’t design or train a convolutional neural network from scratch - you would use an existing model that suits your needs (does well on ImageNet, size is right) and fine-tune it on your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for faster training, use Runtime \\> Change Runtime Type to run this notebook on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import platform\n",
    "import datetime\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "print('Python version:', platform.python_version())\n",
    "print('Tensorflow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “rock paper scissors” dataset is available directly from the `tensorflow_datasets` package. In the cells that follow, we’ll get the data, plot a few examples, and also prepare a preprocessing function (which we won’t apply yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'rock_paper_scissors',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,  # ensures output is (image, label)\n",
    "    with_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = tfds.show_examples(ds_train, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(['rock', 'paper', 'scissors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the base models that comes in Keras has its own `preprocess_input` function. We’re going to use a [MobileNetV2](https://keras.io/api/applications/mobilenet/#mobilenetv2-function) model (which expects image data to have values in the -1 to 1 range), so we will use its specific preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IMG_SIZE = 224\n",
    "INPUT_IMG_SHAPE = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, (INPUT_IMG_SIZE, INPUT_IMG_SIZE))\n",
    "    # Preprocessing for MobileNetV2 - scales to whatever range MobileNetV2 expects\n",
    "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)  \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won’t apply this preprocessing to our dataset yet, but we will get this function ready so that we can apply it to a test sample very soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload custom test sample\n",
    "\n",
    "This code expects a PNG image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    " \n",
    "# Edit the filename here as needed\n",
    "filename = 'scissors.png'\n",
    " \n",
    "# pre-process image\n",
    "image = Image.open(filename).convert('RGB')\n",
    "image_resized = image.resize((INPUT_IMG_SIZE, INPUT_IMG_SIZE), Image.BICUBIC)\n",
    "test_sample = np.array(image_resized)\n",
    "test_sample = test_sample.reshape(1, INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(4,4));\n",
    "plt.imshow(test_sample.reshape(INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify with MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras Applications](https://keras.io/api/applications/) are pre-trained models with saved weights, that you can download and use without any additional training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a table of the models available as Keras Applications.\n",
    "\n",
    "In this table, the top-1 and top-5 accuracy refer to the model's performance on the ImageNet validation dataset, and depth is the depth of the network including activation layers, batch normalization layers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Model</th>\n",
    "<th align=\"right\">Size</th>\n",
    "<th align=\"right\">Top-1 Accuracy</th>\n",
    "<th align=\"right\">Top-5 Accuracy</th>\n",
    "<th align=\"right\">Parameters</th>\n",
    "<th align=\"right\">Depth</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Xception</td>\n",
    "<td align=\"right\">88 MB</td>\n",
    "<td align=\"right\">0.790</td>\n",
    "<td align=\"right\">0.945</td>\n",
    "<td align=\"right\">22,910,480</td>\n",
    "<td align=\"right\">126</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>VGG16</td>\n",
    "<td align=\"right\">528 MB</td>\n",
    "<td align=\"right\">0.713</td>\n",
    "<td align=\"right\">0.901</td>\n",
    "<td align=\"right\">138,357,544</td>\n",
    "<td align=\"right\">23</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>VGG19</td>\n",
    "<td align=\"right\">549 MB</td>\n",
    "<td align=\"right\">0.713</td>\n",
    "<td align=\"right\">0.900</td>\n",
    "<td align=\"right\">143,667,240</td>\n",
    "<td align=\"right\">26</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet50</td>\n",
    "<td align=\"right\">98 MB</td>\n",
    "<td align=\"right\">0.749</td>\n",
    "<td align=\"right\">0.921</td>\n",
    "<td align=\"right\">25,636,712</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet101</td>\n",
    "<td align=\"right\">171 MB</td>\n",
    "<td align=\"right\">0.764</td>\n",
    "<td align=\"right\">0.928</td>\n",
    "<td align=\"right\">44,707,176</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet152</td>\n",
    "<td align=\"right\">232 MB</td>\n",
    "<td align=\"right\">0.766</td>\n",
    "<td align=\"right\">0.931</td>\n",
    "<td align=\"right\">60,419,944</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet50V2</td>\n",
    "<td align=\"right\">98 MB</td>\n",
    "<td align=\"right\">0.760</td>\n",
    "<td align=\"right\">0.930</td>\n",
    "<td align=\"right\">25,613,800</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet101V2</td>\n",
    "<td align=\"right\">171 MB</td>\n",
    "<td align=\"right\">0.772</td>\n",
    "<td align=\"right\">0.938</td>\n",
    "<td align=\"right\">44,675,560</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet152V2</td>\n",
    "<td align=\"right\">232 MB</td>\n",
    "<td align=\"right\">0.780</td>\n",
    "<td align=\"right\">0.942</td>\n",
    "<td align=\"right\">60,380,648</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>InceptionV3</td>\n",
    "<td align=\"right\">92 MB</td>\n",
    "<td align=\"right\">0.779</td>\n",
    "<td align=\"right\">0.937</td>\n",
    "<td align=\"right\">23,851,784</td>\n",
    "<td align=\"right\">159</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>InceptionResNetV2</td>\n",
    "<td align=\"right\">215 MB</td>\n",
    "<td align=\"right\">0.803</td>\n",
    "<td align=\"right\">0.953</td>\n",
    "<td align=\"right\">55,873,736</td>\n",
    "<td align=\"right\">572</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MobileNet</td>\n",
    "<td align=\"right\">16 MB</td>\n",
    "<td align=\"right\">0.704</td>\n",
    "<td align=\"right\">0.895</td>\n",
    "<td align=\"right\">4,253,864</td>\n",
    "<td align=\"right\">88</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MobileNetV2</td>\n",
    "<td align=\"right\">14 MB</td>\n",
    "<td align=\"right\">0.713</td>\n",
    "<td align=\"right\">0.901</td>\n",
    "<td align=\"right\">3,538,984</td>\n",
    "<td align=\"right\">88</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DenseNet121</td>\n",
    "<td align=\"right\">33 MB</td>\n",
    "<td align=\"right\">0.750</td>\n",
    "<td align=\"right\">0.923</td>\n",
    "<td align=\"right\">8,062,504</td>\n",
    "<td align=\"right\">121</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DenseNet169</td>\n",
    "<td align=\"right\">57 MB</td>\n",
    "<td align=\"right\">0.762</td>\n",
    "<td align=\"right\">0.932</td>\n",
    "<td align=\"right\">14,307,880</td>\n",
    "<td align=\"right\">169</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DenseNet201</td>\n",
    "<td align=\"right\">80 MB</td>\n",
    "<td align=\"right\">0.773</td>\n",
    "<td align=\"right\">0.936</td>\n",
    "<td align=\"right\">20,242,984</td>\n",
    "<td align=\"right\">201</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>NASNetMobile</td>\n",
    "<td align=\"right\">23 MB</td>\n",
    "<td align=\"right\">0.744</td>\n",
    "<td align=\"right\">0.919</td>\n",
    "<td align=\"right\">5,326,716</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>NASNetLarge</td>\n",
    "<td align=\"right\">343 MB</td>\n",
    "<td align=\"right\">0.825</td>\n",
    "<td align=\"right\">0.960</td>\n",
    "<td align=\"right\">88,949,818</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>EfficientNetB0</td>\n",
    "<td align=\"right\">29 MB</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">5,330,571</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>EfficientNetB1</td>\n",
    "<td align=\"right\">31 MB</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">7,856,239</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>EfficientNetB2></td>\n",
    "<td align=\"right\">36 MB</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">9,177,569</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>EfficientNetB3</td>\n",
    "<td align=\"right\">48 MB</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">12,320,535</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>EfficientNetB4</td>\n",
    "<td align=\"right\">75 MB</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">19,466,823</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>EfficientNetB5</td>\n",
    "<td align=\"right\">118 MB</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">30,562,527</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>EfficientNetB6</td>\n",
    "<td align=\"right\">166 MB</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">43,265,143</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>EfficientNetB7</td>\n",
    "<td align=\"right\">256 MB</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">66,658,687</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A variety of other models is available from other sources - for example, the [Tensorflow Hub](https://tfhub.dev/).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use MobileNetV2, which is designed specifically to be small and fast (so it can run on mobile devices!)\n",
    "\n",
    "MobileNets come in various sizes controlled by a multiplier for the depth (number of features), and trained for various sizes of input images. We will use the 224x224 input image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "  input_shape=INPUT_IMG_SHAPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess test sample before using it\n",
    "test_sample_pre, _ = preprocess_image(test_sample, label=2)\n",
    "base_probs = base_model.predict(test_sample_pre)\n",
    "base_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = tf.keras.utils.get_file(\n",
    "    'ImageNetLabels.txt',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
    "imagenet_classes = np.array(open(url).read().splitlines())[1:]\n",
    "imagenet_classes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see what the top 5 predicted classes are for my test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_likely_classes = np.argsort(base_probs.squeeze())[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4));\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_sample.reshape(INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3));\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "p = sns.barplot(x=imagenet_classes[most_likely_classes],y=base_probs.squeeze()[most_likely_classes]);\n",
    "plt.ylabel(\"Probability\");\n",
    "p.set_xticklabels(p.get_xticklabels(), rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNetV2 is trained on a specific task: classifying the images in the ImageNet dataset by selecting the most appropriate of 1000 class labels.\n",
    "\n",
    "It is not trained for our specific task: classifying an image of a hand as rock, paper, or scissors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: fine-tuning a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical convolutional neural network looks something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/LongerVision/Resource/master/AI/Visualization/PlotNeuralNet/vgg16.png\" alt=\"Image via PlotNeuralNet\" />\n",
    "<figcaption aria-hidden=\"true\">Image via <a href=\"https://github.com/HarisIqbal88/PlotNeuralNet\">PlotNeuralNet</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a sequence of convolutional layers followed by pooling layers. These layers are *feature extractors* that “learn” key features of our input images.\n",
    "\n",
    "Then, we have one or more fully connected layers followed by a fully connected layer with a softmax activation function. This part of the network is for *classification*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea behind transfer learning is that the *feature extractor* part of the network can be re-used across different tasks and different domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is especially useful when we don’t have a lot of task-specific data. We can get a pre-trained feature extractor trained on a lot of data from another task, then train the classifier on task-specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general process is:\n",
    "\n",
    "-   Get a pre-trained model, without the classification layer.\n",
    "-   Freeze the base model.\n",
    "-   Add a classification layer.\n",
    "-   Train the model (only the weights in your classification layer will be updated).\n",
    "-   (Optional) Un-freeze some of the last layers in your base model.\n",
    "-   (Optional) Train the model again, with a smaller learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our own classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will get the MobileNetV2 model *without* the fully connected layer at the top of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "  input_shape=INPUT_IMG_SHAPE,\n",
    "  include_top=False, \n",
    "  pooling='avg'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will *freeze* the model. We're not going to train the MobileNetV2 part of the model, we're just going to use it to extract features from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’l make a *new* model out of the “headless” already-fitted MobileNetV2, with a brand-new, totally untrained classification head on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(base_model)\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=3,\n",
    "    activation=tf.keras.activations.softmax\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’l compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we’ll use data augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    image = tf.image.random_crop(\n",
    "    image, size=(image.shape[0] - 4, image.shape[1] - 4, image.shape[2])\n",
    "    )\n",
    "    image = tf.image.resize(image, (INPUT_IMG_SIZE, INPUT_IMG_SIZE))\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we apply data augmentation and preprocessing to the training data; and just preprocessing to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=4\n",
    "ds_train = ds_train.map(preprocess_image)\n",
    "ds_train = ds_train.map(augment_image)\n",
    "ds_train = ds_train.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "ds_test  = ds_test.map(preprocess_image)\n",
    "ds_test  = ds_test.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start training our model. Remember, we are *only* updating the weights in the classification head.\n",
    "\n",
    "Also note that we are not doing early stopping or otherwise “using” the data passed as `validation_data`, so it’s OK to use the test data in this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "hist = model.fit(\n",
    "    ds_train,\n",
    "    epochs = n_epochs, \n",
    "    validation_data = ds_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "accuracy = hist.history['accuracy']\n",
    "val_accuracy = hist.history['val_accuracy']\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss, label='Training set')\n",
    "plt.plot(val_loss, label='Test set', linestyle='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(accuracy, label='Training set')\n",
    "plt.plot(val_accuracy, label='Test set', linestyle='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have fitted our own classification head, but there's one more step we can attempt to customize the model for our particular application.\n",
    "\n",
    "We are going to “un-freeze” the later parts of the model, and train it for a few more epochs on our data, so that the high-level features are better suited for our specific classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base_model.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are *not* creating a new model. We're just going to continue training the model we already started training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_at = 149\n",
    "\n",
    "# freeze first layers\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False\n",
    "    \n",
    "# use a smaller training rate for fine-tuning\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "model.compile(\n",
    "    optimizer = opt,\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_fine = 20\n",
    "\n",
    "hist_fine = model.fit(\n",
    "    ds_train,\n",
    "    epochs=n_epochs + n_epochs_fine,\n",
    "    initial_epoch=n_epochs,  \n",
    "    validation_data = ds_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = hist.history['loss'] + hist_fine.history['loss']\n",
    "val_loss = hist.history['val_loss'] + hist_fine.history['val_loss']\n",
    "\n",
    "accuracy = hist.history['accuracy'] + hist_fine.history['accuracy']\n",
    "val_accuracy = hist.history['val_accuracy'] + hist_fine.history['val_accuracy']\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss, label='Training set')\n",
    "plt.plot(val_loss, label='Test set', linestyle='--')\n",
    "plt.plot([n_epochs, n_epochs], plt.ylim(),label='Fine Tuning',linestyle='dotted')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(accuracy, label='Training set')\n",
    "plt.plot(val_accuracy, label='Test set', linestyle='dotted')\n",
    "plt.plot([n_epochs, n_epochs], plt.ylim(), label='Fine Tuning', linestyle='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify custom test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict(test_sample_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4));\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_sample.reshape(INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3));\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "p = sns.barplot(x=classes,y=test_probs.squeeze());\n",
    "plt.ylabel(\"Probability\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, for most machine learning problems, you wouldn’t design or train a convolutional neural network from scratch - you would use an existing model that suits your needs (does well on ImageNet, size is right) and fine-tune it on your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning isn’t only for image classification.\n",
    "\n",
    "There are many problems that can be solved by taking a VERY LARGE task-generic “feature detection” model trained on a LOT of data, and fine-tuning it on a small custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, consider [AI Dungeon](https://play.aidungeon.io/), a game in the style of classic text-based adventure games.\n",
    "\n",
    "It was trained by fine-tuning a version of GPT-2. GPT-2 is a language model with 1.5 billion parameters, trained on a dataset of 8 million web pages, with the objective of predicting the next word in a sequence.\n",
    "\n",
    "The creator of AI Dungeon fine-tuned GTP-2 using story-games scraped from [ChooseYourStory.com](https://chooseyourstory.com/Stories/).\n",
    "\n",
    "With so much data, the model doesn’t only learn about language and language features - it learns about the world described by all that text! Since the game is based on a fine-tuned version of that model, it also knows a lot about the world."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 }
}

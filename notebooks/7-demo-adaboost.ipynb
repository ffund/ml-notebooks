{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: AdaBoost Classifier\n",
    "\n",
    "*Fraida Fund*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will build and train our own AdaBoost classifier, in order to better understand how this algorithm works. (At the end, we’l look at the `sklearn` implementation of AdaBoost and note that its behavior is identical.)\n",
    "\n",
    "This demo is based on the following blog post: [AdaBoost: Implementation and intuition](https://xavierbourretsicotte.github.io/AdaBoost.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for non-demo purposes, you can use the `sklearn` implementation, `AdaBoostClassifier` ([reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "\n",
    "First, we will generate and plot some “toy” data for a binary classification problem with class labels $-1, 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(noise=0.1, factor=0.4, n_samples=50, random_state=3)\n",
    "y = y*2-1\n",
    "x1 = X[:,0]\n",
    "x2 = X[:,1]\n",
    "sns.scatterplot(x=x1, y=x2, hue=y, palette={-1:'red', 1:'blue'});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the base estimator and the number of estimators\n",
    "\n",
    "The parameters of the base estimator and the number of base estimators are tuning parameters.\n",
    "\n",
    "-   If this number of estimators (number of rounds of boosting) is small, the ensemble may have large bias.\n",
    "-   If the base estimator is too complex (e.g. a deep tree), the ensemble may have high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 15\n",
    "\n",
    "dt = []\n",
    "for i in range(n_estimators):\n",
    "  dt.append(DecisionTreeClassifier(max_depth = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize weights\n",
    "\n",
    "In the first step of the algorithm, let $w_i = \\frac{1}{N}$ for all $i$ in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.repeat(1/len(y), repeats=len(y))\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop\n",
    "\n",
    "In each iteration, we:\n",
    "\n",
    "-   Fit a decision stump (tree with depth 1) using a *weighted* version of the training data set, and get the predictions of the decision stump for the training data.\n",
    "-   Compute weighted error:\n",
    "\n",
    "$$err_m = \\frac{\\sum_{i=1}^N w_i 1(y_i \\neq \\hat{f}^m(x_i))}{\\sum_{i=1}^N w_i}$$\n",
    "\n",
    "-   Compute coefficient;\n",
    "\n",
    "$$\\alpha_m = \\log \\left( \\frac{1-err_m}{err_m} \\right)$$\n",
    "\n",
    "-   Update weights:\n",
    "\n",
    "$$w_{i+1} \\leftarrow w_i e^{\\alpha_m 1(y_i \\neq \\hat{f}^m(x_i))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w      = np.zeros(shape=(n_estimators+1, len(weights)))\n",
    "y_pred = np.zeros(shape=(n_estimators, len(y)))\n",
    "err    = np.zeros(n_estimators)\n",
    "alpha  = np.zeros(n_estimators)\n",
    "acc    = np.zeros(n_estimators)\n",
    "\n",
    "w[0] = weights\n",
    "\n",
    "# loop over the number of base estimators\n",
    "for m in range(n_estimators):\n",
    "  # fit decision stump and get its predictions\n",
    "  dt[m].fit(X, y, sample_weight=w[m])\n",
    "  y_pred[m] = dt[m].predict(X)\n",
    "\n",
    "  # compute accuracy of the stump\n",
    "  # (not really required, just out of interest)\n",
    "  acc[m] = accuracy_score(y, y_pred[m])\n",
    "\n",
    "  # compute weighted error\n",
    "  err[m] = sum(w[m]*(y_pred[m]!=y))/sum(w[m])\n",
    "\n",
    "  # compute coefficient\n",
    "  alpha[m] = np.log((1.0-err[m])/err[m])\n",
    "\n",
    "  # update weights\n",
    "  w[m+1] = w[m]*np.exp(alpha[m]*(y_pred[m]!=y))/np.sum(w[m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble prediction\n",
    "\n",
    "The ensemble prediction is\n",
    "\n",
    "$$\\hat{f}(x) = \\text{sign} \\left[\\sum_{m=1}^M \\alpha_m \\hat{f}^m(x)\\right]$$\n",
    "\n",
    "The sign of the ensemble output gives the predicted class.\n",
    "\n",
    "The magnitude,\n",
    "\n",
    "$$\\text{abs} \\left[\\sum_{m=1}^M \\alpha_m \\hat{f}^m(x)\\right]$$\n",
    "\n",
    "indicates how confident the ensemble is in the prediction.\n",
    "\n",
    "We will store the ensemble output from each stage of training, so that we can see how it changes. In general, however, it is only necessary to compute the ensemble output once, after the last iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_en = np.zeros(shape=(n_estimators, len(y)))\n",
    "acc_en = np.zeros(n_estimators)\n",
    "\n",
    "for m in range(n_estimators):\n",
    "  # compute ensemble prediction and its accuracy \n",
    "  for i in range(m+1):\n",
    "    y_pred_en[m] += alpha[i]*dt[i].predict(X)\n",
    "  acc_en[m] = np.mean(y==np.sign(y_pred_en[m]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We will create the following plots in each iteration:\n",
    "\n",
    "1.  Plot of decision boundaries for the decision stump learned in this iteration.\n",
    "\n",
    "-   Each region is shaded red or blue according to the prediction for the region, $\\hat{y}_{R_k}$.\n",
    "-   The intensity of the color of the region indicates the weighted proportion of samples in the region that belong to the predicted class. This is the estimated probability that a sample in this region belongs to the predicted class:\n",
    "\n",
    "$$P(y=\\hat{y}_{R_k} | x) = \\frac{\\sum_{i:x_i \\in R_k} w_i 1(y_i=\\hat{y}_{R_k})} {\\sum_{i:x_i \\in R_k} w_i} $$\n",
    "\n",
    "-   Training points are plotted on top of the decision regions, with their color indicating their true class, and their size indicating their relative weight at the beginning of this iteration.\n",
    "\n",
    "1.  Tree visualization for the decision stump learned in this iteration.\n",
    "\n",
    "-   Each leaf node is shaded red or blue according to the prediction for the node. The intensity of the color is again\n",
    "\n",
    "$$P(y=\\hat{y}_{R_k} | x)$$\n",
    "\n",
    "-   The `value=[a, b]` line in each node gives the weighted sum of samples in each class that appear at that node. (These weighted values are used to compute the GINI index and choose the feature and cutpoint to split on):\n",
    "\n",
    "$$\\sum_{i:x_i \\in R_k} w_i 1(y_i=-1), \\sum_{i:x_i \\in R_k} w_i 1(y_i=1)$$\n",
    "\n",
    "1.  Scatter plot of training points showing *change* in weight after this iteration.\n",
    "\n",
    "-   The color of each point shows the ratio of its weight *after* this iteration, to its weight *before* this iteration. Any sample that is misclassified by the decision stump should have its weight increase by a factor of $e^{\\alpha_m}$.\n",
    "\n",
    "1.  Ensemble prediction after this iteration.\n",
    "\n",
    "-   The color of each point shows its predicted class: $$\\text{sign} \\left[\\sum_{m=1}^M \\alpha_m \\hat{f}^m(x)\\right]$$\n",
    "\n",
    "-   The size of each point shows the confidence of the prediction: $$\\text{abs} \\left[\\sum_{m=1}^M \\alpha_m \\hat{f}^m(x)\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for plotting decision regions and scatter plot of data\n",
    "\n",
    "def plot_decision_boundary(classifier, X, y, N = 10, scatter_weights = np.ones(len(y)) , ax = None ):\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid( np.linspace(x_min, x_max, N), np.linspace(y_min, y_max, N))\n",
    "    zz = np.array( [classifier.predict_proba(np.array([xi,yi]).reshape(1,-1))[:,1] for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n",
    "            \n",
    "    Z = zz.reshape(xx.shape)\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "    # get current axis and plot\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z, 2, cmap='RdBu', alpha=.5, vmin=0, vmax=1)\n",
    "    ax.scatter(X[:,0],X[:,1], c = y, cmap = cm_bright, s = scatter_weights * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(n_estimators):\n",
    "  # plot the decision stump and its decision regions\n",
    "  # size of each point shows its relative weight\n",
    "  fig = plt.figure(figsize = (20, 4*n_estimators));\n",
    "  plt.subplot(n_estimators, 4, 1+m*4)\n",
    "  plot_decision_boundary(dt[m], X,y,N = 50, scatter_weights =w[m]*30/sum(w[m]) )\n",
    "  plt.title(\"Decision boundary for iteration %d (%0.2f)\" % (m, acc[m]));\n",
    "\n",
    "  # plot the tree diagram for the decision stump\n",
    "  plt.subplot(n_estimators, 4, 2+m*4)\n",
    "  plot_tree(dt[m], filled=True, rounded=True, feature_names=['x1', 'x2']);\n",
    "  plt.title(\"Iteration %d \\n Alpha = %0.2f, Err = %0.2f\" % (m, alpha[m], err[m]));\n",
    "\n",
    "  # plot the change in weights - show which points have increased weight\n",
    "  # following this iteration\n",
    "  plt.subplot(n_estimators, 4, 3+m*4)\n",
    "  sns.scatterplot(x=x1, y=x2, hue=w[m+1]/w[m], legend=False);\n",
    "  plt.title(\"Samples with > weight after iteration %d\" % m);\n",
    "\n",
    "  # plot ensemble prediction and its accuracy \n",
    "  # size of point shows confidence in prediction\n",
    "  plt.subplot(n_estimators, 4, 4+m*4)\n",
    "  sns.scatterplot(x=x1, y=x2, hue=np.sign(y_pred_en[m]),\n",
    "                  size=10*np.abs(y_pred_en[m]), legend=False,\n",
    "                  palette={-1:'red', 0:'purple', 1:'blue'});\n",
    "  plt.title(\"Ensemble prediction (%0.2f)\" % acc_en[m]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sklearn` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf_ab = AdaBoostClassifier(n_estimators = n_estimators, algorithm='SAMME',\n",
    "                            estimator = DecisionTreeClassifier(max_depth=1))\n",
    "clf_ab.fit(X, y)\n",
    "accuracy_score(clf_ab.predict(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in this implementation, we don’t have information about the weights at each step, so our visualization won’t include that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_en_sk = np.zeros(shape=(n_estimators, len(y)))\n",
    "acc_en_sk = np.zeros(n_estimators)\n",
    "conf_en_sk = np.zeros(n_estimators)\n",
    "\n",
    "for m, pred in enumerate(clf_ab.staged_predict(X)): \n",
    "  y_pred_en_sk[m] = pred\n",
    "  acc_en_sk[m] = np.mean(y==np.sign(y_pred_en_sk[m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(n_estimators):\n",
    "  # plot the decision stump and its decision regions\n",
    "  # size of each point shows its relative weight\n",
    "  fig = plt.figure(figsize = (15, 4*n_estimators));\n",
    "  plt.subplot(n_estimators, 3, 1+m*3)\n",
    "  plot_decision_boundary(clf_ab.estimators_[m], X,y,N = 50 )\n",
    "  plt.title(\"Decision boundary for iteration %d (%0.2f)\" % \n",
    "            (m, accuracy_score(clf_ab.estimators_[m].predict(X), y)));\n",
    "\n",
    "  # plot the tree diagram for the decision stump\n",
    "  plt.subplot(n_estimators, 3, 2+m*3)\n",
    "  plot_tree(clf_ab.estimators_[m], filled=True, rounded=True, feature_names=['x1', 'x2']);\n",
    "  plt.title(\"Iteration %d \\n Alpha = %0.2f, Err = %0.2f\" % \n",
    "            (m, clf_ab.estimator_weights_[m], clf_ab.estimator_errors_[m]));\n",
    "\n",
    "  # plot ensemble prediction and its accuracy \n",
    "  # size of point shows confidence in prediction\n",
    "  plt.subplot(n_estimators, 3, 3+m*3)\n",
    "  sns.scatterplot(x=x1, y=x2, hue=np.sign(y_pred_en_sk[m]),\n",
    "                  legend=False,\n",
    "                  palette={-1:'red', 0:'purple', 1:'blue'});\n",
    "  plt.title(\"Ensemble prediction (%0.2f)\" % acc_en_sk[m]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall decision boundary looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(clf_ab, X, y)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 }
}

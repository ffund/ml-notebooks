{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook\n",
    "\n",
    "Many college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously. In the article “Beauty in the Classroom: Professors’ Pulchritude and Putative Pedagogical Productivity” ([PDF](https://www.nber.org/papers/w9853.pdf)), authors Daniel Hamermesh and Amy M. Parker suggest (based on a data set of teaching evaluation scores collected at UT Austin) that student evaluation scores can partially be predicted by features unrelated to teaching, such as the physical attractiveness of the instructor.\n",
    "\n",
    "In this notebook, we will use this data to try and predict a course- and instructor-specific “baseline” score (excluding the effect of teaching quality), against which to measure instructor performance."
   ],
   "id": "936dcf8e-7476-462f-910e-7e19a643d7fa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribution\n",
    "\n",
    "Parts of this lab are based on a lab assignment from the OpenIntro textbook “Introductory Statistics with Randomization and Simulation” that is released under a Creative Commons Attribution-ShareAlike 3.0 Unported license. The book website is at <https://www.openintro.org/book/isrs/>."
   ],
   "id": "18afffb9-6e42-4db0-9cc3-bf9d4005bca8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data were gathered from end of semester student evaluations for a large sample of professors from the University of Texas at Austin. In addition, six students looked at a photograph of each professor in the sample, and rated the professors’ physical appearance. More specifically:\n",
    "\n",
    "> Each of the professors’ pictures was rated by each of six undergraduate students: Three women and three men, with one of each gender being a lower-division, two upper-division students (to accord with the distribution of classes across the two levels). The raters were told to use a 10 (highest) to 1 rating scale, to concentrate on the physiognomy of the professor in the picture, to make their ratings independent of age, and to keep 5 in mind as an average.\n",
    "\n",
    "We are using a slightly modified version of the original data set from the published paper. The dataset was released along with the textbook “Data Analysis Using Regression and Multilevel/Hierarchical Models” (Gelman and Hill, 2007).)"
   ],
   "id": "8d69d8ce-dd99-4ac0-802d-f86f80aa327d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We will start by importing relevant libraries, setting up our notebook, reading in the data, and checking that it was loaded correctly."
   ],
   "id": "4ae23a8d-6e76-4238-9911-bd5bfd70d0af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ],
   "id": "894878bf-0031-461c-845d-1fb65d1bc376"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://www.openintro.org/stat/data/evals.csv' -O 'evals.csv'"
   ],
   "id": "3a230b50-39e0-4ade-829d-d4d527f3e20b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('evals.csv')\n",
    "df.head()\n",
    "df.columns\n",
    "df.shape"
   ],
   "id": "33a13584-a7c6-491e-922d-2fff16fc5795"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the data frame represents a different course, and columns represent features of the courses and professors. Here’s the data dictionary:\n",
    "\n",
    "| variable        | description                                                                           |\n",
    "|------------------------------------------|------------------------------|\n",
    "| `score`         | average professor evaluation score: (1) very unsatisfactory - (5) excellent.          |\n",
    "| `rank`          | rank of professor: teaching, tenure track, tenured.                                   |\n",
    "| `ethnicity`     | ethnicity of professor: not minority, minority.                                       |\n",
    "| `gender`        | gender of professor: female, male.                                                    |\n",
    "| `language`      | language of school where professor received education: english or non-english.        |\n",
    "| `age`           | age of professor.                                                                     |\n",
    "| `cls_perc_eval` | percent of students in class who completed evaluation.                                |\n",
    "| `cls_did_eval`  | number of students in class who completed evaluation.                                 |\n",
    "| `cls_students`  | total number of students in class.                                                    |\n",
    "| `cls_level`     | class level: lower, upper.                                                            |\n",
    "| `cls_profs`     | number of professors teaching sections in course in sample: single, multiple.         |\n",
    "| `cls_credits`   | number of credits of class: one credit (lab, PE, etc.), multi credit.                 |\n",
    "| `bty_f1lower`   | beauty rating of professor from lower level female: (1) lowest - (10) highest.        |\n",
    "| `bty_f1upper`   | beauty rating of professor from upper level female: (1) lowest - (10) highest.        |\n",
    "| `bty_f2upper`   | beauty rating of professor from second upper level female: (1) lowest - (10) highest. |\n",
    "| `bty_m1lower`   | beauty rating of professor from lower level male: (1) lowest - (10) highest.          |\n",
    "| `bty_m1upper`   | beauty rating of professor from upper level male: (1) lowest - (10) highest.          |\n",
    "| `bty_m2upper`   | beauty rating of professor from second upper level male: (1) lowest - (10) highest.   |\n",
    "| `bty_avg`       | average beauty rating of professor.                                                   |\n",
    "| `pic_outfit`    | outfit of professor in picture: not formal, formal.                                   |\n",
    "| `pic_color`     | color of professor’s picture: color, black & white.                                   |\n",
    "\n",
    "Source: [OpenIntro book](https://www.openintro.org/book/isrs/)."
   ],
   "id": "d1abd293-ad95-4e9f-a8c3-26473b98f0d1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that:\n",
    "\n",
    "-   `score` is the target variable - this is what we want our model to predict. We expect that the score is a function of the teaching quality, characteristics of the course, and non-teaching related characteristics of the instructor. However, the “true” teaching quality for each course is not known.\n",
    "-   the variables that begin with a `cls_` prefix are features that relate to the course. These features could potentially affect student evaluations: for example, students may rank one-credit lab courses more highly than multi-credit lecture courses.\n",
    "-   variables such as `rank`, `ethnicity`, `gender`, `language`, `age`, and the variables with a `bty_` prefix are features that relate to the instructor. They do not necessarily to the quality of instruction! These features may also affect student evaluations: for example, students may rate instructors more highly if they are physically attractive.\n",
    "-   variables with the `pic_` prefix describe the photograph that was shown to the students who provided the `bty_` scores. This should have no effect on the student evaluations, since those were evaluations by students who were enrolled in the course (not the students who were shown the photograph and asked to provide an attractiveness score.) (For your reference: on the bottom of page 7 of the paper, the authors describe why they include this variable and how they used it )"
   ],
   "id": "0d43a324-a42c-4ca9-ba17-9e32eb195c7f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data\n",
    "\n",
    "As always, start by exploring the data:"
   ],
   "id": "24a7eee1-1469-4a8e-bc4c-c71563a29fbd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ],
   "id": "6db26db2-5328-4dab-ab94-5968c02ae45e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, plot_kws={'alpha':0.5, 'size': 0.1})"
   ],
   "id": "09ca6b3f-26ce-4b75-ad54-081f050a2bcf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With so many numeric variables, the pair plot is hard to read. We can create a pairplot excluding some variables that we don’t expect to be useful for visualization: `cls_perc_eval`, `cls_did_eval`. We will also exclude the individual attractiveness ratings `bty_f1lower`, `bty_f1upper`, `bty_f2upper`, `bty_m1lower`, `bty_m1upper`, `bty_m2upper`, since the overall attractiveness rating is still represented by `bty_avg`."
   ],
   "id": "6180c5a9-7cf0-4939-8c51-8ef43f1d9589"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, vars=['age', 'cls_students', 'bty_avg', 'score'], plot_kws={'alpha':0.5, 'size': 0.1})"
   ],
   "id": "7c582c66-b03a-4e56-8794-41aee10ecf4d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of our exploration of the data, we can also examine the effect of non-numeric variables related to the instructor and the class: `rank`, `ethnicity`, `gender`, `language`, `cls_level`, `cls_profs`, `cls_credits`."
   ],
   "id": "811c4e98-7619-49d6-8413-fee913773f48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ['rank', 'ethnicity', 'gender', 'language', 'cls_level', 'cls_profs', 'cls_credits']:\n",
    "    df.groupby([feature])['score'].describe()"
   ],
   "id": "b4f58330-1752-49bf-9ddf-2a242072d697"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion Question 1\n",
    "\n",
    "Describe the relationship between `score` and the overall attractiveness rating `bty_avg`. Is there an apparent correlation? If so, is it a positive or a negative correlation? What about `age` and `cls_students`, do they appear to be correlated with `score`?\n",
    "\n",
    "Also describe the relationship between `score` and the categorical variables you explored above that are related to characteristics of the *instructor*: `rank`, `ethnicity`, `gender`, `language`. Which of these variables have an apparent correlation with `score`? Is it a positive or a negative correlation?\n",
    "\n",
    "Are any of the apparent relationships you observed unexpected to you? Explain.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "4c07e19f-a603-4afa-ad90-def6e4d0200a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables\n",
    "\n",
    "To represent a categorical variable (with no inherent ordering) in a regression, we can use *one hot encoding*. It works as follows:\n",
    "\n",
    "-   For a categorical variable $x$ with values $1,\\cdots,M$\n",
    "-   Represent with $M$ binary features: $\\phi_1, \\phi_2, \\cdots , \\phi_M$\n",
    "-   Model in regression $w1_1 \\phi_1 + \\cdots + w_M \\phi_M$\n",
    "\n",
    "We can use the `get_dummies` function in `pandas` for one hot encoding. Create a copy of the dataframe with all categorical variables transformed into indicator (“dummy”) variables, and save it in a new data frame called `df_enc`.\n",
    "\n",
    "Compare the columns of the `df` data frame versus the `df_enc` data frame."
   ],
   "id": "76181d91-68f2-4ed0-ad6c-e2fc8e91191b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc = pd.get_dummies(df)\n",
    "df_enc.columns"
   ],
   "id": "65281ac4-663a-4adb-ab29-89e97bef4041"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "\n",
    "Next, we split the encoded data into a training set (70%) and test set (30%). We will be especially interested in evaluating the model performance on the test set. Since it was not used to train the model parameters (intercept and coefficients), the performance on this data gives us a better idea of how the model may perform on new data.\n",
    "\n",
    "We’ll use the `train_test_split` method in `sklearn`’s `model_selection` module. Since it randomly splits the data, we’ll pass a random “state” into the function that makes the split repeatable (same split every time we run this notebook) and ensures that everyone in the class will have exactly the same split."
   ],
   "id": "b8428941-2c52-47f9-8902-12742ad0ee7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(df_enc, test_size=0.3, random_state=9)\n",
    "# why 9? see https://dilbert.com/strip/2001-10-25\n",
    "train.shape\n",
    "test.shape"
   ],
   "id": "087e1266-7ffc-49fe-8d12-59f51b812065"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple linear regression\n",
    "\n",
    "Now we are finally ready to train a regression model.\n",
    "\n",
    "Since the article is nominally abou the attractiveness of the instructor, we will train the simple linear regression on the `bty_avg` feature.\n",
    "\n",
    "In the cell that follows, we will write code to\n",
    "\n",
    "-   use `sklearn` to fit a simple linear regression model on the training set, using `bty_avg` as the feature on which to train. Save your fitted model in a variable `reg_simple`.\n",
    "-   print the intercept and coefficient of the model.\n",
    "-   use `predict` on the fitted model to estimate the evaluation score on the training set, and save this array in `y_pred_train`.\n",
    "-   use `predict` on the fitted model to estimate the evaluation score on the test set, and save this array in `y_pred_test`.\n",
    "\n",
    "Then run the cell after that one, which will show you the training data, the test data, and your regression line."
   ],
   "id": "364df8fc-32d2-4753-9827-a2014f304655"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_simple = LinearRegression().fit(train[['bty_avg']], train['score'])\n",
    "reg_simple.coef_\n",
    "reg_simple.intercept_\n",
    "\n",
    "y_pred_train = reg_simple.predict(train[['bty_avg']])\n",
    "y_pred_test = reg_simple.predict(test[['bty_avg']])"
   ],
   "id": "17f8baf7-73af-47f3-bacc-6cebbfca2742"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=train, x=\"bty_avg\", y=\"score\", color='blue', alpha=0.5);\n",
    "sns.scatterplot(data=test, x=\"bty_avg\", y=\"score\", color='green', alpha=0.5);\n",
    "sns.lineplot(data=train, x=\"bty_avg\", y=y_pred_train, color='red');"
   ],
   "id": "bddaa2a0-650f-4811-b115-629aa571d2fd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate simple linear regression performance\n",
    "\n",
    "Next, we will evaluate our model performance.\n",
    "\n",
    "In the following cell, we will write a *function* to compute key performance metrics for our model:\n",
    "\n",
    "-   compute the R2 score on your training data\n",
    "-   compute the MSE on your training data\n",
    "-   compute the MSE, divided by the sample variance of `score`, on your training data. Recall that this metric tells us the ratio of average error of your model to average error of prediction by mean.\n",
    "-   and compute the same three metrics for your test set"
   ],
   "id": "a79ffe09-db10-4a90-b03b-2b013effa9db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_performance(y_true_train, y_pred_train, y_true_test, y_pred_test):\n",
    "\n",
    "    r2_train = metrics.r2_score(y_true_train, y_pred_train)\n",
    "    mse_train = metrics.mean_squared_error(y_true_train, y_pred_train)\n",
    "    norm_mse_train = metrics.mean_squared_error(y_true_train, y_pred_train)/(np.std(y_true_train)**2)\n",
    "\n",
    "    r2_test = metrics.r2_score(y_true_test, y_pred_test)\n",
    "    mse_test = metrics.mean_squared_error(y_true_test, y_pred_test)\n",
    "    norm_mse_test = metrics.mean_squared_error(y_true_test, y_pred_test)/(np.std(y_true_test)**2)\n",
    "\n",
    "    #print(\"Training:   %f %f %f\" % (r2_train, mse_train, norm_mse_train))\n",
    "    #print(\"Test:       %f %f %f\" % (r2_test, mse_test, norm_mse_test))\n",
    "\n",
    "    return [r2_train, mse_train, norm_mse_train, r2_test, mse_test, norm_mse_test]"
   ],
   "id": "5f4b71e0-4f95-4358-8686-83a4bf83b981"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call your function to print the performance of the simple linear regression. Is a simple linear regression on `bty_avg` better than a “dumb” model that predicts the mean value of `score` for all samples?"
   ],
   "id": "8ab2c837-64e7-409d-9b58-e9f969521471"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = regression_performance(train['score'], y_pred_train, test['score'], y_pred_test)"
   ],
   "id": "429d4481-8dab-4600-96e0-5991d2e7cb6a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple linear regression\n",
    "\n",
    "Next, we’ll see if we can improve model performance using multiple linear regression, with more features included.\n",
    "\n",
    "To start, we need to decide which features to use as input to our model. One possible approach is to use every feature in the dataset excluding the target variable, `score`.\n",
    "\n",
    "You can build and view this list of features by running:"
   ],
   "id": "6981bbd5-47df-4c30-a9fc-8d8d8aa14a60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_enc.columns.drop(['score'])\n",
    "features"
   ],
   "id": "0fdbe92b-3096-4f0e-a39a-2ee168a9b229"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we will write code to\n",
    "\n",
    "-   use `sklearn` to fit a linear regression model on the training set, using the `features` array as the list of features to train on. Save your fitted model in a variable `reg_multi`.\n",
    "-   print a table of the features used in the regression and the coefficient assigned to each. If you have saved your fitted regression in a variable named `reg_multi`, you can create and print this table with:\n",
    "\n",
    "``` python\n",
    "df_coef = pd.DataFrame(data = \n",
    "                        {'feature': features, \n",
    "                         'coefficient': reg_multi.coef_})\n",
    "df_coef\n",
    "```"
   ],
   "id": "2bfba4cf-fb31-41a5-bdaa-6bc17dcedc7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_multi = LinearRegression().fit(train[features], train['score'])\n",
    "df_coef = pd.DataFrame(data = \n",
    "                        {'feature': features, \n",
    "                         'coefficient': reg_multi.coef_})\n",
    "df_coef"
   ],
   "id": "82376d02-d2b4-4798-94e5-58d3c9025d64"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion Question 2\n",
    "\n",
    "Look at the list of features and coefficients, especially those related to the attractiveness ratings.\n",
    "\n",
    "Are these results surprising, based on the results of the simple linear regression? Explain your answer.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "a6b441db-5bb8-4bea-b17c-e98357daa7b0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of collinearity\n",
    "\n",
    "Note especially the coefficients associated with each of the individual attractiveness rankings, and the coefficient associated with the average attractiveness ranking. Each of these features separately seems to have a large effect; however, because they are strongly *collinear*, they cancel one another out.\n",
    "\n",
    "(You should be able to see the collinearity clearly in the pairplot you created.)\n",
    "\n",
    "In the following cell, we will write code to\n",
    "\n",
    "-   create a new `features` array, that drops the *individual* attractiveness rankings in addition to the `score` variable (but do *not* drop the average attractiveness ranking)\n",
    "-   use `sklearn` to fit a linear regression model on the training set, using the new `features` array as the list of features to train on. Save your fitted model in a variable `reg_avgbty`.\n",
    "-   print a table of the features used in the regression and the coefficient assigned to each."
   ],
   "id": "8f285695-2231-4aa8-97db-a59ef446b016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_enc.columns.drop(['score', \n",
    "    'bty_f1lower', 'bty_f1upper', 'bty_f2upper', \n",
    "    'bty_m1lower', 'bty_m1upper', 'bty_m2upper'])\n",
    "reg_avgbty = LinearRegression().fit(train[features], train['score'])\n",
    "\n",
    "df_coef = pd.DataFrame(data = \n",
    "                        {'feature': features, \n",
    "                         'coefficient': reg_avgbty.coef_})\n",
    "df_coef"
   ],
   "id": "eae7a60a-b847-4062-8dbc-193b6c57c727"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion Question 3\n",
    "\n",
    "Given the model parameters you have found, which is associated with the strongest effect (on average) on the evaluation score:\n",
    "\n",
    "-   Instructor ethnicity\n",
    "-   Instructor gender\n",
    "\n",
    "(Note that in general, we cannot use the coefficient to compare the effect of features that have a different range. But both ethnicity and gender are represented by binary one hot-encoded variables.)\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "36a63bc2-7f98-45dd-907b-b9ec9e4d812d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate multiple regression model performance\n",
    "\n",
    "Evaluate the performance of your `reg_avgbty` model. In the next cell, we will write code to:\n",
    "\n",
    "-   use the `predict` function on your fitted regression to find $\\hat{y}$ for all samples in the *training* set, and save this in an array called `y_pred_train`\n",
    "-   use the `predict` function on your fitted regression to find $\\hat{y}$ for all samples in the *test* set, and save this in an array called `y_pred_test`\n",
    "-   call the `regression_performance` function you wrote in a previous cell, and print the performance metrics on the training and test set."
   ],
   "id": "bed42e9b-4952-497b-92be-f39ccabbc3e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = reg_avgbty.predict(train[features])\n",
    "y_pred_test = reg_avgbty.predict(test[features])\n",
    "\n",
    "vals = regression_performance(train['score'], y_pred_train, test['score'], y_pred_test)"
   ],
   "id": "c257438b-d20d-4033-8033-744ea9f154ce"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion Question 4\n",
    "\n",
    "Based on the analysis above, what portion of the variation in instructor teaching evaluation can be explained by the factors unrelated to teaching performance, such as the physical characteristics of the instructor?\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "f75e5531-e350-4841-ac79-868563122bdf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion Question 5\n",
    "\n",
    "Based on the analysis above, is your model better at predicting instructor teaching scores than a “dumb” model that just assigns the mean teaching score to every instructor? Explain.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "41b60b45-ba65-4722-bd8a-13d8ddde95b1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion Question 6\n",
    "\n",
    "Suppose you are hired by the ECE department to develop a classifer that will identify high-performing faculty, who will then be awarded prizes for their efforts.\n",
    "\n",
    "Based on the analysis above, do you think it would be fair to use scores on teaching evaluations as an input to your classifier? Explain your answer.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "adfcdfdd-0864-462a-ac83-ec0cbfc0d0cc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring unexpected correlation\n",
    "\n",
    "There are some features that we do *not* expect to be correlated with the instructor’s score.\n",
    "\n",
    "For example, consider the “features” related to the photograph used by the students who rated the instructor’s attractiveness.\n",
    "\n",
    "There is no reason that characteristics of an instructor’s photograph - whether it was in black and white or color, how the instructor was dressed in the photograph - should influence the ratings of students in the instructor’s class. (These students did not even see the photograph.)\n",
    "\n",
    "In the next cell, we will write code to\n",
    "\n",
    "-   create a new `features` array that drops the `score` variable, all of the individual attractiveness rankings, and the variables related to the photograph used for attractiveness rankings.\n",
    "-   use it to fit a model (saved in `reg_nopic`).\n",
    "-   use `reg_nopic` to predict the evaluation scores on both the training and test set\n",
    "-   compute the same set of metrics as above."
   ],
   "id": "eaea9e6f-e6ce-45ae-b11d-f03a7bdbef82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_enc.columns.drop(['score', \n",
    "    'bty_f1lower', 'bty_f1upper', 'bty_f2upper', \n",
    "    'bty_m1lower', 'bty_m1upper', 'bty_m2upper', \n",
    "    'pic_outfit_formal', 'pic_outfit_not formal',\n",
    "    'pic_color_black&white', 'pic_color_color'])\n",
    "\n",
    "reg_nopic = LinearRegression().fit(train[features], train['score'])\n",
    "\n",
    "\n",
    "y_pred_train = reg_nopic.predict(train[features])\n",
    "y_pred_test = reg_nopic.predict(test[features])\n",
    "\n",
    "vals = regression_performance(train['score'], y_pred_train, test['score'], y_pred_test)\n"
   ],
   "id": "4bc01479-aa04-4555-8cc6-c434f6ef843e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion Question 7\n",
    "\n",
    "Is your model less predictive when features related to the instructor photograph are excluded? Explain.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "f7ffbeb9-8104-41f9-abd0-250b235c4a82"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a machine learning model seems to use a feature that is not expected to be correlated with the target variable (such as the characteristics of the instructor’s photograph…), this can sometimes be a signal that information is “leaking” between the training and test set.\n",
    "\n",
    "In this dataset, each row represents a single course. However, some instructors teach more than one course, and an instructor might get similar evaluation scores on all of the courses he or she teaches.\n",
    "\n",
    "(According to the paper for which this dataset was collected, 94 faculty members taught the 463 courses represented in the dataset, with some faculty members teaching as many as 13 courses.)\n",
    "\n",
    "For example, consider the output of the following command, which prints all of the one credit courses in the data:"
   ],
   "id": "7003d4ee-64e9-4561-afcd-3c48bef91e91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['cls_credits']=='one credit']"
   ],
   "id": "e99aa823-980d-40cc-bd92-8d5717fa17f0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that 10 out of 27 one-credit courses are taught by what seems to be the same instructor - we don’t know his name, but let’s call him John. John is a teaching-track professor, minority ethnicity, male, English-language trained, 50 years old, average attractiveness 3.333, and whose photograph is in color and not formal.\n",
    "\n",
    "This provides a clue regarding the apparent importance of the `cls_credits` variable and other “unexpected” variables in predicting the teaching score.\n",
    "\n",
    "Certain variables may be used by the model to identify the instructor, and then learn a relationship between the *individual instructor* and his or her typical evaluation score, instead of learning a true relationship between the *variable* and the evaluation score.\n",
    "\n",
    "In other words: the model learns “an instructor who is teaching-track, minority, male, English-language-trained, 50 years old, has average attractiveness 3.333, and whose photograph is in color and not formal typically scores X” - but it’s really just learning “John typically scores X”."
   ],
   "id": "5721eef6-71f5-41dd-9512-55c3200ea5d8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if this is plausible, let’s add an “instructor ID” to each row in our data frame. The data set doesn’t include an instructor ID column, but we can still uniquely identify every instructor by looking at the combination of rank, ethnicity, gender, language of training, age, attractiveness score, and characteristics of the photo (formal or not, black and white or color)."
   ],
   "id": "33b9ba7f-4966-4bdb-b293-dfa15b0cd925"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructor_id = df[['rank', 'ethnicity', 'gender', 'language',\n",
    "        'pic_outfit', 'pic_color']].agg('-'.join, axis=1)\n",
    "instructor_id +=  '-' + df['age'].astype(str)\n",
    "instructor_id +=  '-' + df['bty_avg'].astype(str)\n",
    "\n",
    "df_enc = df_enc.assign(instructor_id = instructor_id)\n",
    "\n",
    "df_enc['instructor_id'].head()"
   ],
   "id": "201e1612-dea9-4605-9115-24b6be1a6f86"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s plot the frequency with which each “instructor ID” appears in the data:"
   ],
   "id": "3f403dbf-8449-4e08-9445-c2306afa2ffa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_  = plt.figure(figsize=(12,6))\n",
    "ax = sns.countplot(x=\"instructor_id\", data=df_enc, \n",
    "    order=df_enc['instructor_id'].value_counts().index)\n",
    "_  = ax.set(xticklabels=[])"
   ],
   "id": "87ebe0ea-d484-4e7d-b64f-741b197bc00f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 95 unique instructor IDs. According to the paper, the data set includes 94 instructors. We are working with a slightly modified version of the data from the paper. It seems we’ve been able to uniquely identify every instructor.\n",
    "\n",
    "Some instructors are represented as many as 13 times in the dataset. Only a handful of instructors appear only once in the data."
   ],
   "id": "a1bfacd2-9a41-425b-a478-cdec49f120f9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can see that most instructors get similar scores for all of the courses they teach, with a few exceptions:"
   ],
   "id": "d2d1f5a7-09e2-4a7d-be98-3839a22ba47d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_order = df_enc.groupby('instructor_id')['score'].agg('mean').sort_values(ascending=False).index\n",
    "\n",
    "_  = plt.figure(figsize=(12,6))\n",
    "ax = sns.boxplot(x=df_enc['instructor_id'], y=df_enc['score'], \n",
    "                 order=score_order,\n",
    "                 color='white', width=0.4)\n",
    "ax = sns.stripplot(x=df_enc['instructor_id'], y=df_enc['score'], \n",
    "                   order=score_order)\n",
    "_  = ax.set(xticklabels=[])"
   ],
   "id": "1f64275c-9656-46a4-8e40-40c01fe46e5a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore this issue further, we will repeat our analysis using two different ways of splitting the dataset:\n",
    "\n",
    "1.  **Random split**: shuffle data and split it into training and test sets. Train the model using the training data, then evaluate its performance on the test set. (This is what we have done so far.)\n",
    "2.  **Group split**: split data into training and test sets in a way that ensures that each individual *instructor* is represented in either the training data or the test data, but not both. Train the model using the training data, then evaluate its performance on the test set. If the model is “memorizing” individual instructors, rather than learning a general relationship between features and teaching evaluation score, it will have much worse performance on the test set, because it has to predict scores for instructors it hasn’t “seen” yet.\n",
    "\n",
    "Because the dataset is small, the performance evaluation may be influenced by the random sample of rows that happen to end up in the training vs. test set. (If a few rows more rows than usual that are very “easy” to predict are placed in the test set, we might see better performance than we would with a different test set.) So, we will also repeat the splitting procedure several times, and look at the *average* performance across different train-test splits."
   ],
   "id": "bf64c580-cbbd-4b16-aead-ae020a99fd0f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "metrics_rs = np.zeros((n_splits, 6))\n",
    "rs = model_selection.KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "for i, split in enumerate(rs.split(df_enc)):\n",
    "    train_idx, test_idx = split\n",
    "    train = df_enc.iloc[train_idx]\n",
    "    test = df_enc.iloc[test_idx]\n",
    "        \n",
    "    features = df_enc.columns.drop(['score', 'instructor_id'])\n",
    "\n",
    "    # train a multiple linear regression using \n",
    "    # the train dataset and the list of features created above\n",
    "    # save the fitted model in reg_rndsplit\n",
    "    # then use the model to create y_pred_train and y_pred_test, \n",
    "    # the model predictions on the training set and test set.\n",
    "    # Finally, return the array of model performance metrics\n",
    "\n",
    "    reg_rndsplit = LinearRegression().fit(train[features], train['score'])\n",
    "\n",
    "    y_pred_train = reg_rndsplit.predict(train[features])\n",
    "    y_pred_test = reg_rndsplit.predict(test[features])\n",
    "\n",
    "    metrics_rs[i] = regression_performance(train['score'], y_pred_train, test['score'], y_pred_test)\n",
    "\n",
    "\n",
    "np.mean(metrics_rs, axis=0)\n"
   ],
   "id": "98b5f1de-b646-4239-8cb9-ed048aeaa49b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach to splitting the data, the model appears to have some predictive value on the test set (which is supposed to represent performance on “new” data.)"
   ],
   "id": "f65e00ba-2637-4702-a132-2eed55808e58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.bar(x=['Train', 'Test'], height=np.mean(metrics_rs, axis=0)[[0,3]])\n",
    "_ = plt.ylabel(\"R2\")\n",
    "_ = plt.ylim(0, 1)"
   ],
   "id": "35d69789-d466-4996-a79b-831697b50368"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will perform our splits, train a model, and get performance metrics according to the second scheme, in which an instructor may be present in either the training set or the test set, but not both."
   ],
   "id": "a9ff2872-feea-4fa1-9e2e-a9840e8f5c7c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "metrics_gs = np.zeros((n_splits, 6))\n",
    "gs = model_selection.GroupKFold(n_splits=n_splits)\n",
    "\n",
    "for i, split in enumerate(gs.split(df_enc, \n",
    "                                   df_enc['score'], \n",
    "                                   df_enc['instructor_id'])):\n",
    "\n",
    "    train_idx, test_idx = split\n",
    "\n",
    "    train = df_enc.iloc[train_idx]\n",
    "    test = df_enc.iloc[test_idx]\n",
    "        \n",
    "    features = df_enc.columns.drop(['score', 'instructor_id'])\n",
    "\n",
    "    # train a multiple linear regression using \n",
    "    # the train dataset and the list of features created above\n",
    "    # save the fitted model in reg_grpsplit\n",
    "    # then use the model to create y_pred_train and y_pred_test, \n",
    "    # the model predictions on the training set and test set.\n",
    "    # Finally, return the array of model performance metrics\n",
    "\n",
    "    reg_grpsplit = LinearRegression().fit(train[features], train['score'])\n",
    "\n",
    "    y_pred_train = reg_grpsplit.predict(train[features])\n",
    "    y_pred_test = reg_grpsplit.predict(test[features])\n",
    "\n",
    "    metrics_gs[i] = regression_performance(train['score'], y_pred_train, test['score'], y_pred_test)\n",
    "\n",
    "np.mean(metrics_gs, axis=0)\n"
   ],
   "id": "f1359756-d9b1-42b2-9ed5-649a2422879c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the second approach to splitting the data, we can see that the model has no predictive value on the test set."
   ],
   "id": "f7693c38-cd01-4ac9-a2b8-90f80098e3fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.bar(x=['Train', 'Test'], height=np.mean(metrics_gs, axis=0)[[0,3]])\n",
    "_ = plt.ylabel(\"R2\")\n",
    "_ = plt.ylim(0, 1)"
   ],
   "id": "64a68462-b8a4-4f3b-8b25-d0bbe7f95878"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion Question 8\n",
    "\n",
    "Based on your analysis above, do you think your model will be useful to predict the teaching evaluation scores of a new faculty member at UT Austin, based on his or her physical characteristics and the characteristics of the course?\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "5895c901-98cc-446a-8811-dbbea7de9550"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: what went wrong?\n",
    "\n",
    "In this case study, we saw *two* problems:\n",
    "\n",
    "The first problem is that the model was “memorizing” the individual instructors that appeared in the training data, rather than learning a general relationship between the features and the target variable. This is known as *overfitting*.\n",
    "\n",
    "Usually, when a model is overfitting, it will be evident in the evaluation on the test set, because a model that overfits on training data will have excellent performance on training data and poor performance on test data. That’s where the second problem comes in: data leakage! We expect the model to be able to predict a baseline score for instructors it has not been trained on, but our model was being trained on data from a set of instructors, then evaluated on data from the same instructors.\n",
    "\n",
    "As a result of this data leakage, the model had overly optimistic error on the test set. The model appeared to generalize to new, unseen, data, but in fact would not generalize to different instructors.\n",
    "\n",
    "One of the “red flags” that helped us identify the problem was that the model seemed to be learning from features that we know are not really informative - for example, the characteristics of the photo used to derive the attractiveness ratings. This is often a sign of data leakage."
   ],
   "id": "be398cee-c989-4fa1-97cd-f2202f19f460"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection\n",
    "===============\n",
    "\n",
    "We know that (in the “classic” view), the test error of a model tends to decrease and then increase as we increase model complexity. For low complexity, the bias dominates; for high complexity, the variance dominates.\n",
    "\n",
    "The training error, however, only decreases with increasing model complexity. If we use training error to select a model, we’ll select a model that overfits. And during training, when we select a model, only the training error is available to us.\n",
    "\n",
    "![Image from “Elements of Statistical Learning”](https://i.stack.imgur.com/alkeM.png)\n",
    "\n",
    "*Image source: Elements of Statistical Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is cross validation. Until now, we have been dividing our data into two parts:\n",
    "\n",
    "-   Training data: used to train the model\n",
    "-   Test data: used to evaluate the performance of our model on new, unseen data\n",
    "\n",
    "Now, we will make one more split:\n",
    "\n",
    "-   Training data: used to train the model\n",
    "-   Validation data: used to select the model complexity (usually by tuning some *hyperparameters*)\n",
    "-   Test data: used to evaluate the performance of our model on new, unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we will refine this idea in order to reduce the dependence on the particular samples we choose for the training, and to increase the number of samples available for training. In K-fold cross validation, we split the data into $K$ parts, each part being approximately equal in size. For each split, we fit the data on $K-1$ parts and test the data on the remaining part. Then, we average the score over the $K$ parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for $K=5$, it might look like this:\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from ipywidgets import interact, fixed, widgets\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection using best K-Fold CV score\n",
    "\n",
    "First, we will try to use K-fold CV to select a polynomial model to fit the data in our first example.\n",
    "\n",
    "We will use the `scikit-learn` module for K-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_polynomial_regression_data(n=100, xrange=[-1,1], coefs=[1,0.5,0,2], sigma=0.5):\n",
    "  x = np.random.uniform(xrange[0], xrange[1], n)\n",
    "  y = np.polynomial.polynomial.polyval(x,coefs) + sigma * np.random.randn(n)\n",
    "\n",
    "  return x.reshape(-1,1), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs=[1, 0.5, 0, 2.5]\n",
    "n_samples = 100\n",
    "sigma = 0.4\n",
    "\n",
    "# generate polynimal data\n",
    "x, y = generate_polynomial_regression_data(n=n_samples, coefs=coefs, sigma=sigma)\n",
    "\n",
    "# divide into training and test set\n",
    "# (we will later divide training data again, into training and validation set)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)\n",
    "\n",
    "\n",
    "@interact(d = widgets.IntSlider(min=1, max=25, value=1), show_train = True, show_test = False)\n",
    "def plot_poly_fit(d, show_train, show_test,\n",
    "                  xtr = fixed(x_train), ytr = fixed(y_train), \n",
    "                  xts = fixed(x_test), yts = fixed(y_test)):\n",
    "  \n",
    "  xtr_trans = np.power(xtr, np.arange(0, d))\n",
    "\n",
    "  if show_train:\n",
    "    sns.scatterplot(x = xtr.squeeze(), y = ytr);\n",
    "  if show_test:\n",
    "    sns.scatterplot(x = xts.squeeze(), y = yts);\n",
    "  reg = LinearRegression().fit(xtr_trans, ytr)\n",
    "  ytr_hat = reg.predict(xtr_trans)\n",
    "\n",
    "  mse_tr = metrics.mean_squared_error(ytr, ytr_hat)\n",
    "  mse_ts = metrics.mean_squared_error(yts, reg.predict(np.power(xts, np.arange(0, d))))\n",
    "\n",
    "  sns.lineplot(x = xtr.squeeze(), y = ytr_hat, color='red')\n",
    "  plt.xlabel('x');\n",
    "  plt.ylabel('y');\n",
    "  plt.title(\"Training MSE: %f\\nTest MSE: %f\" % (mse_tr, mse_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting a data set for K-fold cross validation is conceptually very simple. The basic idea is:\n",
    "\n",
    "-   We get a list of indices of training data, and decide how many “folds” we will use. The number of validation samples in each fold $N_{val}$ wil be the total number of training samples, divided by the number of folds.\n",
    "-   Then, we iterate over the number of folds. In the first fold, we put the first $N_{val}$ samples in the validation set and exclude them from the training set. In the second fold, we put the second batch of $N_{val}$ samples in the validation set, and exclude them from the training set. Continue until $K$ folds.\n",
    "\n",
    "In most circumstances, we will shuffle the list of training data indices first.\n",
    "\n",
    "The `scikit-learn` library provides a `KFold` that does this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfold = 5\n",
    "kf = KFold(n_splits=nfold,shuffle=True)\n",
    "\n",
    "for isplit, idx in enumerate(kf.split(x_train)):     \n",
    "    idx_tr, idx_val = idx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "although it’s also easy to do this ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfold = 5                                   # number of folds (you choose!)\n",
    "nval = x_train.shape[0]//nfold              # number of validation samples per fold\n",
    "idx_split = [i*nval for i in range(nfold)]  \n",
    "idx_list = np.arange(x_train.shape[0])      # list of training data indices\n",
    "np.random.shuffle(idx_list)                 # shuffle list of indices\n",
    "\n",
    "for i, idx in enumerate(idx_split):\n",
    "  idx_val = idx_list[idx:idx+nval]\n",
    "  idx_tr = np.delete(idx_list, idx_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outer loop can be used to divide the data into training and validation, but then we’ll also need an inner loop to train and validate each model for this particular fold.\n",
    "\n",
    "In this case, suppose we want to evaluate polynomial models with different model orders from $d=1$ ($\\hat{y} = w_0 + w_1 x$) to $d=10$ ($\\hat{y} = w_0 + w_1 x + w_2 x^2 + \\ldots + w_10 x^10$).\n",
    "\n",
    "We could do something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "# create a k-fold object\n",
    "nfold = 5\n",
    "kf = KFold(n_splits=nfold,shuffle=True)\n",
    "\n",
    "# model orders to be tested\n",
    "dmax = 10\n",
    "dtest_list = np.arange(1,dmax+1)\n",
    "nd = len(dtest_list)\n",
    "\n",
    "for isplit, idx in enumerate(kf.split(x_train)):     \n",
    "  idx_tr, idx_val = idx \n",
    "\n",
    "  for dtest in dtest_list:\n",
    "    # get \"transformed\" training and validation data\n",
    "    x_train_dtest =  x_train[idx_tr]**np.arange(1,dtest+1)\n",
    "    y_train_kfold =  y_train[idx_tr]\n",
    "    x_val_dtest   =  x_train[idx_val]**np.arange(1,dtest+1)\n",
    "    y_val_kfold   =  y_train[idx_val]\n",
    "\n",
    "    # fit model on training data\n",
    "    reg_dtest = LinearRegression().fit(x_train_dtest, y_train_kfold)\n",
    "    \n",
    "    # measure MSE on validation data\n",
    "    y_hat   = reg_dtest.predict(x_val_dtest)\n",
    "    mse_val = metrics.mean_squared_error(y_val_kfold, y_hat)\n",
    "    r2_val  = metrics.r2_score(y_val_kfold, y_hat)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, however, that there was a lot of wasted computation there. We computed the same polynomial features multiple times in different folds. Instead, we should compute the entire set of transformed features in advance, then just select the ones we need in each iteration over model order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "# create a k-fold object\n",
    "nfold = 5\n",
    "kf = KFold(n_splits=nfold,shuffle=True)\n",
    "\n",
    "# model orders to be tested\n",
    "dmax = 10\n",
    "dtest_list = np.arange(1,dmax+1)\n",
    "nd = len(dtest_list)\n",
    "\n",
    "# create transformed features up to d_max\n",
    "x_train_trans = x_train**np.arange(1,dmax+1)\n",
    "\n",
    "for isplit, idx in enumerate(kf.split(x_train)):     \n",
    "  idx_tr, idx_val = idx \n",
    "\n",
    "  for dtest in dtest_list:\n",
    "    # get \"transformed\" training and validation data for this model order\n",
    "    x_train_dtest =  x_train_trans[idx_tr,  :dtest]\n",
    "    y_train_kfold = y_train[idx_tr]\n",
    "    x_val_dtest   =  x_train_trans[idx_val, :dtest]\n",
    "    y_val_kfold   = y_train[idx_val]\n",
    "\n",
    "    # fit model on training data\n",
    "    reg_dtest = LinearRegression().fit(x_train_dtest, y_train_kfold)\n",
    "    \n",
    "    # measure MSE on validation data\n",
    "    y_hat   = reg_dtest.predict(x_val_dtest)\n",
    "    mse_val = metrics.mean_squared_error(y_val_kfold, y_hat)\n",
    "    r2_val  = metrics.r2_score(y_val_kfold, y_hat)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s much better! Let’s look at what this is doing - we’ll run it again with some extra visualization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "# create a k-fold object\n",
    "nfold = 5\n",
    "kf = KFold(n_splits=nfold,shuffle=True)\n",
    "\n",
    "# model orders to be tested\n",
    "dmax = 10\n",
    "dtest_list = np.arange(1,dmax+1)\n",
    "nd = len(dtest_list)\n",
    "\n",
    "# create transformed features up to d_max\n",
    "x_train_trans = x_train**np.arange(1,dmax+1)\n",
    "\n",
    "# create a big figure\n",
    "fig, axs = plt.subplots(nfold, nd, sharex=True, sharey=True)\n",
    "fig.set_figheight(nfold+1);\n",
    "fig.set_figwidth(nd+1);\n",
    "\n",
    "for isplit, idx in enumerate(kf.split(x_train)):     \n",
    "  idx_tr, idx_val = idx \n",
    "\n",
    "  for didx, dtest in enumerate(dtest_list):\n",
    "    # get \"transformed\" training and validation data for this model order\n",
    "    x_train_dtest =  x_train_trans[idx_tr,  :dtest]\n",
    "    y_train_kfold = y_train[idx_tr]\n",
    "    x_val_dtest   =  x_train_trans[idx_val, :dtest]\n",
    "    y_val_kfold   = y_train[idx_val]\n",
    "\n",
    "    # fit model on training data\n",
    "    reg_dtest = LinearRegression().fit(x_train_dtest, y_train_kfold)\n",
    "    \n",
    "    # measure MSE on validation data\n",
    "    y_hat   = reg_dtest.predict(x_val_dtest)\n",
    "    mse_val = metrics.mean_squared_error(y_val_kfold, y_hat)\n",
    "    r2_val  = metrics.r2_score(y_val_kfold, y_hat)\n",
    "\n",
    "    # this is just for visualization/understanding - in a \"real\" problem you would not include this\n",
    "    p = sns.lineplot(x = x_train_dtest[:,0].squeeze(), y = reg_dtest.predict(x_train_dtest), color='red', ax=axs[isplit, didx]);\n",
    "    p = sns.scatterplot(x = x_val_dtest[:, 0].squeeze(), y = y_val_kfold,  ax=axs[isplit, didx]);\n",
    "\n",
    "plt.tight_layout()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we’ll add some arrays in which to save the validation performance from each fold, so that we can average them afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a k-fold object\n",
    "nfold = 5\n",
    "kf = KFold(n_splits=nfold,shuffle=True)\n",
    "\n",
    "# model orders to be tested\n",
    "dmax = 10\n",
    "dtest_list = np.arange(1,dmax+1)\n",
    "nd = len(dtest_list)\n",
    "\n",
    "mse_val = np.zeros((nd,nfold))\n",
    "r2_val  = np.zeros((nd,nfold))\n",
    "\n",
    "# create transformed features up to d_max\n",
    "x_train_trans = x_train**np.arange(1,dmax+1)\n",
    "\n",
    "# loop over the folds\n",
    "# the first loop variable tells us how many out of nfold folds we have gone through\n",
    "# the second loop variable tells us how to split the data\n",
    "for isplit, idx in enumerate(kf.split(x_train)):\n",
    "        \n",
    "  # these are the indices for the training and validation indices\n",
    "  # for this iteration of the k folds\n",
    "  idx_tr, idx_val = idx \n",
    "\n",
    "  x_train_kfold = x_train_trans[idx_tr]\n",
    "  y_train_kfold = y_train[idx_tr]\n",
    "  x_val_kfold = x_train_trans[idx_val]\n",
    "  y_val_kfold = y_train[idx_val]\n",
    "\n",
    "  for didx, dtest in enumerate(dtest_list):\n",
    "\n",
    "    # get transformed features\n",
    "    x_train_dtest =  x_train_kfold[:, :dtest]\n",
    "    x_val_dtest   =  x_val_kfold[:, :dtest]\n",
    "\n",
    "    # fit data\n",
    "    reg_dtest = LinearRegression().fit(x_train_dtest, y_train_kfold)\n",
    "    \n",
    "    # measure MSE on validation data\n",
    "    y_hat = reg_dtest.predict(x_val_dtest)\n",
    "    mse_val[didx, isplit] = metrics.mean_squared_error(y_val_kfold, y_hat)\n",
    "    r2_val[didx, isplit] = metrics.r2_score(y_val_kfold, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the mean (across K folds) validation error for each model order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=dtest_list, y=mse_val.mean(axis=1), marker='o');\n",
    "plt.xlabel(\"Model order\");\n",
    "plt.ylabel(\"K-fold MSE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see which model order gave us the lowest MSE on the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_min = np.argmin(mse_val.mean(axis=1))\n",
    "d_min_mse = dtest_list[idx_min]\n",
    "d_min_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_val.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=dtest_list, y=mse_val.mean(axis=1));\n",
    "sns.scatterplot(x=dtest_list, y=mse_val.mean(axis=1), hue=dtest_list==d_min_mse, s=100, legend=False);\n",
    "\n",
    "plt.xlabel(\"Model order\");\n",
    "plt.ylabel(\"K-fold MSE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also select by highest R2 (instead of lowest MSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max = np.argmax(r2_val.mean(axis=1))\n",
    "d_max_r2 = dtest_list[idx_max]\n",
    "d_max_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_val.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=dtest_list, y=r2_val.mean(axis=1));\n",
    "sns.scatterplot(x=dtest_list, y=r2_val.mean(axis=1), hue=dtest_list==d_max_r2, s=100, legend=False);\n",
    "\n",
    "plt.xlabel(\"Model order\");\n",
    "plt.ylabel(\"K-fold R2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection using 1-SE “rule”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the minimum K-fold CV error for model selection, we sometimes will still select an overly complex model <sup>\\[2\\]</sup>.\n",
    "\n",
    "As an alternative, we can use the “one standard error rule” <sup>\\[3\\]</sup>. According to this “rule”, we choose the least complex model whose error is no more than one standard error above the error of the best model - i.e. the simplest model whose performance is comparable to the best model.\n",
    "\n",
    "<small>\\[2\\] See [Cawley & Talbot (J of Machine Learning Research, 2010)](http://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf) for more on this.</small>\n",
    "\n",
    "<small>\\[3\\] See Chapter 7 of [Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply this rule as follows:\n",
    "\n",
    "-   Find the MSE for each fold for each model candidate\n",
    "-   For each model candidate, compute the mean and standard error of the MSE over the $K$ folds. We will compute the standard error as $$\\frac{\\sigma_{\\text{MSE}}}{\\sqrt{K-1}}$$ where $\\sigma_{\\text{MSE}}$ is the standard deviation of the MSE over the $K$ folds.\n",
    "-   Find the model with the smallest mean MSE (across the $K$ folds). Compute the *target* as mean MSE + SE for this model.\n",
    "-   Select the least complex model whose mean MSE is below the target.\n",
    "\n",
    "This works for any metric that is a “lower is better” metric. If you are using a “higher is better” metric, such as R2, for example, you would modify the last two steps:\n",
    "\n",
    "-   Find the model with the **largest** mean R2 (across the $K$ folds). Compute the **mean R2 - SE of R2** for this model. Call this quantity the *target*.\n",
    "-   Select the least complex model whose mean R2 is **above** the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_min = np.argmin(mse_val.mean(axis=1))\n",
    "target = mse_val[idx_min,:].mean() + mse_val[idx_min,:].std()/np.sqrt(nfold-1)\n",
    "# np.where returns indices of values where condition is satisfied\n",
    "idx_one_se = np.where(mse_val.mean(axis=1) < target)\n",
    "d_one_se = np.min(dtest_list[idx_one_se])\n",
    "d_one_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x=dtest_list, y=mse_val.mean(axis=1), yerr=mse_val.std(axis=1)/np.sqrt(nfold-1));\n",
    "plt.hlines(y=target, xmin=np.min(dtest_list), xmax=np.max(dtest_list), ls='dotted')\n",
    "sns.scatterplot(x=dtest_list, y=mse_val.mean(axis=1), hue=dtest_list==d_one_se, s=100, legend=False);\n",
    "\n",
    "plt.xlabel(\"Model order\");\n",
    "plt.ylabel(\"K-fold MSE\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max = np.argmax(r2_val.mean(axis=1))\n",
    "target_r2 = r2_val[idx_max,:].mean() - r2_val[idx_max,:].std()/np.sqrt(nfold-1)\n",
    "# np.where returns indices of values where condition is satisfied\n",
    "idx_one_se_r2 = np.where(r2_val.mean(axis=1) > target_r2)\n",
    "d_one_se_r2 = np.min(dtest_list[idx_one_se_r2])\n",
    "d_one_se_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x=dtest_list, y=r2_val.mean(axis=1), yerr=r2_val.std(axis=1)/np.sqrt(nfold-1));\n",
    "plt.hlines(y=target_r2, xmin=np.min(dtest_list), xmax=np.max(dtest_list), ls='dotted')\n",
    "sns.scatterplot(x=dtest_list, y=r2_val.mean(axis=1), hue=dtest_list==d_one_se_r2, s=100, legend=False);\n",
    "\n",
    "plt.xlabel(\"Model order\");\n",
    "plt.ylabel(\"K-fold R2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the course of COVID with a “cubic model”\n",
    "---------------------------------------------------\n",
    "\n",
    "As part of the materials for this lesson, you read about some attempts early in the COVID-19 pandemic to predict how the number of cases or deaths would evolve. You were asked to consider:\n",
    "\n",
    "> The forecasts produced by these models were all very wrong, but they appeared to fit the data well! What was wrong with the approach used to produce these models? How did they miscalculate so badly?\n",
    "\n",
    "Now, we’ll take that process apart, see what went wrong, and see what we could have done differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will get U.S. COVID data and read it in to our notebook environment. We’ll also add a field called `daysElapsed` which will count the number of days since March 1, 2020 for each row of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://covidtracking.com/data/download/national-history.csv -O national-history.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('national-history.csv')\n",
    "df.date = pd.to_datetime(df.date)\n",
    "df = df.sort_values(by=\"date\")\n",
    "df = df.assign(daysElapsed =  (df.date - pd.to_datetime('2020-03-01')).dt.days)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s assume that we are making this prediction sometime near the beginning of May 2020 (like in the reading), well into the first wave in the U.S., and we want to predict when this wave will fall off (deaths go back to zero).\n",
    "\n",
    "We’ll use all of the data up to May 2020 for training. But afterwards, we’ll go back and see how well our model did by comparing its predictions for May and June 2020 to the real course of the pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = df[(df.date <= '2020-05-01') & (df.date > '2020-03-01')]\n",
    "df_ts = df[(df.date < '2020-06-30') & (df.date >= '2020-05-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=df_tr.daysElapsed, y=df_tr.deathIncrease);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we will use a polynomial basis to fit a linear regression model, so let’s generate “transformed” versions of our feature: days since the beginning of March 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_poly = df_tr.daysElapsed.values.reshape(-1,1)**np.arange(1, 5)\n",
    "df_ts_poly = df_ts.daysElapsed.values.reshape(-1,1)**np.arange(1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now go ahead and fit a model to our training data, then compute the R2 score for the model. It seems like a good fit on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_covid = LinearRegression().fit(df_tr_poly, df_tr.deathIncrease)\n",
    "deathIncrease_fitted = reg_covid.predict(df_tr_poly)\n",
    "metrics.r2_score(df_tr.deathIncrease, deathIncrease_fitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and looks reasonably good in this visualization (although of course, we recognize that negative deaths should be impossible):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=df_tr.daysElapsed, y=df_tr.deathIncrease);\n",
    "sns.lineplot(x=df_tr.daysElapsed, y=deathIncrease_fitted);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But despite looking good on training data, the model does a terrible job of predicting the end of the first wave. The model predicts that deaths fall to zero around day 70, but we can see that the actual fall off is much slower, and there are still hundreds of deaths each day at day 120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deathIncrease_fitted_future = reg_covid.predict(df_ts_poly)\n",
    "\n",
    "sns.scatterplot(x=df_tr.daysElapsed, y=df_tr.deathIncrease);\n",
    "sns.lineplot(x=df_tr.daysElapsed, y=deathIncrease_fitted);\n",
    "\n",
    "sns.scatterplot(x=df_ts.daysElapsed, y=df_ts.deathIncrease);\n",
    "sns.lineplot(x=df_ts.daysElapsed, y=deathIncrease_fitted_future);\n",
    "\n",
    "plt.ylim(0, 3000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way to train and evaluate this model would be to use a validation set that is *not* used to fit the model parameters to evaluate its performance, or even better: to use cross validation to evaluate different candidate models.\n",
    "\n",
    "Let’s see how we might do that for this time series data. One possible approach would be to create multiple “folds”, where in each fold, the number of samples in the training set increases (and the validation set is always the ten days after the training set - we are validating whether our model can predict deaths ten days into the future).\n",
    "\n",
    "Here’s what that might look like (blue dots are training data, orange dots are validation data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxval = [10, 20, 30, 40, 50]\n",
    "nfold = len(idxval)\n",
    "\n",
    "fig, axs = plt.subplots(1, nfold, sharex=True, sharey=True)\n",
    "fig.set_figwidth(15);\n",
    "fig.set_figheight(2);\n",
    "\n",
    "for isplit, idx in enumerate(idxval):\n",
    "        \n",
    "    x_train_kfold = df_tr_poly[:idx]\n",
    "    y_train_kfold = df_tr.deathIncrease.values[:idx]\n",
    "    x_val_kfold = df_tr_poly[idx:idx+10]\n",
    "    y_val_kfold = df_tr.deathIncrease.values[idx:idx+10]\n",
    "\n",
    "    p = sns.scatterplot(x=x_train_kfold[:,0], y=y_train_kfold, ax=axs[isplit]);\n",
    "    p = sns.scatterplot(x=x_val_kfold[:,0], y=y_val_kfold, ax=axs[isplit]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this cross validation to evaluate different polynomial model orders. For each model order, and each “fold”, we’ll compute the training and validation MSE and R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model orders to be tested\n",
    "dmax = 4\n",
    "dtest_list = np.arange(1,dmax+1)\n",
    "nd = len(dtest_list)\n",
    "\n",
    "idxval = [10, 20, 30, 40, 50]\n",
    "nfold = len(idxval)\n",
    "\n",
    "mse_val = np.zeros((nd,nfold))\n",
    "r2_val  = np.zeros((nd,nfold))\n",
    "mse_tr  = np.zeros((nd,nfold))\n",
    "r2_tr   = np.zeros((nd,nfold))\n",
    "\n",
    "# loop over the folds\n",
    "# the first loop variable tells us how many out of nfold folds we have gone through\n",
    "# the second loop variable tells us how to split the data\n",
    "for isplit, idx in enumerate(idxval):\n",
    "        \n",
    "    x_train_kfold = df_tr_poly[:idx]\n",
    "    y_train_kfold = df_tr.deathIncrease.values[:idx]\n",
    "    x_val_kfold = df_tr_poly[idx:idx+10]\n",
    "    y_val_kfold = df_tr.deathIncrease.values[idx:idx+10]\n",
    "\n",
    "    for didx, dtest in enumerate(dtest_list):\n",
    "\n",
    "      # get transformed features\n",
    "      x_train_dtest =  x_train_kfold[:, :dtest]\n",
    "      x_val_dtest   =  x_val_kfold[:, :dtest]\n",
    "\n",
    "      # fit data\n",
    "      reg_dtest = LinearRegression().fit(x_train_dtest, y_train_kfold)\n",
    "      \n",
    "      # measure MSE on validation data\n",
    "      y_hat = reg_dtest.predict(x_val_dtest)\n",
    "      mse_val[didx, isplit] = metrics.mean_squared_error(y_val_kfold, y_hat)\n",
    "      r2_val[didx, isplit] = metrics.r2_score(y_val_kfold, y_hat)\n",
    "\n",
    "      # measure MSE on training data\n",
    "      y_hat_tr = reg_dtest.predict(x_train_dtest)\n",
    "      mse_tr[didx, isplit] = metrics.mean_squared_error(y_train_kfold, y_hat_tr)\n",
    "      r2_tr[didx, isplit] = metrics.r2_score(y_train_kfold, y_hat_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the result, we see that the training R2 is reasonably good for the polynomial model (here, we show the mean across folds for each polynomial degree):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_tr.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the validation R2 tells a more accurate story: the model is wildly overfitting, and it has no predictive power whatsoever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_val.mean(axis=1)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 }
}

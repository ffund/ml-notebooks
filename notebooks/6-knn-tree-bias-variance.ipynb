{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias/variance of non-parametric models\n",
    "======================================\n",
    "\n",
    "*Fraida Fund*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate data from the true function\n",
    "\n",
    "$$ t(x) = e^{-x^2} + 1.5 e^{-(x-2)^2}$$\n",
    "\n",
    "in the range $-5 < x < 5$.\n",
    "\n",
    "To this we will add Gaussian noise $\\epsilon$ so that\n",
    "\n",
    "$$ y = t(x) + \\epsilon$$\n",
    "\n",
    "We will use this data for *all* of the models trained in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions to generate data\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "def generate(n_samples, noise, n_repeat=1):\n",
    "    X = np.random.rand(n_samples) * 10 - 5\n",
    "    X = np.sort(X)\n",
    "    if n_repeat == 1:\n",
    "        y = f(X) + np.random.normal(0.0, noise, n_samples)\n",
    "    else:\n",
    "        y = np.zeros((n_samples, n_repeat))\n",
    "        for i in range(n_repeat):\n",
    "            y[:, i] = f(X) + np.random.normal(0.0, noise, n_samples)\n",
    "\n",
    "    X = X.reshape((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up simulation\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation settings\n",
    "n_repeat = 500       # Number of iterations for computing expectations\n",
    "n_train  = 500        # Size of the training set\n",
    "n_test   = 1000       # Size of the test set\n",
    "noise    = 0.15       # Standard deviation of the noise\n",
    "np.random.seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simulation(estimators):\n",
    "\n",
    "  n_estimators = len(estimators)\n",
    "  plt.figure(figsize=(5*n_estimators, 10))\n",
    "\n",
    "  # Loop over estimators to compare\n",
    "  for n, (name, estimator) in enumerate(estimators):\n",
    "      # Compute predictions\n",
    "      y_predict = np.zeros((n_test, n_repeat))\n",
    "\n",
    "      for i in range(n_repeat):\n",
    "          estimator.fit(X_train[i].reshape(-1,1), y_train[i])\n",
    "          y_predict[:, i] = estimator.predict(X_test.reshape(-1,1))\n",
    "\n",
    "      # Bias^2 + Variance + Noise decomposition of the mean squared error\n",
    "      y_error = np.zeros(n_test)\n",
    "\n",
    "      for i in range(n_repeat):\n",
    "          for j in range(n_repeat):\n",
    "              y_error += (y_test[:, j] - y_predict[:, i]) ** 2\n",
    "\n",
    "      y_error /= (n_repeat * n_repeat)\n",
    "\n",
    "      y_noise = np.var(y_test, axis=1)\n",
    "      y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 2\n",
    "      y_var = np.var(y_predict, axis=1)\n",
    "\n",
    "      # Plot figures\n",
    "      plt.subplot(2, n_estimators, n + 1)\n",
    "      plt.plot(X_test, f(X_test), \"b\", label=\"$f(x)$\")\n",
    "      plt.plot(X_train[0], y_train[0],  \".b\", alpha=0.2, label=\"$y = f(x)+noise$\")\n",
    "\n",
    "      for i in range(20):\n",
    "          if i == 0:\n",
    "              plt.plot(X_test, y_predict[:, i], \"r\", label=r\"$\\^y(x)$\")\n",
    "          else:\n",
    "              plt.plot(X_test, y_predict[:, i], \"r\", alpha=0.1)\n",
    "\n",
    "      plt.plot(X_test, np.mean(y_predict, axis=1), \"c\",\n",
    "              label=r\"$E[ \\^y(x)]$\")\n",
    "\n",
    "      plt.xlim([-5, 5])\n",
    "      plt.title(name)\n",
    "\n",
    "      if n == n_estimators - 1:\n",
    "          plt.legend(loc=(1.1, .5))\n",
    "\n",
    "      plt.subplot(2, n_estimators, n_estimators + n + 1)\n",
    "      plt.plot(X_test, y_noise, \"c\", label=\"$noise(x)$\", alpha=0.3)\n",
    "      plt.plot(X_test, y_bias, \"b\", label=\"$bias^2(x)$\", alpha=0.6),\n",
    "      plt.plot(X_test, y_var, \"g\", label=\"$variance(x)$\", alpha=0.6),\n",
    "      plt.plot(X_test, y_error, \"r\", label=\"$error(x)$\", alpha=0.4)\n",
    "      plt.title(\"{0:.4f} (error) = {1:.4f} (bias^2) \\n\"\n",
    "            \" + {2:.4f} (var) + {3:.4f} (noise)\".format( np.mean(y_error),\n",
    "                                                        np.mean(y_bias),\n",
    "                                                        np.mean(y_var),\n",
    "                                                        np.mean(y_noise)))\n",
    "\n",
    "      plt.xlim([-5, 5])\n",
    "      plt.ylim([0, 0.1])\n",
    "\n",
    "      if n == n_estimators - 1:\n",
    "\n",
    "          plt.legend(loc=(1.1, .5))\n",
    "\n",
    "  plt.subplots_adjust(right=.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = generate(n_samples=n_test, noise=noise, n_repeat=n_repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros(shape=(n_repeat, n_train))\n",
    "y_train = np.zeros(shape=(n_repeat, n_train))\n",
    "\n",
    "for i in range(n_repeat):\n",
    "    X, y = generate(n_samples=n_train, noise=noise)\n",
    "    X_train[i] = X.ravel()\n",
    "    y_train[i] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(X_test, f(X_test), \"b\", label=\"$f(x)$\");\n",
    "plt.plot(X_train[0], y_train[0],  \".b\", alpha=0.2, label=\"$y = f(x)+noise$\");\n",
    "plt.legend(loc=(1.1, .5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Nearest Neighbors\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following KNN regression models. Which model will have more bias? Which model will have more variance?\n",
    "\n",
    "-   **Model A**: K = 1\n",
    "-   **Model B**: K = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [(\"1-NN\", KNeighborsRegressor(n_neighbors=1)),\n",
    "              (\"75-NN\", KNeighborsRegressor(n_neighbors=75))]\n",
    "\n",
    "plot_simulation(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree by depth\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following decision tree regression models. Which model will have more bias? Which model will have more variance?\n",
    "\n",
    "-   **Model A**: Max depth = 5\n",
    "-   **Model B**: Max depth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [(\"DT - depth <= 5\",  DecisionTreeRegressor(max_depth=5)),\n",
    "              (\"DT - depth <= 100\", DecisionTreeRegressor(max_depth=100))]\n",
    "\n",
    "plot_simulation(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree by pruning parameter\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we use cost complexity tuning to train the decision tree that minimizes\n",
    "\n",
    "$$\\sum_{m=1}^{|T|} \\sum_{x_i}^{R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha |T| $$\n",
    "\n",
    "Consider the following decision tree regression models. Which model will have more bias? Which model will have more variance?\n",
    "\n",
    "-   **Model A**: $\\alpha = 0.00001$\n",
    "-   **Model B**: $\\alpha = 0.001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [(\"DT - α = 10e-6\",   DecisionTreeRegressor(ccp_alpha=10e-6)),\n",
    "              (\"DT - α = 10e-4\", DecisionTreeRegressor(ccp_alpha=10e-4))]\n",
    "\n",
    "plot_simulation(estimators)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "colab": {
   "toc_visible": "true"
  }
 }
}

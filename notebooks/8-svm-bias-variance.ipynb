{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias and variance of SVMs\n",
    "=========================\n",
    "\n",
    "*Fraida Fund*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore the bias and variance of SVM models, and see how we can tune this tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to train a model to classify two “blobs” of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 100\n",
    "n_test = 500\n",
    "n_train = 100\n",
    "sigma= 0.8\n",
    "cluster_centers = np.array([[-1,1],[2,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.zeros((n_test, n_repeat, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = make_blobs(n_samples=n_test, centers=cluster_centers,\n",
    "                  random_state=0, cluster_std=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_test[:,0], y=x_test[:,1], hue=y_test);\n",
    "\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");\n",
    "plt.xlim(-5,5);\n",
    "plt.ylim(-2,5);\n",
    "\n",
    "# get the true decision boundary \n",
    "mid = [cluster_centers[:,0].mean(), cluster_centers[:,1].mean()]\n",
    "slp = -1.0/((cluster_centers[1,1]-cluster_centers[0,1])/(cluster_centers[1,0]-cluster_centers[0,0]))\n",
    "b = mid[1]-slp*mid[0]\n",
    "x_true = np.arange(-5,5)\n",
    "y_true = slp*x_true + b\n",
    "sns.lineplot(x=x_true, y=y_true, color='black', label=\"True decision boundary\")\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which will have greater bias, and which will have greater variance?\n",
    "\n",
    "-   **Model A**: Linear SVM with $C=0.01$\n",
    "\n",
    "-   **Model B**: Linear SVM with $C=100$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: here is $C$ in the SVM problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname*{minimize}_{\\mathbf{w}, \\mathbf{\\epsilon}} \\quad & \\frac{1}{2} \\sum_{j=1}^p w_j^2  + C \\sum_{i=1}^n \\epsilon_i  \\\\\n",
    "\\text{subject to} \\quad & y_i(w_0 + \\sum_{j=1}^p w_j x_{ij}) \\geq 1-\\epsilon_i, \\quad \\forall i \\\\\n",
    "& \\epsilon_i \\geq 0, \\quad \\forall i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The greater the value of $C$, the more heavily the “margin violators” penalize the overall objective function. Therefore,\n",
    "\n",
    "-   If $C$ is large, the margin must be narrow (with few “margin violators”).\n",
    "-   If $C$ is small, the margin may be wider (with more “margin violators”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_sim = np.zeros((40000, n_repeat, 2))\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax_a, ax_b = fig.subplots(1, 2, sharex=True, sharey=True)\n",
    "\n",
    "# now simulate training the model many times, on different training data every time\n",
    "# and evaluate using the test data\n",
    "for i in tqdm(range(n_repeat), total=n_repeat, desc=\"Simulation iteration\"):\n",
    " \n",
    "  # train both models on newly generated training data\n",
    "  X, y = make_blobs(n_samples=n_test, centers=cluster_centers,\n",
    "                  cluster_std=sigma)\n",
    "\n",
    "  clf_a = SVC(kernel='linear', C=0.01).fit(X, y)\n",
    "  clf_b = SVC(kernel='linear', C=100.0).fit(X, y)\n",
    "\n",
    "  y_predict[:, i, 0] = clf_a.predict(x_test)\n",
    "  y_predict[:, i, 1] = clf_b.predict(x_test)\n",
    "\n",
    "\n",
    "  xx, yy = np.meshgrid(np.arange(-5, 5, .05),\n",
    "                      np.arange(-5, 5, .05))\n",
    "  \n",
    "  Z = clf_a.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "  Z_sim[:, i, 0] = Z\n",
    "  Z = Z.reshape(xx.shape)\n",
    "  ax_a.contour(xx, yy, Z, levels=[0.5], alpha=0.1, colors='plum');\n",
    "\n",
    "  plt.xlim(-5,5);\n",
    "  plt.ylim(-2,5);\n",
    "\n",
    "  Z = clf_b.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "  Z_sim[:, i, 1] = Z\n",
    "  Z = Z.reshape(xx.shape)\n",
    "  ax_b.contour(xx, yy, Z, levels=[0.5], alpha=0.1, colors='plum');\n",
    "\n",
    "  plt.xlim(-5,5);\n",
    "  plt.ylim(-2,5);\n",
    "\n",
    "\n",
    "cs_a = ax_a.contour(xx, yy, Z_sim[:,:,0].mean(axis=1).reshape(200,200), levels=[0.5], colors='magenta', linewidths=2);\n",
    "cs_b = ax_b.contour(xx, yy, Z_sim[:,:,1].mean(axis=1).reshape(200,200), levels=[0.5], colors='magenta', linewidths=2);\n",
    "\n",
    "# plot data\n",
    "sns.scatterplot(x=x_test[:,0], y=x_test[:,1], hue=y_test, ax=ax_a, legend=False);\n",
    "sns.scatterplot(x=x_test[:,0], y=x_test[:,1], hue=y_test, ax=ax_b, legend=False);\n",
    "\n",
    "sns.lineplot(x=x_true, y=y_true, color='black', ax=ax_a)\n",
    "sns.lineplot(x=x_true, y=y_true, color='black', ax=ax_b)\n",
    "\n",
    "\n",
    "ax_a.set_title(\"Model A\");\n",
    "ax_b.set_title(\"Model B\");\n",
    "\n",
    "ax_a.set_ylabel(\"x2\");\n",
    "ax_a.set_xlabel(\"x1\");\n",
    "ax_b.set_xlabel(\"x1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_polynomial_classifier_data(n=100, xrange=[-1,1], coefs=[1,0.5,0,2], sigma=0.5):\n",
    "  x = np.random.uniform(xrange[0], xrange[1], size=(n, 2))\n",
    "  ysep = np.polynomial.polynomial.polyval(x[:,0],coefs)\n",
    "  y = (x[:,1]>ysep).astype(int)\n",
    "  x[:,0] = x[:,0] + sigma * np.random.randn(n)\n",
    "  x[:,1] = x[:,1] + sigma * np.random.randn(n)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 100\n",
    "n_test = 500\n",
    "n_train = 1000\n",
    "sigma= 0.3\n",
    "coefs=np.array([0.3, 1, -1.5, -2])\n",
    "xrange=[-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.zeros((n_test, n_repeat, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data once\n",
    "x_test, y_test = generate_polynomial_classifier_data(n=n_test, xrange=xrange, coefs=coefs, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_test[:,0], y=x_test[:,1], hue=y_test);\n",
    "\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");\n",
    "plt.xlim((xrange[0], xrange[1]));\n",
    "plt.ylim((xrange[0], xrange[1]));\n",
    "\n",
    "# Plot true function\n",
    "xtrue = np.arange(-1.5, 1.5, .05)\n",
    "ytrue = np.polynomial.polynomial.polyval(xtrue,coefs) \n",
    "sns.lineplot(x=xtrue, y=ytrue, color='black', label='True decision boundary');\n",
    "\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to train a model to classify data that is separated by a polynomial boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which will have greater bias, and which will have greater variance?\n",
    "\n",
    "-   **Model A**: SVM with linear kernel, $C = 1$\n",
    "\n",
    "-   **Model B**: SVM with RBF kernel, $C = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_sim = np.zeros((3600, n_repeat, 2))\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax_a, ax_b = fig.subplots(1, 2, sharex=True, sharey=True)\n",
    "\n",
    "# now simulate training the model many times, on different training data every time\n",
    "# and evaluate using the test data\n",
    "for i in tqdm(range(n_repeat), total=n_repeat, desc=\"Simulation iteration\"):\n",
    "\n",
    "  # train both models on newly generated training data\n",
    "  X, y = generate_polynomial_classifier_data(n=n_train, xrange=xrange, coefs=coefs, sigma=sigma)\n",
    "\n",
    "  clf_a = SVC(kernel='linear', C=1).fit(X, y)\n",
    "  clf_b = SVC(kernel='rbf', gamma=10,  C=1).fit(X, y)\n",
    "\n",
    "  y_predict[:, i, 0] = clf_a.predict(x_test)\n",
    "  y_predict[:, i, 1] = clf_b.predict(x_test)\n",
    "\n",
    "\n",
    "  xx, yy = np.meshgrid(np.arange(-1.5, 1.5, .05),\n",
    "                      np.arange(-1.5, 1.5, .05))\n",
    "  \n",
    "  Z = clf_a.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "  Z_sim[:, i, 0] = Z\n",
    "  Z = Z.reshape(xx.shape)\n",
    "  ax_a.contour(xx, yy, Z, levels=[0.5], alpha=0.1, colors='plum');\n",
    "  plt.xlim((xrange[0], xrange[1]));\n",
    "  plt.ylim((xrange[0], xrange[1]));\n",
    "\n",
    "  Z = clf_b.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "  Z_sim[:, i, 1] = Z\n",
    "  Z = Z.reshape(xx.shape)\n",
    "  ax_b.contour(xx, yy, Z, levels=[0.5], alpha=0.1, colors='plum');\n",
    "  plt.xlim((xrange[0], xrange[1]));\n",
    "  plt.ylim((xrange[0], xrange[1]));\n",
    "\n",
    "\n",
    "cs_a = ax_a.contour(xx, yy, Z_sim[:,:,0].mean(axis=1).reshape(60,60), levels=[0.5], colors='magenta', linewidths=2);\n",
    "cs_b = ax_b.contour(xx, yy, Z_sim[:,:,1].mean(axis=1).reshape(60,60), levels=[0.5], colors='magenta', linewidths=2);\n",
    "\n",
    "\n",
    "# Plot true function\n",
    "xtrue = np.arange(-1.5, 1.5, .05)\n",
    "ytrue = np.polynomial.polynomial.polyval(xtrue,coefs) \n",
    "sns.lineplot(x=xtrue, y=ytrue, color='black', ax=ax_a);\n",
    "sns.lineplot(x=xtrue, y=ytrue, color='black', ax=ax_b);\n",
    "\n",
    "sns.scatterplot(x=x_test[:,0], y=x_test[:,1], hue=y_test, ax=ax_a, legend=False, alpha=0.1);\n",
    "sns.scatterplot(x=x_test[:,0], y=x_test[:,1], hue=y_test, ax=ax_b, legend=False, alpha=0.1);\n",
    "\n",
    "ax_a.set_title(\"Model A\");\n",
    "ax_b.set_title(\"Model B\");\n",
    "\n",
    "ax_a.set_ylabel(\"x2\");\n",
    "ax_a.set_xlabel(\"x1\");\n",
    "ax_b.set_xlabel(\"x1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the RBF kernel is defined as\n",
    "\n",
    "$$K(x,z) = \\exp(-\\frac{\\|x-z\\|^2_2}{\\sigma^2})$$\n",
    "\n",
    "where $\\sigma$ is the bandwidth, or equivalently, using a $\\gamma$ parameter,\n",
    "\n",
    "$$K(x, z) = \\exp(-\\gamma \\| x - z \\|^2_2 )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, here is the RBF kernel centered on a single point, computed over the entire feature space, with two different values of $\\gamma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "test_point = np.random.uniform(0,1,size=2)\n",
    "xx, yy = np.meshgrid(np.arange(0 ,5, .05), np.arange(0, 5, .05))\n",
    "\n",
    "gamma_a=0.05\n",
    "gamma_b=5\n",
    "Z_a = rbf_kernel(np.c_[xx.ravel(), yy.ravel()], test_point.reshape(1, -1), gamma=gamma_a)\n",
    "Z_b = rbf_kernel(np.c_[xx.ravel(), yy.ravel()], test_point.reshape(1, -1), gamma=gamma_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "plt.scatter(x=test_point[0], y=test_point[1])\n",
    "cs = plt.contourf(xx, yy, Z_a.reshape(xx.shape), vmin=0, vmax=1);\n",
    "plt.title(\"Gamma: %f\" % gamma_a);\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "\n",
    "plt.scatter(x=test_point[0], y=test_point[1])\n",
    "cs = plt.contourf(xx, yy, Z_b.reshape(xx.shape), vmin=0, vmax=1);\n",
    "plt.title(\"Gamma: %f\" % gamma_b);\n",
    "\n",
    "plt.subplots_adjust(right=0.8);\n",
    "cbar_ax = plt.axes([0.85, 0.15, 0.05, 0.7]);\n",
    "plt.colorbar(cax=cbar_ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when the kernel bandwidth is large ($\\gamma$ is small), the influence of each point extends much farther in the feature space than if the kernel bandwidth is small ($\\gamma$ is large)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to train a model to classify data that is separated by a polynomial boundary.\n",
    "\n",
    "Which will have greater bias, and which will have greater variance?\n",
    "\n",
    "-   **Model A**: SVM with RBF kernel and $\\gamma=0.05$\n",
    "\n",
    "-   **Model B**: SVM with RBF kernel and $\\gamma=5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 100\n",
    "n_test = 500\n",
    "n_train = 100\n",
    "sigma= 0.3\n",
    "coefs=np.array([0.3, 1, -1.5, -2])\n",
    "xrange=[-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.zeros((n_test, n_repeat, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data once\n",
    "x_test, y_test = generate_polynomial_classifier_data(n=n_test, xrange=xrange, coefs=coefs, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_sim = np.zeros((3600, n_repeat, 2))\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax_a, ax_b = fig.subplots(1, 2, sharex=True, sharey=True)\n",
    "\n",
    "# now simulate training the model many times, on different training data every time\n",
    "# and evaluate using the test data\n",
    "for i in tqdm(range(n_repeat), total=n_repeat, desc=\"Simulation iteration\"):\n",
    "\n",
    "  # train both models on newly generated training data\n",
    "  X, y = generate_polynomial_classifier_data(n=n_train, xrange=xrange, coefs=coefs, sigma=sigma)\n",
    "\n",
    "  clf_a = SVC(kernel='rbf', gamma=0.05, C=10).fit(X, y)\n",
    "  clf_b = SVC(kernel='rbf', gamma=5, C=10).fit(X, y)\n",
    "\n",
    "  y_predict[:, i, 0] = clf_a.predict(x_test)\n",
    "  y_predict[:, i, 1] = clf_b.predict(x_test)\n",
    "\n",
    "\n",
    "  xx, yy = np.meshgrid(np.arange(-1.5, 1.5, .05),\n",
    "                      np.arange(-1.5, 1.5, .05))\n",
    "  \n",
    "  Z = clf_a.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "  Z_sim[:, i, 0] = Z\n",
    "  Z = Z.reshape(xx.shape)\n",
    "  ax_a.contour(xx, yy, Z, levels=[0.5], alpha=0.1, colors='plum');\n",
    "  plt.xlim((xrange[0], xrange[1]));\n",
    "  plt.ylim((xrange[0], xrange[1]));\n",
    "\n",
    "  Z = clf_b.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "  Z_sim[:, i, 1] = Z\n",
    "  Z = Z.reshape(xx.shape)\n",
    "  ax_b.contour(xx, yy, Z, levels=[0.5], alpha=0.1, colors='plum');\n",
    "  plt.xlim((xrange[0], xrange[1]));\n",
    "  plt.ylim((xrange[0], xrange[1]));\n",
    "\n",
    "\n",
    "cs_a = ax_a.contour(xx, yy, Z_sim[:,:,0].mean(axis=1).reshape(60,60), levels=[0.5], colors='magenta', linewidths=2);\n",
    "cs_b = ax_b.contour(xx, yy, Z_sim[:,:,1].mean(axis=1).reshape(60,60), levels=[0.5], colors='magenta', linewidths=2);\n",
    "\n",
    "\n",
    "# Plot true function\n",
    "xtrue = np.arange(-1.5, 1.5, .05)\n",
    "ytrue = np.polynomial.polynomial.polyval(xtrue,coefs) \n",
    "sns.lineplot(x=xtrue, y=ytrue, color='black', ax=ax_a);\n",
    "sns.lineplot(x=xtrue, y=ytrue, color='black', ax=ax_b);\n",
    "\n",
    "#sns.scatterplot(x_test[:,0], x_test[:,1], y_test, ax=ax_a, legend=False, alpha=0.1);\n",
    "#sns.scatterplot(x_test[:,0], x_test[:,1], y_test, ax=ax_b, legend=False, alpha=0.1);\n",
    "\n",
    "ax_a.set_title(\"Model A\");\n",
    "ax_b.set_title(\"Model B\");\n",
    "\n",
    "ax_a.set_ylabel(\"x2\");\n",
    "ax_a.set_xlabel(\"x1\");\n",
    "ax_b.set_xlabel(\"x1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter search\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For models with a single hyperparameter controlling bias-variance (for example: $k$ in $k$ nearest neighbors), we used sklearns's `KFoldCV` or `validation_curve` to test a range of values for the hyperparameter, and to select the best one.\n",
    "\n",
    "When we have *multiple* hyperparameters to tune, we can use `GridSearchCV` to select the best *combination* of them.\n",
    "\n",
    "For example, we just saw saw three ways to tune the bias-variance of an SVM classifier:\n",
    "\n",
    "-   Changing the kernel\n",
    "-   Changing $C$\n",
    "-   For an RBF kernel, changing $\\gamma$\n",
    "\n",
    "To get the best performance from an SVM classifier, we need to find the best *combination* of these hyperparameters. This notebook shows how to use `GridSearchCV` to tune an SVM classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with a subset of the MNIST handwritten digits data. First, we will get the data, and assign a small subset of samples to training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=1000, test_size=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try this initial parameter “grid”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [0.1, 1000], 'kernel': ['linear']},\n",
    "  {'C': [0.1, 1000], 'gamma': [0.01, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’l set up the grid search. We can use `fit` on it, just like any other `sklearn` model.\n",
    "\n",
    "I added `return_train_score=True` to my `GridSearchSV` so that it will show me training scores as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=100, n_jobs=-1, return_train_score=True)\n",
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inform our search, we will use our understanding of how SVMs work, and especially how the $C$ and $\\gamma$ parameters control the bias and variance of the SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s tackle the linear SVM first, since it’s faster to fit. We didn’t see any change in the accuracy when we vary $C$. So, we should extend the range of $C$ over which we search.\n",
    "\n",
    "I’ll try higher and lower values of $C$, to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [1e-6, 1e-4, 1e-2, 1e2, 1e4, 1e6], 'kernel': ['linear']},\n",
    " ]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=100, n_jobs=-1, return_train_score=True)\n",
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=pd.DataFrame(clf.cv_results_), x='param_C', y='mean_train_score', label=\"Training score\");\n",
    "sns.lineplot(data=pd.DataFrame(clf.cv_results_), x='param_C', y='mean_test_score', label=\"Validation score\");\n",
    "plt.xscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we get a slightly better validation score near the smaller values for $C$! What does this mean?\n",
    "\n",
    "Let’s try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': np.linspace(1e-5, 1e-7, num=10), 'kernel': ['linear']},\n",
    " ]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=100, n_jobs=-1, return_train_score=True)\n",
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=pd.DataFrame(clf.cv_results_), x='param_C', y='mean_train_score', label=\"Training score\");\n",
    "sns.lineplot(data=pd.DataFrame(clf.cv_results_), x='param_C', y='mean_test_score', label=\"Validation score\");\n",
    "plt.xscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can be satisfied that we have found a good hyperparameter only when we see the high bias AND high variance side of the validation curve!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s look at the RBF kernel.\n",
    "\n",
    "In our first search, the accuracy of the RBF kernel is very poor. We may have high bias, high variance, (or both)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $C=0.1$ in our first search, both training and validation scores were low. This suggests high bias.\n",
    "\n",
    "When $C=1000$ in our first search, training scores were high and validation scores were low. This suggests high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What next? We know from our discussion of bias and variance of SVMs that to combat overfitting, we can decrease $\\gamma$ and/or decrease $C$.\n",
    "\n",
    "For now, let’s keep the higher value of $C$, and try to reduce the overfitting by decreasing $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [1000], 'gamma': [1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11], 'kernel': ['rbf']},\n",
    " ]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(SVC(), param_grid, cv=2, refit=True, verbose=100, n_jobs=-1, return_train_score=True)\n",
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=pd.DataFrame(clf.cv_results_), x='param_gamma', y='mean_train_score', label=\"Training score\")\n",
    "sns.lineplot(data=pd.DataFrame(clf.cv_results_), x='param_gamma', y='mean_test_score', label=\"Validation score\")\n",
    "plt.xscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that (at least for $C=1000$), values of $\\gamma$ greater than `1e-5` seem to overfit, while decreasing $\\gamma$ lower than `1e-10` may underfit.\n",
    "\n",
    "But we know that changing $C$ also affects the bias variance tradeoff! For different values of $C$, the best value of $\\gamma$ will be different, and there may be a better *combination* of $C$ and $\\gamma$ than any we have seen so far. We can try to increase and decrease $C$ to see if that improves the validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a better idea of where to search, we can set up our “final” search grid.\n",
    "\n",
    "We know that to find the best validation accuracy for the linear kernel, we should make sure our search space includes `1e-6` and `1e-7`. I chose to vary $C$ from `1e-8` to `1e-4`. (I want to make sure the best value is not at the edge of the search space, so that we can be sure there isn’t a better value if we go lower/higher.)\n",
    "\n",
    "We know that to find the best validation accuracy for the RBF kernel, we should make sure our search space includes $\\gamma$ values around `1e-6` and `1e-7` when $C=1000$. For larger values of $C$, we expect that we’l get better results with smaller values of $\\gamma$. For smaller values of $C$, we expect that we’l get better results with larger values of $\\gamma$. I chose to vary $C$ from `1` to `1e6` and $\\gamma$ from `1e-4` to `1e-11`.\n",
    "\n",
    "That’s a big search grid, so this takes a long time to fit! (Try this at home with a larger training set to get an idea...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [1e-8, 1e-7, 1e-6, 1e-5, 1e-4], 'kernel': ['linear']},\n",
    "  {'C': [1, 1e2, 1e3, 1e4, 1e5, 1e6], 'gamma': [1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11], 'kernel': ['rbf']},\n",
    " ]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=100, n_jobs=-1, return_train_score=True)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the linear kernel, here's what we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv   = pd.DataFrame(clf.cv_results_)\n",
    "df_cv = df_cv[df_cv['param_kernel']=='linear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df_cv, x='param_C', y='mean_train_score', label=\"Training score\")\n",
    "sns.lineplot(data=df_cv, x='param_C', y='mean_test_score', label=\"Validation score\")\n",
    "plt.xscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the RBF kernel, here's what we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv   = pd.DataFrame(clf.cv_results_)\n",
    "df_cv = df_cv[df_cv['param_kernel']=='rbf']\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "ax1=plt.subplot(1,2,1)\n",
    "pvt = pd.pivot_table(df_cv, values='mean_test_score', index='param_C', columns='param_gamma')\n",
    "sns.heatmap(pvt, annot=True, cbar=False, vmin=0, vmax=1, cmap='PiYG');\n",
    "plt.title(\"Validation scores\");\n",
    "\n",
    "ax2=plt.subplot(1,2,2, sharey=ax1)\n",
    "plt.setp(ax2.get_yticklabels(), visible=False)\n",
    "pvt = pd.pivot_table(df_cv, values='mean_train_score', index='param_C', columns='param_gamma')\n",
    "sns.heatmap(pvt, annot=True, cbar=False, vmin=0, vmax=1, cmap='PiYG');\n",
    "plt.title(\"Training scores\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that $\\gamma$ and $C$ control the bias-variance tradeoff of the SVM model as follows.\n",
    "\n",
    "-   In the top left region, $C$ is small (the margin is wider) and $\\gamma$ is small (the kernel bandwidth is large). In this region, the model has more bias (is prone to underfit). The validation scores and training scores are both low.\n",
    "-   On the right side (and we'd expect to see this on the bottom right if we extend the range of $C$ even higher), $C$ is large (the margin is narrower) and $\\gamma$ is large (the kernel bandwidth is small. In this region, the model has more variance (is likely to overfit). The validation scores are low, but the training scores are high.\n",
    "\n",
    "In the middle, we have a region of good combinations of $C$ and $\\gamma$.\n",
    "\n",
    "Since the parameter grid above shows us the validation accuracy decreasing both as we increase each parameter\\* and also as we decrease each parameter, we can be a bit more confident that we captured the point in the bias-variance surface where the error is smallest.\n",
    "\n",
    "\\* $C$ is different because increasing $C$ even more may not actually change the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the “best” parameters, with which the model was re-fitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can evaluate the re-fitted model on the test set. (Note that the `GridSearchCV` only used the training set; we have not used the test set at all for model fitting.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred, y_test)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "colab": {
   "toc_visible": "true"
  }
 }
}

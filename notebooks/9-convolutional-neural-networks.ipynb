{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks\n",
    "\n",
    "*Fraida Fund*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will find out makes convolutional neural networks so powerful for computer vision applications!\n",
    "\n",
    "We will use three varieties of neural networks to classify our own handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for faster training, use Runtime \\> Change Runtime Type to run this notebook on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if you are using a GPU -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "  print(\"No GPU available.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the `device` above - throughout this notebook, we will move models and data to this `device` for computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a fully connected neural network on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will train a simple neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s get the MNIST dataset from Pytorch’s `datasets` module. We will also define some “transforms” that will be applied to each batch of data - in this case, we will make it into a “tensor” and normalize it using the mean and standard deviation of MNIST pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=0.1307, std=0.3081)\n",
    "])\n",
    "\n",
    "# get datasets with basic transform\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=basic_transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=basic_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will prepare data loaders. We will use early stopping, which is a kind of model selection, so we will need a separate validation set to be split out of the training set for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_size = len(train_dataset) // 6\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at one data sample - specifically, its shape: it is a 3D volume with dimensions \\[C, H, W\\] (number of channels, height, and width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a fully connected neural network, we will need to “flatten” the data from its current 3D 1x28x28 shape to a 1D 784 shape, using a `Flatten` layer right after the input.\n",
    "\n",
    "Then, we will have:\n",
    "\n",
    "-   One hidden layer with $N_H=512$ units, with ReLu activation.\n",
    "-   One output layer with $N_O=10$ units, one for each of the 10 possible classes.\n",
    "-   The output will be logits (pre-softmax), and then we can use `argmax` to find the most probable class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in mind, we can define our fully connected neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FCNet(nn.Module):\n",
    "    def __init__(self, input_dim=28*28, hidden_dim=512, output_dim=10):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)           # [B, 1, 28, 28] → [B, 784]\n",
    "        x = F.relu(self.hidden(x))   \n",
    "        x = self.output(x)           \n",
    "        return x                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fc = FCNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we have to select an optimizer and a loss function. Since this is a multi-class classification problem with logit output, we use `CrossEntropyLoss`. We use the Adam optimizer for our gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_fc.parameters(), lr=0.005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to train our network.\n",
    "\n",
    "We wil specify the number of epochs. But we will also configure the training process to stop before the max number of epochs, if no we hit a “peak” in the validation set accuracy and then fail to improve on it for “patience” consecutive epochs.\n",
    "\n",
    "We will also save the model weights each time we achieve a new “peak” performance on the validation set, and at the end, we will restore the weights that had the best performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0\n",
    "best_model_state = None\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "train_loss_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    # Train for one epoch\n",
    "    model_fc.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_fc(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Evaluate on validation set at the end of the epoch\n",
    "    model_fc.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_fc(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "    train_loss_history.append(running_loss)\n",
    "    val_acc_history.append(val_acc)\n",
    "\n",
    "    # Check if early stopping should be triggered\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model_fc.state_dict(), \"best_fc_model.pt\")  # save model weights to file if it's the best so far\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Epoch {epoch+1:03d} - Loss: {running_loss:.4f} - Val Acc: {val_acc:.4f} - Early Stopping Triggered\")\n",
    "            break\n",
    "            \n",
    "    print(f\"Epoch {epoch+1:03d} - Loss: {running_loss:.4f} - Val Acc: {val_acc:.4f} - Patience Counter {counter}\")\n",
    "\n",
    "# Restore whatever \"best model\" we found\n",
    "model_fc.load_state_dict(torch.load(\"best_fc_model.pt\"))\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make predictions with our fitted model, and compute accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fc.eval()  # Set model to evaluation mode\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model_fc (images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "# Compute test accuracy\n",
    "test_acc = (np.array(y_true) == np.array(y_pred)).mean()\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network does OK, although not great! Currently, the state of the art (best result) on the MNIST dataset is 0.21% classification error - you can see some of the best-performing methods at [this link](https://benchmarks.ai/mnist).\n",
    "\n",
    "Looking at some of the samples that are misclassified by our fully connected network, we can see that many of these samples are difficult for humans to classify as well. (Some may even be labeled incorrectly!) The human error rate on MNIST is likely to be about 0.2-0.3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(y_pred).flatten()\n",
    "y_true = np.array(y_true).flatten()\n",
    "\n",
    "# find misclassified samples\n",
    "mis_idx = np.where(y_pred != y_true)[0]\n",
    "num_samples = min(10, len(mis_idx))  \n",
    "chosen_idx = np.random.choice(mis_idx, num_samples, replace=False)\n",
    "\n",
    "plt.figure(figsize=(num_samples * 1.25, 2))\n",
    "for i, idx in enumerate(chosen_idx):\n",
    "    image, _ = test_dataset[idx]\n",
    "    plt.subplot(1, num_samples, i + 1)\n",
    "    sns.heatmap(image.squeeze(), cmap=plt.cm.gray, cbar=False,\n",
    "                xticklabels=False, yticklabels=False)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Idx {idx}\\nTrue: {y_true[idx]}\\nPred: {y_pred[idx]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try our fully connected neural network on our own test sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s try to classify our own test sample (as in a previous notebook, when we did this for a logistic regression model).\n",
    "\n",
    "On a plain white piece of paper, in a black or other dark-colored pen, write a digit of your choice from 0 to 9. Take a photo of your handwritten digit.\n",
    "\n",
    "Edit your photo (crop, rotate as needed), using a photo editor of your choice (I used Google Photos), so that your photo is approximately square, and includes only the digit and the white background. Upload your image here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    " \n",
    "filename = 'input.png'\n",
    " \n",
    "image = Image.open(filename)\n",
    "p = plt.imshow(np.asarray(image), cmap=plt.cm.gray,);\n",
    "p = plt.title('Shape: ' + str(np.asarray(image).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to grayscale image - 'L' format means each pixel is \n",
    "# represented by a single value from 0 to 255\n",
    "image_bw = image.convert('L')\n",
    "p = plt.imshow(np.asarray(image_bw), cmap=plt.cm.gray,);\n",
    "p = plt.title('Shape: ' + str(np.asarray(image_bw).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize image \n",
    "image_bw_resized = image_bw.resize((28,28), Image.BICUBIC)\n",
    "p = plt.imshow(np.asarray(image_bw_resized), cmap=plt.cm.gray,);\n",
    "p = plt.title('Shape: ' + str(np.asarray(image_bw_resized).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert image, to match training data\n",
    "import PIL.ImageOps    \n",
    "\n",
    "image_bw_resized_inverted = PIL.ImageOps.invert(image_bw_resized)\n",
    "p = plt.imshow(np.asarray(image_bw_resized_inverted), cmap=plt.cm.gray,);\n",
    "p = plt.title('Shape: ' + str(np.asarray(image_bw_resized_inverted).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, turn to a numpy array\n",
    "test_sample = np.array(image_bw_resized_inverted).reshape(1, 28, 28)\n",
    "p = plt.imshow(np.reshape(test_sample, (28,28)), cmap=plt.cm.gray,);\n",
    "p = plt.title('Shape: ' + str(test_sample.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can predict the class of this sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = basic_transform(test_sample.squeeze())  # shape: [1, 28, 28] → [28, 28] → tensor [1, 28, 28]\n",
    "input_tensor = input_tensor.unsqueeze(0).to(device)  # shape: [1, 1, 28, 28]\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fc.eval()\n",
    "with torch.no_grad():\n",
    "    output = model_fc(input_tensor)\n",
    "    probabilities = torch.softmax(output, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=np.arange(0,10), y=probabilities.cpu().squeeze());\n",
    "plt.ylabel(\"Probability\");\n",
    "plt.xlabel(\"Class\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to try\n",
    "\n",
    "-   What if we use a test sample where the image is not so well centered?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fully connected neural network was OK, but for images, there are important reasons why we will often prefer a convolutional neural network instead:\n",
    "\n",
    "-   Dimension - images can have a huge number of pixels, and for image classification problems, we can also have a very large number of possible classes. A deep, fully connected network for these problems will have a *lot* of weights to learn.\n",
    "-   Images (and videos!) have a structure that is wasted on the fully connected network.\n",
    "-   Relevant features may be anywhere in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea behind convolutional neural networks is that a “neuron” is connected to a small part of image at a time (locally connected).\n",
    "\n",
    "By having multiple locally connected neurons covering the entire image, we effectively “scan” the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does convolution do? Let’s look at a visual example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a horizontal Sobel filter, which detects horizontal edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_sobel = np.array([[1,2,1],[0,0,0],[-1,-2,-1]])\n",
    "plt.imshow(horizontal_sobel, cmap='RdBu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an image of random noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.random.uniform(0,1,size=(10,10))\n",
    "plt.imshow(img, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution of the Sobel filter and the random image doesn’t pick up anything interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "img_conv = signal.correlate2d(img, horizontal_sobel, mode='same')\n",
    "plt.imshow(img_conv, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the convolution of the Sobel filter and this digit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_index = 3675\n",
    "img = test_dataset[img_index][0].squeeze().cpu().numpy()\n",
    "plt.imshow(img.reshape(28,28), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_conv = signal.correlate2d(img.reshape(28,28), horizontal_sobel, mode='same')\n",
    "plt.imshow(img_conv, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a vertical Sobel filter, which detects vertical edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_sobel =  np.array([[-1,0,1],[-2,0,2],[-1,0,1]])\n",
    "plt.imshow(vertical_sobel, cmap='RdBu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look what it finds in the digit -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_conv = signal.correlate2d(img.reshape(28,28), vertical_sobel, mode='same')\n",
    "plt.imshow(img_conv, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional layer is like an array of these filters - each one “sweeps” the image and looks for a different high-level “feature”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Attribution: this example is based on a post by [Victor Zhou](https://victorzhou.com/blog/intro-to-cnns-part-1/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a great interactive demo of the Sobel filters in [this tutorial on edge detection](https://cse442-17f.github.io/Sobel-Laplacian-and-Canny-Edge-Detection-Algorithms/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a convolutional neural network on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next section, we will train a convolutional neural network.\n",
    "\n",
    "We have explicitly defined it in two parts, a feature extraction part and a classification part, for clarify.\n",
    "\n",
    "Also, we will try to improve performance using the following techniques:\n",
    "\n",
    "-   **Dropout layers**: Because deep networks can be prone to overfitting, we will also add *dropout* layers to our network architecture. In each training stage, a dropout layer will “zero” a random selection of outputs (just for that stage). You can read more about this technique in [this paper](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf).\n",
    "-   **Batch normalization**: This technique re-scales and centers the data in the mini-batch when applied between layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this time we do *not* `Flatten` the data at the input. The input is processed directly as a 3D volume through a sequence of `Conv2D`, `BatchNormalization`, `Activation`, `MaxPooling2D`, and `Dropout` layers, and finally `Flatten` and `Dense` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv Layer 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3),         # input: [B, 1, 28, 28] → [B, 32, 26, 26]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Conv Layer 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3),        # → [B, 32, 24, 24]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),             # → [B, 32, 12, 12]\n",
    "\n",
    "            # Conv Layer 3\n",
    "            nn.Conv2d(32, 64, kernel_size=3),        # → [B, 64, 10, 10]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Conv Layer 4\n",
    "            nn.Conv2d(64, 64, kernel_size=3),        # → [B, 64, 8, 8]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),             # → [B, 64, 4, 4]\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),                            # → [B, 64*4*4 = 1024]\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, n_classes)                # logits output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_conv.parameters(), lr=0.005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will train the model with early stopping for up to 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0\n",
    "best_model_state = None\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "train_loss_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    # Train for one epoch\n",
    "    model_conv.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_conv(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Evaluate on validation set at the end of the epoch\n",
    "    model_conv.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_conv(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "    train_loss_history.append(running_loss)\n",
    "    val_acc_history.append(val_acc)\n",
    "\n",
    "    # Check if early stopping should be triggered\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model_conv.state_dict(), \"best_conv_model.pt\")  # save model weights to file if it's the best so far\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Epoch {epoch+1:03d} - Loss: {running_loss:.4f} - Val Acc: {val_acc:.4f} - Early Stopping Triggered\")\n",
    "            break\n",
    "            \n",
    "    print(f\"Epoch {epoch+1:03d} - Loss: {running_loss:.4f} - Val Acc: {val_acc:.4f} - Patience Counter {counter}\")\n",
    "\n",
    "# Restore whatever \"best model\" we found\n",
    "model_conv.load_state_dict(torch.load(\"best_conv_model.pt\"))\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv.eval()  # Set model to evaluation mode\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model_conv (images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "# Compute test accuracy\n",
    "test_acc = (np.array(y_true) == np.array(y_pred)).mean()\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a much better result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some of the samples that are misclassified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(y_pred).flatten()\n",
    "y_true = np.array(y_true).flatten()\n",
    "\n",
    "# find misclassified samples\n",
    "mis_idx = np.where(y_pred != y_true)[0]\n",
    "num_samples = min(10, len(mis_idx))  \n",
    "chosen_idx = np.random.choice(mis_idx, num_samples, replace=False)\n",
    "\n",
    "plt.figure(figsize=(num_samples * 1.25, 2))\n",
    "for i, idx in enumerate(chosen_idx):\n",
    "    image, _ = test_dataset[idx]\n",
    "    plt.subplot(1, num_samples, i + 1)\n",
    "    sns.heatmap(image.squeeze(), cmap=plt.cm.gray, cbar=False,\n",
    "                xticklabels=False, yticklabels=False)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Idx {idx}\\nTrue: {y_true[idx]}\\nPred: {y_pred[idx]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try our convolutional neural network on our own test sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this convolutional neural network to predict the class of the test sample we uploaded previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_sample.reshape(28, 28), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv.eval()\n",
    "with torch.no_grad():\n",
    "    output = model_conv(input_tensor)\n",
    "    probabilities = torch.softmax(output, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=np.arange(0,10), y=probabilities.cpu().squeeze());\n",
    "plt.ylabel(\"Probability\");\n",
    "plt.xlabel(\"Class\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at output of convolutional layers\n",
    "\n",
    "Because deep learning is so complex, it can be difficult to understand why it makes the decisions it does. One way to better understand the behavior of a neural network is to visualize the output of each layer for a given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### Visualization magic\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interact, IntSlider\n",
    "from IPython.display import display\n",
    "\n",
    "activations = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "layer_names = []\n",
    "for name, layer in model_conv.named_modules():\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        pre_name = f\"{name}_pre\"\n",
    "        layer.register_forward_hook(get_activation(pre_name))\n",
    "        layer_names.append(pre_name)\n",
    "    if isinstance(layer, nn.ReLU):\n",
    "        layer.register_forward_hook(get_activation(name))\n",
    "        layer_names.append(name)\n",
    "\n",
    "def visualize_layer(test_idx=0, layer_idx=0):\n",
    "    model_conv.eval()\n",
    "    img, _ = test_dataset[test_idx]\n",
    "    x_tensor = img.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        _ = model_conv(x_tensor)\n",
    "    name = layer_names[layer_idx]\n",
    "    fmap = activations[name].squeeze(0)\n",
    "    n_channels = fmap.shape[0]\n",
    "    if fmap.ndim == 1 or (fmap.shape[1:] == torch.Size([1, 1])):\n",
    "        fmap_flat = fmap.view(-1).numpy()\n",
    "        plt.figure(figsize=(len(fmap_flat) * 0.25, 2))\n",
    "        plt.imshow(fmap_flat[np.newaxis, :], cmap='gray', aspect='auto')\n",
    "        plt.title(f\"{name} (Vector Output)\")\n",
    "        plt.yticks([]); plt.xticks(range(len(fmap_flat)))\n",
    "        plt.show()\n",
    "        return\n",
    "    n_cols = int(np.ceil(np.sqrt(n_channels)))\n",
    "    n_rows = int(np.ceil(n_channels / n_cols))\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 2, n_rows * 2))\n",
    "    fig.suptitle(name)\n",
    "    for i in range(n_channels):\n",
    "        ax = axes.flat[i]\n",
    "        ax.imshow(fmap[i], cmap='gray')\n",
    "        ax.set_title(f\"{i}\", fontsize=8)\n",
    "        ax.axis('off')\n",
    "    for i in range(n_channels, len(axes.flat)):\n",
    "        axes.flat[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "widget = interact(\n",
    "    visualize_layer,\n",
    "    test_idx=IntSlider(0, 0, len(test_dataset) - 1, step=1, description='Test Index'),\n",
    "    layer_idx=IntSlider(0, 0, len(layer_names) - 1, step=1, description='Layer Index')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the convolutional layers close to the input capture small details, while those close to the output of the model capture more general features that are less sensitive to local variations in the input image. We can see this characteristic in the visualizations above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and restoring a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this model took a long time to train, it may be useful to save the results, so that we can re-use the model later without having to re-train. We can save the weights of a model using `torch.save`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_conv.state_dict(), \"saved_model.pt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you click on the folder icon in the menu on the left side of the Colab window, you can see this file in your workspace. You can download the file for later use.\n",
    "\n",
    "To use the model again in the future, you can load it using `load_model`, then use it to make predictions without having to train it.\n",
    "\n",
    "Here we define a brand-new instance of a `ConvNet`, and then load the already-trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv_new = ConvNet().to(device)\n",
    "model_conv_new.load_state_dict(torch.load(\"saved_model.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try one more way to improve the model performance:\n",
    "\n",
    "-   **Data augmentation**: To supply more training samples, we can provide slightly modified versions of training samples - for example, samples with a small rotation applied - on which to train the model.\n",
    "\n",
    "We will implement this by modifying the `transform` that is passed to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_transform = transforms.Compose([\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "    ], p=1.0),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=10,                # rotate between [-10°, +10°]\n",
    "        translate=(0.1, 0.1),      # shift up to 10% in x and y\n",
    "        scale=(0.95, 1.05)         # zoom in/out slightly\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# get datasets with augmented transform\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=aug_transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=aug_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will train a `ConvNet` exactly as before, but using this augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aug = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_aug.parameters(), lr=0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0\n",
    "best_model_state = None\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "train_loss_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    # Train for one epoch\n",
    "    model_aug.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_aug(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Evaluate on validation set at the end of the epoch\n",
    "    model_aug.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_aug(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "    train_loss_history.append(running_loss)\n",
    "    val_acc_history.append(val_acc)\n",
    "\n",
    "    # Check if early stopping should be triggered\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model_aug.state_dict(), \"best_aug_model.pt\")  # save model weights to file if it's the best so far\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Epoch {epoch+1:03d} - Loss: {running_loss:.4f} - Val Acc: {val_acc:.4f} - Early Stopping Triggered\")\n",
    "            break\n",
    "            \n",
    "    print(f\"Epoch {epoch+1:03d} - Loss: {running_loss:.4f} - Val Acc: {val_acc:.4f} - Patience Counter {counter}\")\n",
    "\n",
    "# Restore whatever \"best model\" we found\n",
    "model_aug.load_state_dict(torch.load(\"best_aug_model.pt\"))\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aug.eval()  # Set model to evaluation mode\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model_aug (images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "# Compute test accuracy\n",
    "test_acc = (np.array(y_true) == np.array(y_pred)).mean()\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some of the samples that are misclassified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(y_pred).flatten()\n",
    "y_true = np.array(y_true).flatten()\n",
    "\n",
    "# find misclassified samples\n",
    "mis_idx = np.where(y_pred != y_true)[0]\n",
    "num_samples = min(10, len(mis_idx))  \n",
    "chosen_idx = np.random.choice(mis_idx, num_samples, replace=False)\n",
    "\n",
    "plt.figure(figsize=(num_samples * 1.25, 2))\n",
    "for i, idx in enumerate(chosen_idx):\n",
    "    image, _ = test_dataset[idx]\n",
    "    plt.subplot(1, num_samples, i + 1)\n",
    "    sns.heatmap(image.squeeze(), cmap=plt.cm.gray, cbar=False,\n",
    "                xticklabels=False, yticklabels=False)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Idx {idx}\\nTrue: {y_true[idx]}\\nPred: {y_pred[idx]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and here is our test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_sample.reshape(28, 28), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aug.eval()\n",
    "with torch.no_grad():\n",
    "    output = model_aug(input_tensor)\n",
    "    probabilities = torch.softmax(output, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=np.arange(0,10), y=probabilities.cpu().squeeze());\n",
    "plt.ylabel(\"Probability\");\n",
    "plt.xlabel(\"Class\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try more of your own test samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    " \n",
    "filename = 'input2.png'\n",
    " \n",
    "image = Image.open(filename)\n",
    "image_bw = image.convert('L')\n",
    "image_bw_resized = image_bw.resize((28,28), Image.BICUBIC)\n",
    "image_bw_resized_inverted = PIL.ImageOps.invert(image_bw_resized)\n",
    "test_sample = np.array(image_bw_resized_inverted).reshape(1, 28, 28)\n",
    "p = plt.imshow(np.reshape(test_sample, (28,28)), cmap=plt.cm.gray,);\n",
    "p = plt.title('Shape: ' + str(test_sample.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = basic_transform(test_sample.squeeze())  # shape: [1, 28, 28] → [28, 28] → tensor [1, 28, 28]\n",
    "input_tensor = input_tensor.unsqueeze(0).to(device)  # shape: [1, 1, 28, 28]\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fc.eval()\n",
    "with torch.no_grad():\n",
    "    output = model_fc(input_tensor)\n",
    "    probabilities = torch.softmax(output, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "sns.barplot(x=np.arange(0,10), y=probabilities.cpu().squeeze());\n",
    "plt.ylabel(\"Probability\");\n",
    "plt.xlabel(\"Class\");\n",
    "plt.title(\"Fully connected network\\nPredicted class: %d\" % int(predicted_class));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv.eval()\n",
    "with torch.no_grad():\n",
    "    output = model_conv(input_tensor)\n",
    "    probabilities = torch.softmax(output, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "sns.barplot(x=np.arange(0,10), y=probabilities.cpu().squeeze());\n",
    "plt.ylabel(\"Probability\");\n",
    "plt.xlabel(\"Class\");\n",
    "plt.title(\"Convolutional network\\nPredicted class: %d\" % int(predicted_class));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aug.eval()\n",
    "with torch.no_grad():\n",
    "    output = model_aug(input_tensor)\n",
    "    probabilities = torch.softmax(output, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "sns.barplot(x=np.arange(0,10), y=probabilities.cpu().squeeze());\n",
    "plt.ylabel(\"Probability\");\n",
    "plt.xlabel(\"Class\");\n",
    "plt.title(\"Convolutional network with data augmentation\\nPredicted class: %d\" % int(predicted_class));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More things to try\n",
    "\n",
    "-   This notebook runs using a free GPU on Colab! Try changing the runtime to CPU: Runtime \\> Change Runtime Type and change Hardware Accelerator to CPU. Then run the notebook again. How much speedup did you get with the GPU, relative to CPU?"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 }
}

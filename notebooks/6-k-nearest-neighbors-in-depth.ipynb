{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K nearest neighbors in depth\n",
    "\n",
    "*Fraida Fund*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# for 3d plots\n",
    "from ipywidgets import interact, fixed\n",
    "from mpl_toolkits import mplot3d\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.model_selection import validation_curve, train_test_split\n",
    "\n",
    "plot_colors = np.array(sns.color_palette().as_hex())\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbors classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a binary classification problem: we have a set of labeled inputs, where the labels are all either $0$ or $1$.\n",
    "\n",
    "Our goal is to train a *classifier* to predict a $0$ or $1$ label for new, unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One conceptually simple approach is to simply find the sample in the training data that is “most similar” to our test sample (a “neighbor” in the feature space), and then give the test sample the same label as the “most similar” training sample. This is the *nearest neighbors* classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following plot, with training data shown in blue (negative class) and red (positive class). A test sample is shown in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "X = np.random.uniform(0, 1, size=(n_samples,2))\n",
    "y = np.array(X[:,0]+X[:,1] >= 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a test point\n",
    "x_test = np.random.uniform(0, 1, size=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y);\n",
    "\n",
    "# Plot test point\n",
    "plt.scatter(x=x_test[:,0], y=x_test[:,1], facecolor=plot_colors[2], edgecolor='black', s=100);\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the nearest neighbor, we need a *distance metric*. I chose to use the L2 norm for this example.\n",
    "\n",
    "The nearest neighbor to the test sample is circled, and its label is applied as the prediction for the test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "distances = np.linalg.norm(x_test - X, ord=2, axis=1)\n",
    "nn = np.argsort(distances)[:k]\n",
    "y_pred = y[nn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y);\n",
    "\n",
    "# Plot test point\n",
    "plt.scatter(x=x_test[:,0], y=x_test[:,1], facecolor=plot_colors[y_pred], edgecolor='black', s=100);\n",
    "\n",
    "# Plot nearest neighbors\n",
    "sns.scatterplot(x=X[nn,0], y=X[nn,1], facecolors='none', edgecolor='black', s=150);\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nearest neighbor classifier works by “memorizing” the training data. One interesting consequence of this is that it will have zero prediction error (or equivalently, 100% accuracy) on the training data, since each training sample's nearest neighbor is itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "y_pred = np.zeros(n_samples)\n",
    "for idx, x_sample in enumerate(X):\n",
    "  distances = np.linalg.norm(x_sample - X, ord=2, axis=1) \n",
    "  nn = np.argsort(distances)[:k]\n",
    "  y_pred[idx] = y[nn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_pred==y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K nearest neighbors classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this approach less sensitive to noise, we might choose to look for multiple similar training samples to each new test sample, and classify the new test sample using the mode of the labels of the similar training samples.\n",
    "\n",
    "This is $k$ nearest neighbors, where $k$ is the number of “neighbors” that we search for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, we show the same data as in the previous example. Now, however, the 3 closest neighbors to the test sample are circled, and the mode of their labels is used as the prediction for the new test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "distances = np.linalg.norm(x_test - X, ord=2, axis=1)\n",
    "nn = np.argsort(distances)[:k]\n",
    "\n",
    "y_pred = stats.mode(y[nn], keepdims=True).mode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y);\n",
    "\n",
    "# Plot test point\n",
    "plt.scatter(x=x_test[:,0], y=x_test[:,1], facecolor=plot_colors[y_pred], edgecolor='black', s=100);\n",
    "\n",
    "# Plot nearest neighbors\n",
    "sns.scatterplot(x=X[nn,0], y=X[nn,1], facecolors='none', edgecolor='black', s=150);\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image shows a set of test points plotted on top of the training data. The size of each test point indicate the *confidence* in the label, which we approximate by the proportion of $k$ neighbors sharing that label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 50\n",
    "X_test = np.random.uniform(0, 1, size=(n_test,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.zeros(n_test)\n",
    "y_pred_prob = np.zeros(n_test)\n",
    "\n",
    "for idx, x_test in enumerate(X_test):\n",
    "  distances = np.linalg.norm(x_test - X, ord=2, axis=1)\n",
    "  nn = np.argsort(distances)[:k]\n",
    "\n",
    "  y_pred[idx] = stats.mode(y[nn], keepdims=True).mode[0]\n",
    "  y_pred_prob[idx] = np.mean(y[nn]==y_pred[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, alpha=0.3, legend=False);\n",
    "\n",
    "# Plot test points\n",
    "sns.scatterplot(x=X_test[:,0], y=X_test[:,1], hue=y_pred, s=50,\n",
    "                size=y_pred_prob, edgecolor='black', legend='full');\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);\n",
    "\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that the training error for $k$ nearest neighbors is not *necessarily* zero (though it can be!), since a training sample may have a different label than its $k$ closest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.zeros(n_samples)\n",
    "for idx, x_sample in enumerate(X):\n",
    "  distances = np.linalg.norm(x_sample - X, ord=2, axis=1)\n",
    "  nn = np.argsort(distances)[:k]\n",
    "  y_pred[idx] = stats.mode(y[nn], keepdims=True).mode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_pred==y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of our hand-coded KNN classifier, let’s try the `sklearn` implementation.\n",
    "\n",
    "Like other `sklearn` classification and regression models, we train it using a `fit` method, and make new predictions using a `predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_knn = KNeighborsClassifier(n_neighbors=k).fit(X, y)\n",
    "y_pred  = clf_knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because it makes it easier to get the decision regions/contours out of the classifier, and plot them on the same figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "xx, yy = np.meshgrid(np.arange(0, 1.1, .01),\n",
    "                      np.arange(0, 1.1, .01))\n",
    "Z = clf_knn.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, levels=5, cmap=\"RdBu_r\", alpha=0.5);\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, alpha=0.3, legend=False);\n",
    "\n",
    "# Plot test points\n",
    "sns.scatterplot(x=X_test[:,0], y=X_test[:,1], hue=y_pred, s=50,\n",
    "                size=y_pred_prob, edgecolor='black', legend='full');\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);\n",
    "\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try:\n",
    "\n",
    "-   What happens as we vary $k$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important limitation of $k$ nearest neighbors is that it does not “learn” anything about which features are most important for determining $y$. Every feature is weighted equally in finding the nearest neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first implication of this is:\n",
    "\n",
    "-   If all features are equally important, but they are not all on the same scale, they must be normalized - rescaled onto the interval $[0,1]$. Otherwise, the features with the largest magnitudes will dominate the total distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "X = np.random.uniform(0, 1, size=(n_samples,2))\n",
    "y = np.array(X[:,0]+X[:,1] >= 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a test point\n",
    "x_test = np.array([[0.5,0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "distances = np.linalg.norm(x_test - X, ord=2, axis=1)\n",
    "nn = np.argsort(distances)[:k]\n",
    "\n",
    "y_pred = stats.mode(y[nn], keepdims=True).mode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y);\n",
    "\n",
    "# Plot test point\n",
    "plt.scatter(x=x_test[:,0], y=x_test[:,1], facecolor=plot_colors[y_pred], edgecolor='black', s=100);\n",
    "\n",
    "# Plot nearest neighbors\n",
    "sns.scatterplot(x=X[nn,0], y=X[nn,1], facecolors='none', edgecolor='black', s=150);\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,0] = 100*X[:,0]\n",
    "x_test[:,0] = 100*x_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.linalg.norm(x_test - X, ord=2, axis=1)\n",
    "nn = np.argsort(distances)[:k]\n",
    "\n",
    "y_pred = stats.mode(y[nn], keepdims=True).mode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y);\n",
    "\n",
    "# Plot test point\n",
    "plt.scatter(x=x_test[:,0], y=x_test[:,1], facecolor=plot_colors[y_pred], edgecolor='black', s=100);\n",
    "\n",
    "# Plot nearest neighbors\n",
    "sns.scatterplot(x=X[nn,0], y=X[nn,1], facecolors='none', edgecolor='black', s=150);\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second implication is:\n",
    "\n",
    "-   Even if some features are more important than others, they will all be considered equally important in the distance calculation. If uninformative features are included, they may dominate the distance calculation.\n",
    "\n",
    "Contrast this with our logistic regression classifier. In the logistic regression, the training process involves learning coefficients. The coefficients weight each feature's effect on the overall output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following images from CIFAR10, a dataset of low-resolution images in ten classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3))\n",
    "\n",
    "plt.subplot(1, 3,1)\n",
    "\n",
    "plt.imshow(X_test[6].astype('uint8'));\n",
    "plt.title(\"%s \\n (test sample)\" % classes[y_test[6][0]]);\n",
    "plt.axis('off');\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "\n",
    "plt.imshow(X_train[38805].astype('uint8'));\n",
    "dist = np.linalg.norm(X_train[38805].ravel()*1.0/255-X_test[6].ravel()*1.0/255, ord=2)\n",
    "plt.title(\"%s \\n distance: %f\" % (classes[y_train[38805][0]], dist));\n",
    "plt.axis('off');\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "\n",
    "plt.imshow(X_train[32488].astype('uint8'));\n",
    "dist = np.linalg.norm(X_train[32488].ravel()*1.0/255-X_test[6].ravel()*1.0/255, ord=2)\n",
    "plt.title(\"%s \\n distance: %f\" % (classes[y_train[32488][0]], dist));\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images above show a test sample, and two training samples with their distances to the test sample.\n",
    "\n",
    "The background pixels in the test sample “count” just as much as the foreground pixels, so that the image of the deer is considered a very close neighbor, while the image of the car is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3))\n",
    "\n",
    "plt.subplot(1, 3,1)\n",
    "\n",
    "plt.imshow(X_test[6].astype('uint8'));\n",
    "plt.title(\"%s \\n (test sample)\" % classes[y_test[6][0]]);\n",
    "plt.axis('off');\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "\n",
    "plt.imshow(X_train[38805].astype('uint8'));\n",
    "dist = np.linalg.norm(X_train[38805].ravel()*1.0/255-X_test[6].ravel()*1.0/255, ord=2)\n",
    "plt.title(\"%s \\n distance: %f\" % (classes[y_train[38805][0]], dist));\n",
    "plt.axis('off');\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "\n",
    "plt.imshow(X_train[20278].astype('uint8'));\n",
    "dist = np.linalg.norm(X_train[20278].ravel()*1.0/255-X_test[6].ravel()*1.0/255, ord=2)\n",
    "plt.title(\"%s \\n distance: %f\" % (classes[y_train[20278][0]], dist));\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(We also see here that pixel-wise L2 distance is not a good measure of visual similarity - the frog on the right is almost as similar to the car as the deer in the middle!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the classification example above, the two classes were separated by a linear *decision boundary*.\n",
    "\n",
    "One of the major benefits of $k$ nearest neighbors is its ability to capture complex decision boundaries. Consider the following example, where the two classes are separated by a polynomial boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "sigma = 0.1\n",
    "k = 3\n",
    "coefs=np.array([0.3, 1, -1.5, -2])\n",
    "xrange=[-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_polynomial_classifier_data(n=100, xrange=[-1,1], coefs=[1,0.5,0,2], sigma=0.5):\n",
    "  x = np.random.uniform(xrange[0], xrange[1], size=(n, 2))\n",
    "  ysep = np.polynomial.polynomial.polyval(x[:,0],coefs)\n",
    "  y = (x[:,1]>ysep).astype(int)\n",
    "  x[:,0] = x[:,0] + sigma * np.random.randn(n)\n",
    "  x[:,1] = x[:,1] + sigma * np.random.randn(n)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_polynomial_classifier_data(n=n_samples, xrange=xrange, coefs=coefs, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y);\n",
    "\n",
    "# Plot true function\n",
    "xtrue = np.linspace(-1, 2)\n",
    "ytrue = np.polynomial.polynomial.polyval(xtrue,coefs) \n",
    "sns.lineplot(x=xtrue, y=ytrue, color='black', label='Polynomial')\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");\n",
    "plt.xlim((xrange[0], xrange[1]));\n",
    "plt.ylim((xrange[0], xrange[1]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `sklearn` implementation of a `KNeighborsClassifier` to learn this training data and use it for new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=k).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can plot out the predictions of the model over the input space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decisions\n",
    "xx, yy = np.meshgrid(np.arange(1.1*X[:,0].min(), 1.1*X[:,0].max(), .05),\n",
    "                      np.arange(1.1*X[:,1].min(), 1.1*X[:,1].max(), .05))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_prob = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "sns.scatterplot(x=xx.ravel(), y=yy.ravel(), hue=Z.ravel(), \n",
    "                size=Z_prob.max(axis=1), \n",
    "                sizes=(20,40),\n",
    "                legend='full', alpha=0.7);\n",
    "\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");\n",
    "plt.xlim((xrange[0], xrange[1]));\n",
    "plt.ylim((xrange[0], xrange[1]));\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or equivalently, as a contour plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "xx, yy = np.meshgrid(np.arange(1.1*X[:,0].min(), 1.1*X[:,0].max(), .01),\n",
    "                      np.arange(1.1*X[:,1].min(), 1.1*X[:,1].max(), .01))\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, levels=5, cmap=\"RdBu_r\", alpha=0.5);\n",
    "\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");\n",
    "plt.xlim((xrange[0], xrange[1]));\n",
    "plt.ylim((xrange[0], xrange[1]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try:\n",
    "\n",
    "-   What happens as we vary $k$?\n",
    "-   What happens as we vary $\\sigma$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K nearest neighbors regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K nearest neighbors can also be used for regression, with just a small change: instead of using the mode of the nearest neighbors to predict the label of a new sample, we use the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "X = np.random.uniform(-6, 6, size=(n_samples,2))\n",
    "y = np.sin(np.sqrt(X[:,0] ** 2 + X[:,1] ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis');\n",
    "cbar = plt.colorbar();\n",
    "\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a test sample, then use $k$ nearest neighbors to predict its value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.random.uniform(-6, 6, size=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "distances = np.linalg.norm(x_test - X, ord=2, axis=1)\n",
    "nn = np.argsort(distances)[:k]\n",
    "\n",
    "y_pred = np.mean(y[nn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training data\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', alpha=0.5, vmin=-1, vmax=1);\n",
    "cbar = plt.colorbar();\n",
    "\n",
    "# Plot nearest neighbors\n",
    "sns.scatterplot(x=X[nn,0], y=X[nn,1], facecolors='none', edgecolor='black', s=150);\n",
    "\n",
    "# Plot test point with prediction\n",
    "plt.scatter(x_test[:,0], x_test[:,1], c=y_pred, s=100, edgecolor='black', cmap='viridis', vmin=-1, vmax=1);\n",
    "\n",
    "# Plot formatting stuff\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");\n",
    "\n",
    "plt.title(\"Predicted value: %f \\n Value of true function: %f\" % \n",
    "          (y_pred,\n",
    "          np.sin(np.sqrt(x_test[:,0] ** 2 + x_test[:,1] ** 2))));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `sklearn` implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_knn = KNeighborsRegressor(n_neighbors = k).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1,2,1, )\n",
    "\n",
    "# Plot the decision boundary\n",
    "xx, yy = np.meshgrid(np.arange(1.1*X[:,0].min(), 1.1*X[:,0].max(), .01),\n",
    "                      np.arange(1.1*X[:,1].min(), 1.1*X[:,1].max(), .01))\n",
    "Z = reg_knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, levels=5, cmap='viridis', vmin=-1, vmax=1, alpha=0.5);\n",
    "\n",
    "# Plot training data\n",
    "plt.scatter(x=X[:,0], y=X[:,1], c=y, cmap='viridis', vmin=-1, vmax=1, alpha=0.5);\n",
    "\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "# Plot the decision boundary\n",
    "Z = np.sin(np.sqrt(xx.ravel() ** 2 +  yy.ravel() ** 2))\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, levels=5, cmap='viridis', vmin=-1, vmax=1, alpha=0.5);\n",
    "\n",
    "# Plot training data\n",
    "plt.scatter(x=X[:,0], y=X[:,1], c=y, cmap='viridis', vmin=-1, vmax=1, alpha=0.5);\n",
    "\n",
    "plt.colorbar();\n",
    "plt.subplots_adjust(wspace=0.05)\n",
    "plt.xlabel(\"x1\");\n",
    "plt.yticks(ticks=[], labels=[]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try:\n",
    "\n",
    "-   What happens as we vary $k$?\n",
    "-   What if $k=N_\\text{train}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that:\n",
    "\n",
    "-   The bias of a model for a given test point is the difference between the value of the *true* function at that point and the *mean* prediction of the model, if it was trained many times on independent training sets.\n",
    "-   The variance of a model is the variance in the prediction of the model for that point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data above, which classifier do you think will have greater bias, and which classifier do you think will have greater variance?\n",
    "\n",
    "-   **Model A**: K nearest neighbors with $k=1$\n",
    "-   **Model B**: K nearest neighbors with $k=15$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in previous lessons, we can verify our intuition by simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 200\n",
    "n_plot = 20\n",
    "n_test = 500\n",
    "n_train = 200\n",
    "sigma= 0.1\n",
    "coefs=np.array([0.3, 1, -1.5, -2])\n",
    "xrange=[-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.zeros((n_test, n_repeat, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data once\n",
    "x_test, y_test = generate_polynomial_classifier_data(n=n_test, xrange=xrange, coefs=coefs, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_test[:,0], y=x_test[:,1], hue=y_test);\n",
    "\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");\n",
    "plt.xlim((xrange[0], xrange[1]));\n",
    "plt.ylim((xrange[0], xrange[1]));\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_sim = np.zeros((3600, n_repeat, 2))\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax_a, ax_b = fig.subplots(1, 2, sharex=True, sharey=True)\n",
    "\n",
    "# now simulate training the model many times, on different training data every time\n",
    "# and evaluate using the test data\n",
    "for i in range(n_repeat):\n",
    "\n",
    "  # train both models on newly generated training data\n",
    "  X, y = generate_polynomial_classifier_data(n=n_train, xrange=xrange, coefs=coefs, sigma=sigma)\n",
    "\n",
    "  clf_a = KNeighborsClassifier(n_neighbors=1).fit(X, y)\n",
    "  clf_b = KNeighborsClassifier(n_neighbors=15).fit(X, y)\n",
    "\n",
    "  y_predict[:, i, 0] = clf_a.predict(x_test)\n",
    "  y_predict[:, i, 1] = clf_b.predict(x_test)\n",
    "\n",
    "  xx, yy = np.meshgrid(np.arange(-1.5, 1.5, .05),\n",
    "                      np.arange(-1.5, 1.5, .05))\n",
    "  Z_a = clf_a.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "  Z_sim[:, i, 0] = Z_a\n",
    "  Z_b = clf_b.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "  Z_sim[:, i, 1] = Z_b\n",
    "\n",
    "  if i < n_plot:\n",
    "    \n",
    "    ax_a.contour(xx, yy, Z_a.reshape(xx.shape), levels=[0.5], alpha=0.4, colors='plum');\n",
    "    plt.xlim((xrange[0], xrange[1]));\n",
    "    plt.ylim((xrange[0], xrange[1]));\n",
    "\n",
    "    ax_b.contour(xx, yy, Z_b.reshape(xx.shape), levels=[0.5], alpha=0.4, colors='plum');\n",
    "    plt.xlim((xrange[0], xrange[1]));\n",
    "    plt.ylim((xrange[0], xrange[1]));\n",
    "\n",
    "\n",
    "cs_a = ax_a.contour(xx, yy, Z_sim[:,:,0].mean(axis=1).reshape(60,60), levels=[0.5], colors='magenta', linewidths=2);\n",
    "cs_b = ax_b.contour(xx, yy, Z_sim[:,:,1].mean(axis=1).reshape(60,60), levels=[0.5], colors='magenta', linewidths=2);\n",
    "\n",
    "\n",
    "# Plot true function\n",
    "xtrue = np.arange(-1.5, 1.5, .05)\n",
    "ytrue = np.polynomial.polynomial.polyval(xtrue,coefs) \n",
    "sns.lineplot(x=xtrue, y=ytrue, color='black', ax=ax_a);\n",
    "sns.lineplot(x=xtrue, y=ytrue, color='black', ax=ax_b);\n",
    "\n",
    "ax_a.set_title(\"Model A\");\n",
    "ax_b.set_title(\"Model B\");\n",
    "\n",
    "ax_a.set_ylabel(\"x2\");\n",
    "ax_a.set_xlabel(\"x1\");\n",
    "ax_b.set_xlabel(\"x1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation of bias and variance *of the boundary*, not of the estimated class labels\n",
    "var_a = np.var(Z_sim[:,:,0], axis=1).mean()\n",
    "mean_pred_a = cs_a.collections[0].get_paths()[0].vertices\n",
    "x_mean_pred_a = mean_pred_a[:,0] \n",
    "y_mean_pred_a = mean_pred_a[:,1]\n",
    "bias_a = np.mean((y_mean_pred_a - np.polynomial.polynomial.polyval(x_mean_pred_a,coefs))**2)\n",
    "\n",
    "\n",
    "var_b = np.var(Z_sim[:,:,1], axis=1).mean()\n",
    "mean_pred_b = cs_b.collections[0].get_paths()[0].vertices\n",
    "x_mean_pred_b = mean_pred_b[:,0] \n",
    "y_mean_pred_b = mean_pred_b[:,1]\n",
    "bias_b = np.mean((y_mean_pred_b - np.polynomial.polynomial.polyval(x_mean_pred_b,coefs))**2)\n",
    "\n",
    "\n",
    "print(\"Model A: \\n Bias^2 on boundary: %f \\n Variance on boundary: %f \" % (bias_a, var_a));\n",
    "print(\"Model B: \\n Bias^2 on boundary: %f \\n Variance on boundary: %f \" % (bias_b, var_b));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try:\n",
    "\n",
    "-   What happens as we vary $k$?\n",
    "-   What happens as we vary $\\sigma$?\n",
    "-   What happens as we vary $N$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to class notes for derivation, but:\n",
    "\n",
    "-   Variance of $k$ nearest neighbors increases with $\\sigma$, decreases with $k$\n",
    "-   Bias of $k$ nearest neighbors tends to increase with $k$, if the true function $t(x)$ is smooth.\n",
    "\n",
    "(The few closest neighbors to a test point are likely to have similar values, and their average will be close to the value of the true function in that “neighborhood”. As $k$ increases, and we begin to include neighbors that are far away, their average may be very different from the value of the true function at that test point.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing $k$ with cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with other hyperparameters that control the bias-variance tradeoff, we can choose the optimal $k$ for a particular problem with K-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we demonstrate the use of the `sklearn` `validation_curve` function for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(noise=0.2, factor=0.5, random_state=0, n_samples=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_train[:,0], y=X_train[:,1], hue=y_train);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we compute and plot the K-fold CV training scores and validation scores for a range of values of $k$ (number of neighbors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 10\n",
    "n_fold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores, valid_scores = validation_curve(\n",
    "                            KNeighborsClassifier(), \n",
    "                            X_train, y_train, \n",
    "                            param_name=\"n_neighbors\", \n",
    "                            param_range=np.arange(1, max_k, 2),\n",
    "                            cv=n_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_val_mean = valid_scores.mean(axis=1)\n",
    "acc_val_se   = valid_scores.std(axis=1)/np.sqrt(n_fold-1)\n",
    "\n",
    "acc_tr_mean = train_scores.mean(axis=1)\n",
    "acc_tr_se   = train_scores.std(axis=1)/np.sqrt(n_fold-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=np.arange(1,max_k, 2), y=acc_tr_mean, marker='o', label=\"Training score\", color=plot_colors[0]);\n",
    "sns.lineplot(x=np.arange(1,max_k, 2), y=acc_val_mean, marker='o', label=\"Validation score\", color=plot_colors[1]);\n",
    "\n",
    "plt.errorbar(x=np.arange(1,max_k, 2), y=acc_tr_mean, yerr=acc_tr_se, label=\"Training score\", capsize=1);\n",
    "plt.errorbar(x=np.arange(1,max_k, 2), y=acc_val_mean, yerr=acc_val_se, label=\"Validation score\", capsize=1);\n",
    "\n",
    "plt.xlabel(\"k\");\n",
    "plt.ylabel(\"Accuracy score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can select the best model according to the 1-SE rule.\n",
    "\n",
    "Note that the mode with the *largest* number of neighbors is the “simplest” model, in the sense that it has the smoothest decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting best model for KNN\n",
    "# we are looking for the *largest* number of neighbors with score no \n",
    "# more than one SE *less than* the *maximum* score\n",
    "nn_list = np.arange(1, max_k, 2)\n",
    "\n",
    "# index of *maximum* score value\n",
    "idx_best = np.argmax(acc_val_mean)\n",
    "# target is no more than one SE *less than* that\n",
    "target = acc_val_mean[idx_best] - acc_val_se[idx_best]\n",
    "\n",
    "# get largest number of neighbors (simplest model) \n",
    "# where mean score is still greater than target\n",
    "idx_1se = max(np.where(acc_val_mean >= target)[0])\n",
    "nn_1se = nn_list[idx_1se]\n",
    "print(\"Best model has number of neighbors: %d\" % nn_1se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The “curse of dimensionality”\n",
    "\n",
    "Classifiers that rely on pairwise distance between points, like the $k$ neighbors methods, are heavily impacted by a problem know as the “curse of dimensionality”. In this section, we will illustrate the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 30\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at a problem with data uniformly distributed in each dimension of the feature space, and two classes separated by a linear boundary.\n",
    "\n",
    "We will generate a test point, and show the $k$ nearest neighbors to the test point. We will also show the length (or area, or volume) that we had to search to find those $k$ test points.\n",
    "\n",
    "Pay special attention to how that length (or area, or volume) changes as we increase the dimensionality of the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(0, 1, size=(n_samples,3))\n",
    "y = np.array(X.sum(axis=1) >= 1.5).astype(int)\n",
    "\n",
    "x_test = 0.5*np.ones(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the 1D problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nearest neighbors\n",
    "distances = np.linalg.norm(x_test[0] - X[:,0], ord=2, axis=1)\n",
    "nn = np.argsort(distances)[:k]\n",
    "mdist = np.max(distances[nn])\n",
    "\n",
    "# Figure formatting stuff\n",
    "plt.figure(figsize=(8,1))\n",
    "plt.xlim(0,1);\n",
    "plt.ylim(-0.01, 0.01);\n",
    "plt.xlabel(\"x1\")\n",
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=0, hue=y, s=20, edgecolor=None, legend=False);\n",
    "# Plot region that encloses nearest neighbors\n",
    "plt.plot([x_test-mdist, x_test+mdist], [0,0], color=plot_colors[2], lw=2, alpha=0.2);\n",
    "# Plot nearest neighbors\n",
    "sns.scatterplot(x=X[nn,0], y=0, facecolors='none', edgecolor='black', s=150);\n",
    "# Plot test point\n",
    "plt.scatter(x=x_test[0], y=0, facecolor=plot_colors[2], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the 2D equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nearest neighbors\n",
    "distances = np.linalg.norm(x_test[0:2] - X[:, 0:2], ord=2, axis=1)\n",
    "nn = np.argsort(distances)[:k]\n",
    "mdist = np.max(distances[nn])\n",
    "\n",
    "# Figure formatting stuff\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "plt.xlim(0,1);\n",
    "plt.ylim(0,1);\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=plot_colors[y], s=20, edgecolor=None, legend=False, ax=ax);\n",
    "# Plot region that encloses nearest neighbors\n",
    "circle = plt.Circle(x_test,mdist,\n",
    "                          fill=True, color=plot_colors[2], alpha=0.2, zorder=1)\n",
    "ax.add_patch(circle)\n",
    "# Plot nearest neighbors\n",
    "sns.scatterplot(x=X[nn,0], y=X[nn,1], facecolors='none', s=100, edgecolor='black', legend=False, ax=ax);\n",
    "# Plot test point\n",
    "plt.scatter(x=x_test[0], y=x_test[1], facecolor=plot_colors[2], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the 3D equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D(elev=20, azim=-20, X=X, y=y, x_test=x_test):\n",
    "    # Figure formatting stuff\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_zlabel('x3')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_zlim(0, 1)\n",
    "\n",
    "\n",
    "    # Get nearest neighbors\n",
    "    distances = np.linalg.norm(x_test - X, ord=2, axis=1)\n",
    "    nn = np.argsort(distances)[:k]\n",
    "    mdist = np.max(distances[nn])\n",
    "\n",
    "    # Plot training data\n",
    "    ax.scatter3D(X[:,0], X[:,1], X[:,2], s=50, c=plot_colors[y])\n",
    "\n",
    "    # Plot region that encloses nearest neighbors\n",
    "    u, v = np.mgrid[0:2*np.pi:16*1j, 0:np.pi:8*1j]\n",
    "    sphere_x = x_test[0] + mdist * np.cos(u) * np.sin(v)\n",
    "    sphere_y = x_test[1] + mdist * np.sin(u) * np.sin(v)\n",
    "    sphere_z = x_test[2] + mdist * np.cos(v)\n",
    "    ax.plot_wireframe(sphere_x, sphere_y, sphere_z, alpha=0.2, color=plot_colors[2])\n",
    "\n",
    "    # Plot nearest neighbors\n",
    "    ax.scatter3D(X[nn,0], X[nn,1], X[nn,2], s=150, c=plot_colors[y[nn]], edgecolor='black')\n",
    "\n",
    "    # Plot test point\n",
    "    ax.scatter3D(x_test[0], x_test[1], x_test[2], s=200, color=plot_colors[2])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_3D, elev=widgets.FloatSlider(min=-90,max=90,step=1, value=20), \n",
    "         azim=widgets.FloatSlider(min=-90,max=90,step=1, value=20), \n",
    "         X=fixed(X), y=fixed(y), x_test=fixed(x_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as the dimensionality of the problem grows, the higher-dimensional space is less densely occupied by the training data, and we need to search a large volume of space to find neighbors of the test point. **The pair-wise distance between points grows** as we add additional dimensions.\n",
    "\n",
    "And in that case, the neighbors may be so far away that they don’t actually have much in common with the test point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try:\n",
    "\n",
    "-   What happens as we vary $k$?\n",
    "-   What happens as we vary $N$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that a separating hyperplane is not affected the same way - see [these notes](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html) for more details.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the length of the smallest hyper-cube that contains all $k$-nearest neighbors of a test point is:\n",
    "\n",
    "$$\\left(\\frac{k}{N}\\right)^{\\frac{1}{d}}$$\n",
    "\n",
    "for $N$ samples with dimensionality $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the expression above, we can see that as number of dimensions increases *linearly*, the number of training samples must increase *exponentially* to counter the “curse”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can reduce $d$ - either by feature selection, or by transforming the data into a lower-dimensional space.\n",
    "\n",
    "(In the unsupervised learning unit of this course, we will learn some methods to transform the high-dimensional data into a lower-dimensional space.)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 }
}
